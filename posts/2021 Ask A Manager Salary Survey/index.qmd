---
title: "Ask A Manager Survey"
image: "./Ask_A_Manager_Logo.png"
description: "This post is about the 2021 Ask A Manager Survey analysis. It provides an overview of the data analysis process attempting to fit a linear regression model to explain variances in reported salaries among the respondents"
date: December 25, 2021
author: "Sivuyile Nzimeni"
categories: [data analysis, data modelling]
listing:
  contents: posts
execute: 
  echo: true
  warning: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)
lapply(as.list(c("tidyverse","janitor","tidymodels","glmnet",
                 "gt","gtExtras","gtsummary")),
       require,character.only=TRUE)
set_gtsummary_theme(theme_gtsummary_journal(journal = "qjecon"))
```

# INTRODUCTION

In early 2021, the [Ask A Manager](https://www.askamanager.org/) blogsite ran their annual [salary survey](https://www.askamanager.org/2021/05/look-at-24000-peoples-real-life-salaries.html). The survey responses are stored on googlesheets, making the data accessible to all interested. In a [previous post](https://www.linkedin.com/pulse/2021-ask-manager-salary-survey-sivuyile-nzimeni/), we discussed data pre-processing and feature selection method. This post focuses two aspects,namely, 1) Reproducibility and 2) Out-of-Sample Testing.

## REPRODUCIBILITY

In the previous post, we detailed the feature selection method by regularised regreesion, specifically, LASSO regression. In this post, we will attempt to reproduce the feature selection method.

## OUT OF SAMPLE TESTING

In this post, we will also follow the traditional machine learning workflow including, splitting the data into a training and testing samples, fitting a model, cross-validation and finally fitting the model on the out-of-sample dataset(testing dataset). This approach can inform us about the model's parsimony. In other words, can the model perform well on an unknown sample.

```{r Import_Data}
Manager_Salaries <- read_csv(file = "./2021_Cleaned_Job_Data.csv")
```

```{r Training_Split}
Manager_Salaries <- recipe(new_annual_salary~.,Manager_Salaries) %>% 
  step_nzv(all_predictors()) %>% 
  prep() %>% 
  bake(Manager_Salaries)

Salary_Split <- initial_split(na.omit(Manager_Salaries))
Salary_Train <- training(Salary_Split)
Salary_Test <- testing(Salary_Split)
```

# LASSO REGRESSION

As mentioned above, the data cleaning process was detailed on an earlier post. It is worth highlighting that the dataset contained several dummy variables indicating various respondent attributes such as job_title, industry, city, age group etc. Our dependent variable is the annual salary. Given the large number of independent variables. In the code below, we fit a LASSO regression model.

```{r LASSO_Definition,echo=TRUE}
X <- model.matrix(new_annual_salary ~.,Salary_Train)[,-1]
Y <- log(Salary_Train$new_annual_salary)

lasso <- glmnet(x=X,
       y= Y,
       alpha = 1)
```

## IMPORTANT VARIABLE EXTRACTION

The resulting object is a list of class "glmnet". Essentially, the results contain all the iterations through alpha and to find the minimum lambda. As such, there are several results in the object. We are primarily interested in extracting the remaining variables at minimum lambda along with their coeffiecients. The [vip package](https://cran.r-project.org/web/packages/vip/vip.pdf) can automate the plotting of the results. However, we want to extract the variables names in order to fit them on our training test. There is probably a package to assist with this step somewhere in the wild, however, we haven't found it yet. Luckily in R, we can write custom functions. The code below details the variable_extractor function.

```{r Extract_Variables,echo=TRUE}
variable_extractor <- function(a_list){
  min_lambda <- data.frame(best_lambda =a_list[["lambda"]]==min(a_list[["lambda"]]))
  min_lambda <- cbind.data.frame(data.frame(index = rownames(min_lambda)),
                                 min_lambda)
  min_lambda <- min_lambda %>% 
    filter(best_lambda == TRUE)
  min_lambda <- as.double(unique(min_lambda$index))
  lasso_variables <- as.matrix(coef(a_list))|>data.frame()
  lasso_variables <- cbind.data.frame(variables = rownames(lasso_variables),
                                      lasso_variables)
  lasso_variables <- tibble(lasso_variables)
  lasso_variables <- lasso_variables[,c(1,min_lambda+1)]
  names(lasso_variables) <- c("variable","importance")
  lasso_variables <- lasso_variables %>% 
    filter(variable != "(Intercept)",
           importance != 0.00000000) %>% 
    arrange(desc(importance))

  return(lasso_variables)
}

lasso_variable <- variable_extractor(lasso)
```

The function takes a list of attribute "glmnet". Thereafter, we find the smallest lambda. In addition, we extract all the coefficients from the object and store them as a wide dataframe. The dataframe is subset to only contain the variable name along with the coefficients of the smallest lambda value. We discard the intercept and variable with coefficients that are equal to 0.0000000. The final output is identical to the variables extracted by the vip package. Finally, we subset both the training dataset and the testing dataset to only contain the selected independent variables. Since the outcome variable is expressed in USD terms, we log the outcome variable.

```{r more_cleaning}
Salary_Train <- cbind.data.frame(Salary_Train$new_annual_salary,
Salary_Train %>% 
  select(lasso_variable$variable))
Salary_Test <- cbind.data.frame(Salary_Test$new_annual_salary,
                                Salary_Test %>% 
                                  select(lasso_variable$variable))

names(Salary_Train)[[1]] <- "new_annual_salary"
names(Salary_Test)[[1]] <- "new_annual_salary"
Salary_Train$new_annual_salary <- log(Salary_Train$new_annual_salary)
Salary_Test$new_annual_salary <- log(Salary_Test$new_annual_salary)
```

## TIDYMODELS: A CLEAN INTERFACE FOR MACHING LEARNING

The R programming language doesn't not lack methods for running machine learning algorithms, it is after all, a statistical programming language. The [tidymodels metapackage](https://www.tidymodels.org/) aims to provide a standard interface for modelling and machine learning using tidyverse principles. Below, we use the package to complete a number of steps including, crossfold_validation, model specification, fitting on the resamples and finally fitting the model on the training dataset.

```{r Salary_Folds,echo=TRUE}
Salary_Folds <- vfold_cv(Salary_Train)

lm_spec <- linear_reg(engine = "lm")

lm_recipe <- recipe(new_annual_salary ~.,Salary_Train) %>% 
  step_nzv(all_predictors())

lm_wf <- workflow(lm_recipe,lm_spec)

doParallel::registerDoParallel(cores = 10)
ctrl_preds <- control_resamples(save_pred = TRUE)
cv_results <- fit_resamples(lm_wf,Salary_Folds,control = ctrl_preds)

lm_wf <- fit(lm_wf,Salary_Train)

collect_metrics(cv_results,summarize = FALSE) %>% 
  filter(.metric == "rsq") %>% 
  summarise(avg_estimate = mean(.estimate),
            .groups = "drop")
```

Across all 10 validation folds, the linear regression model's adjusted R-square averaged 0.272. It is possible to tune the parameters and use battery of other machine learning models to improve performance. In the previous post, we utilised random forest to try an improve the model. Despite, requiring additional computational power, the increases in peformance were marginal to neglible.

At this point, we have completed our first objective. We are able to reproduce the LASSO regresison results in the previous post. The next objective is to determine the parsimony of the model. Here, we fit model on the test data.

```{r fit_on_sets}
Test_Summary <- lm(new_annual_salary~.,data=Salary_Test)
Training_Summary <- lm(new_annual_salary ~.,data=Salary_Train)
```

```{r Comparison_Table}
Training_Model <- gtsummary::tbl_regression(Training_Summary,add_estimate_to_reference_rows=TRUE,
                          intercept = TRUE) %>% 
  as_gt()
Testing_Model <- tbl_regression(Test_Summary,add_estimate_to_reference_rows=TRUE,
                                intercept=TRUE) %>% 
  as_gt()
```

```{r to_gt_train}
Training_Model <- Training_Model %>%
  gt_theme_nytimes() %>% 
  tab_header(title= "Linear Regression Model: Training Dataset",
             subtitle = "Depedent Variable: log(Annual Salary(USD))") %>% 
  gt::tab_source_note("Adjusted R-square: 0.2732") %>% 
  gt::opt_align_table_header(align = "center")
```

```{r to_gt_test}
Testing_Model <- Testing_Model %>% 
  gt_theme_nytimes() %>% 
  tab_header(title= "Linear Regression Model: Test Dataset",
             subtitle = "Depedent Variable: log(Annual Salary(USD))") %>% 
  gt::tab_source_note("Adjusted R-square: 0.2624") %>% 
  gt::opt_align_table_header(align = "center")
```

# RESULTS

### Training Dataset

```{r Training_Mod}
Training_Model
```

### Test Dataset

```{r Test_mod}
Testing_Model
```

# CONCLUSION

The model performs well on an out-of-sample dataset with year of experience (21 - 30 years), job title (Director) and highest level of education (PhD) being among the most important predictors of annual salary. Provided the questions utilised in 2022 are similar or identical to the 2021 survey, the model above can be evaluated on the 2022 survey results.

# REFERENCES

Silge,J. 2021.Fit and predict with tidymodels for #TidyTuesday bird baths in Australia. Available From: https://juliasilge.com/blog/bird-baths/ (Accessed 24 December 2021).

Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686

Sam Firke (2021). janitor: Simple Tools for Examining and Cleaning Dirty Data. R package version 2.1.0. https://CRAN.R-project.org/package=janitor

Kuhn et al., (2020). Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org

Jerome Friedman, Trevor Hastie, Robert Tibshirani (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22. https://www.jstatsoft.org/v33/i01/.

Richard Iannone, Joe Cheng and Barret Schloerke (2021). gt: Easily Create Presentation-Ready Display Tables. R package version 0.3.1. https://CRAN.R-project.org/package=gt

Thomas Mock (2021). gtExtras: A Collection of Helper Functions for the gt Package. R package version 0.2.2.11. https://github.com/jthomasmock/gtExtras

Sjoberg DD, Whiting K, Curry M, Lavery JA, Larmarange J. Reproducible summary tables with the gtsummary package. The R Journal 2021;13:570--80. https://doi.org/10.32614/RJ-2021-053.
