---
title: "2022 Ask A Manager Salary Survey"
image: "./Ask_A_Manager_2022.png"
image-alt: "Ask A Manager Logo 2022"
lang: "en-GB"
date: "December 18,2022"
author: "Sivuyile Nzimeni"
description: "Analysing the 2022 Ask A Manager Salary Survey. In this post, we detail the data preprocessing procedure, feature-selection and modelling process to estimate salaries among respondents."
categories: [Data Modelling,Data Cleaning]
fig-dpi: 300
fig-align: 'center'
fig-cap-location: 'top'
code-copy: true
code-line-numbers: true
page-layout: 'article'
cap-location: 'margin'
number-sections: true
toc: true
toc-title: 'CONTENTS'
execute: 
  echo: true
  warning: false
  cache: false
bibliography: references.bib
---

# INTRODUCTION

Last year, we analysed the [2021 Ask A Manager Salary Survey](https://sivuyilenzimeni.netlify.app/posts/2021%20ask%20a%20manager%20salary%20survey/). The dataset is interesting for many reasons including,most beneficially, the dataset is open and available for all interested parties. It is also offers an opportunity to move beyond old and boring datasets such as `mtcars` and `iris`. The results of the survey offer an opportunity to clean open-text fields, currency data and feature engineering for data modelling.

Fortunately, there is a [2022 Ask A Manager Salary Survey](https://docs.google.com/spreadsheets/d/1Uq5GwatBdujitJkP2X5SDChHSbg68rmuOz9T9eTzu40/edit?resourcekey#gid=1660826355) that is also freely available. Notably, there were only 15 465 respondents compared to 27 919 responds in the previous survey. It is difficult to estimate the reasons for the reduction in responses. Nonetheless,there are sufficient responses to model upon. The second batch of responses offers an opportunity to refine the data preprocessing and modelling process.

## DATA PREPROCESSING

Unlike in the previous post, the data preprocessing step is contained in a stand-alone script `./Scripts/2022_Data_Cleaner.R`. This helps with two main aspects; i) practising modular scripts and ii) reducing the size of the analysis. The data cleaning script has several components in it. The table below summarises the aforementioned components.

```{r Data_Preprocessing_Sections}
#| label: 2022 Data Cleaner Sections
#| fig-cap: A table containing 2022 Data Cleaner sections
#| fig-align: center

library(gt)
library(gtExtras)

data.frame(section = c("Importing Libraries","Importing Data","Open Text Cleaning",
                       "List to Vec","Completeness Check"),
           script_sample =c('lapply(as.list(c("tidyverse","janitor","arrow",
                 "ggthemes","tidytext","tidymodels",
                 "textrecipes","glmnet")),
       require,character.only=TRUE) |>
  suppressWarnings() |> 
  suppressMessages() |> 
  invisible()','read_csv(file = "./Data/Ask A Manager Salary Survey 2022_Responses.csv") |> 
  clean_names() |> 
  mutate(timestamp = mdy_hms(timestamp)) |> 
  filter(!is.na(job_title)) |> 
  mutate(additional_compensation = ifelse(is.na(additional_compensation),
                                          0,additional_compensation),
         full_compensation = salary+additional_compensation) |> 
  filter(!currency %in% c("Other","HKD")) |> 
  rownames_to_column("respondent")','text_cleaner <- function(a_vec){
  tokens <- str_split(a_vec,pattern="\\s{1,}") |> 
    unlist()
  tokens <- tokens[!grepl("\\s{1,}|[[:punct:]]|\\d{1,}",tokens)] |> str_to_lower()
  tokens <- unique(tokens) 
  tokens <- tokens[!tokens %in% c(stopwords::data_stopwords_nltk[["en"]])]
  return(tokens)}','list_to_vec <- function(a_vec){
  new_vec <- a_vec |> 
    as.character()
new_vec <- gsub("(^c+[[:punct:]])|[[:punct:]]",
                replacement = "",
                new_vec)
  return(new_vec)}
','completeness <- function(a_vec){
  incomplete <- round(100/length(a_vec)*a_vec[is.na(a_vec)] |> length(),2)
return(incomplete)}
'),
           reasoning = c("importing libraries required for preprocessing",
                         "import data, convert date column to date type, remove unviable cases and create total compensation columns",
                         "split open-text fields into tokens, remove punctuation, spaces and stopwords. The function returns a list of words per case",
                         "Convert the list of words per case to a vector.",
                         "Check the level of missing values in each column.")) |> 
  gt() |> 
  gt_theme_nytimes() |> 
  tab_header(title = "2022 Data Clean Script Overview",
                 subtitle = "An overview of sections in the data cleaning script") |> 
  opt_align_table_header(align = "center") |> 
  tab_style(style = list(
    cell_fill(color= "#490E6F",alpha=0.8),
    cell_text(color = "white",font=google_font("Fira Code"),
              align = "center",style = "normal",
              weight= "lighter",whitespace = "pre-line")
  ),
  locations= cells_body(
    columns = script_sample
  ))
```

The table above contains a few convenience functions used throughout the script. Importing of libraries is completed through `lapply` followed by `suppressWarnings`, `suppressMessages` and `invisible` functions. The last three functions calls are probably bad practise as warnings may be helpful down the line. A crucial function is the importing data followed by some basic cleaning functions from `lubridate` and @wickham2022 . A more exotic function is `text_cleaner`, the main idea is to remove stopwords, punctuation and white spaces. Ultimately, the function returns a list of terms per case. This practise is not tidy *yet*. Another convenience function `list_to_vec`,an imitation of [concatenating text values in pivot tables](https://www.mrexcel.com/excel-tips/pivot-table-with-text-in-values-area) in excel. Finally, the `completeness` function checks the percentage of missing values within a column. The function is intended to aid in checking the viability of a column in the dataset. Here, columns with missing value percentage \> 5% are excluded from the dataset.

These functions are applied through `purrr::map` a tidyverse equivalent to `base::lapply`, along with `dplyr::mutate` and `dplyr::across` to implement the functions consistently.

Another important task of the cleaning the data is the conversion of currencies to USD for non-USD salary values. This way, we can get closer to comparing apples with apples. The International Monetary Fund's [Exchange Rate Report Wizard](https://www.imf.org/external/np/fin/ert/GUI/Pages/CountryDataBase.aspx) which allows users to exchange rates for multiple time ranges. Importantly, the Hong Kong Dollar is not listed in the report. As such, salaries reported in that currency are not included in the analysis.

The final preprocessing step relies on the `tidymodels` and @hvitfeldt2022 packages to specify a recipe to tokenize the values derived from `list_to_vec`,extract their TFIDF (Term Frequency-Inverse Document Frequency) of the tokens, log the salary values and remove near-zero variance variables.

Finally, the resulting data.frame is used in LASSO regression, which serves as our feature selection step.

```{r Libraries_and_Preprocessing}
#| label: Lasso Regression Plot
#| fig-cap: An image containing a Lasso Regression plot indicating coeffiecients
source("./Scripts/2022_Data_Cleaner.R",
       local = knitr::knit_global())
```

Ultimately, we extract the variables of interest from the lasso model through the function below. The function is an improvement on the previous post, as it does rely on converting each matrix into a data.frame. In this instance, the function subsets the last model and minimum lambda value from the matrices which are in turn converted to a data.frame. The function is similar to Stata17's [lassocoef](https://www.stata.com/features/overview/lasso-model-selection-prediction/) or SPSS29's Best option in [Linear Lasso Regression: Option](https://www.ibm.com/docs/en/spss-statistics/29.0.0?topic=regression-linear-lasso-options). The script outputs training and testing outputs for further analysis. In this way, the preprocessing section is isolated to a single script and negates the need to read-in data.

```{r Variable_Extraction}
#| eval: false
#| echo: true
#| label: Variable Extraction function
#| fig-cap: An R function specifying a process for extracting variables from a lasso model
variable_extractor <- function(a_list){
  min_lambda <- a_list$lambda |> min()
  last_model <- length(a_list$lambda)
  lasso_variables <- as.matrix(coef(a_list))[,last_model] |> 
    data.frame()
  lasso_variables$variable <- rownames(lasso_variables)
  names(lasso_variables) <- c("coef","variable")
  lasso_variables <- lasso_variables[,c("variable","coef")]
  lasso_variables <- lasso_variables[!grepl(pattern = "[[:punct:]]Intercept[[:punct:]]",
                                            lasso_variables$variable),]
return(lasso_variables)}

Salary_Modelling <- cbind(Salary_Modelling[,grep("full_comp_usd",names(Salary_Modelling))],
Salary_Modelling[,names(Salary_Modelling) %in% variable_extractor(Lasso_Model)$variable])
```

# DATA MODELLING

With our training and testing datasets in hand, we can explore some modelling options. Our outcome variable is `full_comp_usd`, a numeric variable, containing the log value of the reported USD compensation. Naturally, linear regression is a candidate model. It may also be useful to compare our linear regression model against some non-linear candidates, here we also fit a Multivariate Adaptive Regression Splines (MARS) and Keras regression models.

## OLS MODEL: SWISS KNIFE APPROACH

```{r Linear_Model}
#| label: Ordinary Least Squares Regression Results
#| fig-cap: OLS Regression Results illustrating an adjusted r-square of 0.15 on 11237 cases and global p value of < 0.001
ols_model <- lm(full_comp_usd ~.,data = Training_Salary)
library(gtsummary)

tbl_regression(ols_model,
               add_estimate_to_reference_rows = TRUE,
               exponentiate = FALSE) |> 
  add_glance_source_note(label = list(
    r.squared ~ "R2",
    adj.r.squared ~ "Adj R2",
    p.value ~ "p-value",
    statistic ~ "F-Statistic",
    df ~ "Degrees of Freedom",
    df.residual ~ "Residual df"),
    include = c(r.squared,adj.r.squared,p.value,df,statistic,
                df.residual)
    ) |> 
  italicize_labels() 
```

Fitting a linear regression model is rather straight forward. The model above yields an Adjusted R Square of 0.153. In other words, the model explains 15% of the variance in salaries. The beta, confidence interval and p-values of the variables considered are listed on the table. One way to report the aforementioned results would be to utilise the `report` package which can aid in reporting models in standard manner including beta values, standard error, confidence intervals etc. for the respective variables considered in the model.

## MARS MODEL: SPLINES AND LINES

Below, we build a MARS model and iterate through 10 cross-validation folds. The resulting models, does not out perform the OLS model. The R-Square average R-Square for the MARS model is 0.15.

```{r MARS_Model}
#| label: MARS Model Results
#| fig-cap: MARS Model Results indicating a selected model with an R-Square of 0.15
#| fig-height: 8.93701
#| fig-width: 8.14961

library(earth)

MARS_Model <- earth(full_comp_usd ~.,Training_Salary,
      degree = 2,
      glm = list(family = "gaussian"),
      pmethod = "exhaustive",
      nfold = 10,
      ncross= 10,
      varmod.method = "earth")

Fold_Rsquares <- lapply(MARS_Model$cv.list,\(x){x["rsq"]})

Fold_Rsquares <- lapply(Fold_Rsquares,data.frame)

theme_set(ggthemes::theme_solarized_2())

map2(Fold_Rsquares,names(Fold_Rsquares),function(x,y){
  x |> 
    mutate(fold_source =y)
})%>%
  do.call(bind_rows,.) |> 
  mutate(iteration = str_extract(fold_source,"\\d{1,2}$") |> as.integer(),
         fold_source = str_remove(fold_source,"[[:punct:]]\\d{1,}") |> as.factor(),
         fold_source = fct_reorder(fold_source,iteration)) |> 
  ggplot(aes(iteration,rsq,group=fold_source))+
  geom_line(aes(colour = fold_source),show.legend = FALSE)+
  scale_colour_manual(values = c("fold1" = "#0F204B",
                                 "fold2"= "#A71930",
                                 "fold3" = "#000000",
                                 "fold4" = "#9E83B7",
                                 "fold5"="#00B140",
                                 "fold6" = "#BB133E",
                                 "fold7"= "#490E6F",
                                 "fold8"= "#0039A7",
                                 "fold9"= "#00675A",
                                 "fold10"= "#EA8400"))+
  scale_x_continuous(breaks = c(1:10))+
  facet_wrap(~fold_source,
             ncol = 2,
             nrow=5, 
             scales = "free")
```

There are several ways to improve the model. Such as including factor variables in place of dummy variables, utilising polynomial regression in place of the gaussian family among others. For a more detailed account on how to work with enhance MARS models, see @milborrow .

```{r MARS_Variables}
#| label: MARS Model Results Visualisation
#| fig-cap: MARS Model Results Visualisation containing variables and their relationships retrieved through the plotmo package. 
#| fig-height: 8.93701
#| fig-width: 8.14961
plotmo(MARS_Model)
```

## KERAS: GUN TO A KNIFE FIGHT

We haven't had a significant difference between the OLS model and MARS model. Below, we use build a @allaire2022 regression model. It is mostly likely a computationally inefficient approach but worth exploring nonetheless. Fortunately, with a few changes to @chollet2022 code , we can fit a regression model through Keras in R. Below, we change the data to matrix format suitable for the type of model we build. Thereafter, we fit it through 500 epochs and visualise the Mean Absolute Error and Loss (Mean Squared Error) of the model.

```{r Random_Forest_Model}
#|fig-cap: Specifying a Keras model for regression
library(keras)
train_data <- model.matrix(full_comp_usd ~.,Training_Salary)[,-1]
train_targets <- Training_Salary$full_comp_usd
build_model <- function() {
model <- keras_model_sequential() %>%
layer_dense(64, activation = "relu") %>%
layer_dense(64, activation = "relu") %>%
layer_dense(1)
model %>% compile(optimizer = "rmsprop",
loss = "mse",
metrics = "mae")
model
}
```

```{r Keras_Model}
#|echo: true
#|fig-cap: Fitting a regression Keras model 
#|include: false
k <- 4
fold_id <- sample(rep(1:k, length.out = nrow(train_data)))
num_epochs <- 100
all_scores <- numeric()
for (i in 1:k) {
cat("Processing fold #", i, "\n")
val_indices <- which(fold_id == i)
val_data <- train_data[val_indices, ]
val_targets <- train_targets[val_indices]
partial_train_data <- train_data[-val_indices, ]
partial_train_targets <- train_targets[-val_indices]
model <- build_model()
model %>% fit(
partial_train_data,
partial_train_targets,
epochs = num_epochs,
batch_size = 16,
verbose = 1
)
results <- model %>% evaluate(val_data,
val_targets, verbose = 0)
all_scores[[i]] <- results[['mae']]
}
```

```{r Historical_Performance}
#| label: MAE and LOSS through the epochs
num_epochs <- 100
all_mae_histories <- list()
for (i in 1:k) {
cat("Processing fold #", i, "\n")
val_indices <- which(fold_id == i)
val_data <- train_data[val_indices, ]
val_targets <- train_targets[val_indices]
partial_train_data <- train_data[-val_indices, ]
partial_train_targets <- train_targets[-val_indices]
model <- build_model()
history <- model %>% fit(
partial_train_data, partial_train_targets,
validation_data = list(val_data, val_targets),
epochs = num_epochs, batch_size = 16, verbose = 1
)
mae_history <- history$metrics$val_mae
all_mae_histories[[i]] <- mae_history
}

plot(history)
```

We know the three model's performance metrics, the next question we need to answer is whether the data maintains the similar results on an out-of-sample dataset. Here we rely on @kuhn2022, `yardstick` package to compare performance between the models. It is probably a better approach to use a tidymodel's workflow and recipes as it provides all the functions needed for preprocessing and modelling.

# FINAL COMPARISON

The results below illustrate the Mean Absolute Percentage Error, a measure of the percentage difference between predicted value and the actual value. Ideally, we want a low MAPE estimate; as it indicates better predictions. There are marginal difference in MAPE values across the three models.

```{r Testing_Performance}
#| label: MAPE Results
#| fig-cap: A table containing MAPE results across the three models.
ols_prediction <- predict(ols_model,Testing_Salary)
ols_results <- data.frame(truth = Testing_Salary$full_comp_usd,
           estimate = ols_prediction) |> 
  mutate(model = "OLS Model")

mars_prediction <- predict(MARS_Model,Testing_Salary)
mars_results <- data.frame(truth = Testing_Salary$full_comp_usd,
                           estimate = mars_prediction)

test_predictors <- model.matrix(full_comp_usd ~.,data = Testing_Salary)[,-1]
test_outcome <- Testing_Salary$full_comp_usd

result <- predict(model,test_predictors)

keras_results <- data.frame(truth = test_outcome,
           estimate = result) |> 
  mutate(model = "KERAS Model")


MAPE_Data <- bind_rows(mape(ols_results,
               truth = truth,
               estimate = estimate) |> 
  mutate(model = "OLS Model"),
mape(mars_results,
    truth = truth,
    estimate = full_comp_usd) |> 
  mutate(model = "MARS Model")) |> 
  bind_rows(mape(keras_results,
    truth = truth,
    estimate = estimate) |> 
  mutate(model = "KERAS Model"))

gt(MAPE_Data |> 
     mutate(.estimate = round(.estimate,3)))
```

We are done squeezing water out of a rock. What can we conclude? There are better feature engineering approaches to explore. These include log-ratios in place of TFIDF, enhancing the dataset by attaching weights to industry, location and other aspects in the dataset. [Julia Silge's](https://juliasilge.com/blog/uk-museums/) recent blog post on high cardinality predictors may also be helpful in this regard. Alternatively, a classification model may serve as a better model to analyse the difference in pay from the dataset.
