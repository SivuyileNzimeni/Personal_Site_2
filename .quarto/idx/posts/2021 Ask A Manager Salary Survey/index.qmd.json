{"title":"Ask A Manager Survey","markdown":{"yaml":{"title":"Ask A Manager Survey","image":"./Ask_A_Manager_Logo.png","description":"This post is about the 2021 Ask A Manager Survey analysis. It provides an overview of the data analysis process attempting to fit a linear regression model to explain variances in reported salaries among the respondents","date":"December 25, 2021","author":"Sivuyile Nzimeni","categories":["data analysis","data modelling"],"execute":{"echo":true,"warning":false}},"headingText":"INTRODUCTION","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE,\n                      message = FALSE,\n                      warning = FALSE)\nlapply(as.list(c(\"tidyverse\",\"janitor\",\"tidymodels\",\"glmnet\",\n                 \"gt\",\"gtExtras\",\"gtsummary\")),\n       require,character.only=TRUE)\nset_gtsummary_theme(theme_gtsummary_journal(journal = \"qjecon\"))\n```\n\n\nIn early 2021, the [Ask A Manager](https://www.askamanager.org/) blogsite ran their annual [salary survey](https://www.askamanager.org/2021/05/look-at-24000-peoples-real-life-salaries.html). The survey responses are stored on googlesheets, making the data accessible to all interested. In a [previous post](https://www.linkedin.com/pulse/2021-ask-manager-salary-survey-sivuyile-nzimeni/), we discussed data pre-processing and feature selection method. This post focuses two aspects,namely, 1) Reproducibility and 2) Out-of-Sample Testing.\n\n## REPRODUCIBILITY\n\nIn the previous post, we detailed the feature selection method by regularised regreesion, specifically, LASSO regression. In this post, we will attempt to reproduce the feature selection method.\n\n## OUT OF SAMPLE TESTING\n\nIn this post, we will also follow the traditional machine learning workflow including, splitting the data into a training and testing samples, fitting a model, cross-validation and finally fitting the model on the out-of-sample dataset(testing dataset). This approach can inform us about the model's parsimony. In other words, can the model perform well on an unknown sample.\n\n```{r Import_Data}\nManager_Salaries <- read_csv(file = \"./2021_Cleaned_Job_Data.csv\")\n```\n\n```{r Training_Split}\nManager_Salaries <- recipe(new_annual_salary~.,Manager_Salaries) %>% \n  step_nzv(all_predictors()) %>% \n  prep() %>% \n  bake(Manager_Salaries)\n\nSalary_Split <- initial_split(na.omit(Manager_Salaries))\nSalary_Train <- training(Salary_Split)\nSalary_Test <- testing(Salary_Split)\n```\n\n# LASSO REGRESSION\n\nAs mentioned above, the data cleaning process was detailed on an earlier post. It is worth highlighting that the dataset contained several dummy variables indicating various respondent attributes such as job_title, industry, city, age group etc. Our dependent variable is the annual salary. Given the large number of independent variables. In the code below, we fit a LASSO regression model.\n\n```{r LASSO_Definition,echo=TRUE}\nX <- model.matrix(new_annual_salary ~.,Salary_Train)[,-1]\nY <- log(Salary_Train$new_annual_salary)\n\nlasso <- glmnet(x=X,\n       y= Y,\n       alpha = 1)\n```\n\n## IMPORTANT VARIABLE EXTRACTION\n\nThe resulting object is a list of class \"glmnet\". Essentially, the results contain all the iterations through alpha and to find the minimum lambda. As such, there are several results in the object. We are primarily interested in extracting the remaining variables at minimum lambda along with their coeffiecients. The [vip package](https://cran.r-project.org/web/packages/vip/vip.pdf) can automate the plotting of the results. However, we want to extract the variables names in order to fit them on our training test. There is probably a package to assist with this step somewhere in the wild, however, we haven't found it yet. Luckily in R, we can write custom functions. The code below details the variable_extractor function.\n\n```{r Extract_Variables,echo=TRUE}\nvariable_extractor <- function(a_list){\n  min_lambda <- data.frame(best_lambda =a_list[[\"lambda\"]]==min(a_list[[\"lambda\"]]))\n  min_lambda <- cbind.data.frame(data.frame(index = rownames(min_lambda)),\n                                 min_lambda)\n  min_lambda <- min_lambda %>% \n    filter(best_lambda == TRUE)\n  min_lambda <- as.double(unique(min_lambda$index))\n  lasso_variables <- as.matrix(coef(a_list))|>data.frame()\n  lasso_variables <- cbind.data.frame(variables = rownames(lasso_variables),\n                                      lasso_variables)\n  lasso_variables <- tibble(lasso_variables)\n  lasso_variables <- lasso_variables[,c(1,min_lambda+1)]\n  names(lasso_variables) <- c(\"variable\",\"importance\")\n  lasso_variables <- lasso_variables %>% \n    filter(variable != \"(Intercept)\",\n           importance != 0.00000000) %>% \n    arrange(desc(importance))\n\n  return(lasso_variables)\n}\n\nlasso_variable <- variable_extractor(lasso)\n```\n\nThe function takes a list of attribute \"glmnet\". Thereafter, we find the smallest lambda. In addition, we extract all the coefficients from the object and store them as a wide dataframe. The dataframe is subset to only contain the variable name along with the coefficients of the smallest lambda value. We discard the intercept and variable with coefficients that are equal to 0.0000000. The final output is identical to the variables extracted by the vip package. Finally, we subset both the training dataset and the testing dataset to only contain the selected independent variables. Since the outcome variable is expressed in USD terms, we log the outcome variable.\n\n```{r more_cleaning}\nSalary_Train <- cbind.data.frame(Salary_Train$new_annual_salary,\nSalary_Train %>% \n  select(lasso_variable$variable))\nSalary_Test <- cbind.data.frame(Salary_Test$new_annual_salary,\n                                Salary_Test %>% \n                                  select(lasso_variable$variable))\n\nnames(Salary_Train)[[1]] <- \"new_annual_salary\"\nnames(Salary_Test)[[1]] <- \"new_annual_salary\"\nSalary_Train$new_annual_salary <- log(Salary_Train$new_annual_salary)\nSalary_Test$new_annual_salary <- log(Salary_Test$new_annual_salary)\n```\n\n## TIDYMODELS: A CLEAN INTERFACE FOR MACHING LEARNING\n\nThe R programming language doesn't not lack methods for running machine learning algorithms, it is after all, a statistical programming language. The [tidymodels metapackage](https://www.tidymodels.org/) aims to provide a standard interface for modelling and machine learning using tidyverse principles. Below, we use the package to complete a number of steps including, crossfold_validation, model specification, fitting on the resamples and finally fitting the model on the training dataset.\n\n```{r Salary_Folds,echo=TRUE}\nSalary_Folds <- vfold_cv(Salary_Train)\n\nlm_spec <- linear_reg(engine = \"lm\")\n\nlm_recipe <- recipe(new_annual_salary ~.,Salary_Train) %>% \n  step_nzv(all_predictors())\n\nlm_wf <- workflow(lm_recipe,lm_spec)\n\ndoParallel::registerDoParallel(cores = 10)\nctrl_preds <- control_resamples(save_pred = TRUE)\ncv_results <- fit_resamples(lm_wf,Salary_Folds,control = ctrl_preds)\n\nlm_wf <- fit(lm_wf,Salary_Train)\n\ncollect_metrics(cv_results,summarize = FALSE) %>% \n  filter(.metric == \"rsq\") %>% \n  summarise(avg_estimate = mean(.estimate),\n            .groups = \"drop\")\n```\n\nAcross all 10 validation folds, the linear regression model's adjusted R-square averaged 0.272. It is possible to tune the parameters and use battery of other machine learning models to improve performance. In the previous post, we utilised random forest to try an improve the model. Despite, requiring additional computational power, the increases in peformance were marginal to neglible.\n\nAt this point, we have completed our first objective. We are able to reproduce the LASSO regresison results in the previous post. The next objective is to determine the parsimony of the model. Here, we fit model on the test data.\n\n```{r fit_on_sets}\nTest_Summary <- lm(new_annual_salary~.,data=Salary_Test)\nTraining_Summary <- lm(new_annual_salary ~.,data=Salary_Train)\n```\n\n```{r Comparison_Table}\nTraining_Model <- gtsummary::tbl_regression(Training_Summary,add_estimate_to_reference_rows=TRUE,\n                          intercept = TRUE) %>% \n  as_gt()\nTesting_Model <- tbl_regression(Test_Summary,add_estimate_to_reference_rows=TRUE,\n                                intercept=TRUE) %>% \n  as_gt()\n```\n\n```{r to_gt_train}\nTraining_Model <- Training_Model %>%\n  gt_theme_nytimes() %>% \n  tab_header(title= \"Linear Regression Model: Training Dataset\",\n             subtitle = \"Depedent Variable: log(Annual Salary(USD))\") %>% \n  gt::tab_source_note(\"Adjusted R-square: 0.2732\") %>% \n  gt::opt_align_table_header(align = \"center\")\n```\n\n```{r to_gt_test}\nTesting_Model <- Testing_Model %>% \n  gt_theme_nytimes() %>% \n  tab_header(title= \"Linear Regression Model: Test Dataset\",\n             subtitle = \"Depedent Variable: log(Annual Salary(USD))\") %>% \n  gt::tab_source_note(\"Adjusted R-square: 0.2624\") %>% \n  gt::opt_align_table_header(align = \"center\")\n```\n\n# RESULTS\n\n### Training Dataset\n\n```{r Training_Mod}\nTraining_Model\n```\n\n### Test Dataset\n\n```{r Test_mod}\nTesting_Model\n```\n\n# CONCLUSION\n\nThe model performs well on an out-of-sample dataset with year of experience (21 - 30 years), job title (Director) and highest level of education (PhD) being among the most important predictors of annual salary. Provided the questions utilised in 2022 are similar or identical to the 2021 survey, the model above can be evaluated on the 2022 survey results.\n\n# REFERENCES\n\nSilge,J. 2021.Fit and predict with tidymodels for #TidyTuesday bird baths in Australia. Available From: https://juliasilge.com/blog/bird-baths/ (Accessed 24 December 2021).\n\nWickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686\n\nSam Firke (2021). janitor: Simple Tools for Examining and Cleaning Dirty Data. R package version 2.1.0. https://CRAN.R-project.org/package=janitor\n\nKuhn et al., (2020). Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org\n\nJerome Friedman, Trevor Hastie, Robert Tibshirani (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22. https://www.jstatsoft.org/v33/i01/.\n\nRichard Iannone, Joe Cheng and Barret Schloerke (2021). gt: Easily Create Presentation-Ready Display Tables. R package version 0.3.1. https://CRAN.R-project.org/package=gt\n\nThomas Mock (2021). gtExtras: A Collection of Helper Functions for the gt Package. R package version 0.2.2.11. https://github.com/jthomasmock/gtExtras\n\nSjoberg DD, Whiting K, Curry M, Lavery JA, Larmarange J. Reproducible summary tables with the gtsummary package. The R Journal 2021;13:570--80. https://doi.org/10.32614/RJ-2021-053.\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE,\n                      message = FALSE,\n                      warning = FALSE)\nlapply(as.list(c(\"tidyverse\",\"janitor\",\"tidymodels\",\"glmnet\",\n                 \"gt\",\"gtExtras\",\"gtsummary\")),\n       require,character.only=TRUE)\nset_gtsummary_theme(theme_gtsummary_journal(journal = \"qjecon\"))\n```\n\n# INTRODUCTION\n\nIn early 2021, the [Ask A Manager](https://www.askamanager.org/) blogsite ran their annual [salary survey](https://www.askamanager.org/2021/05/look-at-24000-peoples-real-life-salaries.html). The survey responses are stored on googlesheets, making the data accessible to all interested. In a [previous post](https://www.linkedin.com/pulse/2021-ask-manager-salary-survey-sivuyile-nzimeni/), we discussed data pre-processing and feature selection method. This post focuses two aspects,namely, 1) Reproducibility and 2) Out-of-Sample Testing.\n\n## REPRODUCIBILITY\n\nIn the previous post, we detailed the feature selection method by regularised regreesion, specifically, LASSO regression. In this post, we will attempt to reproduce the feature selection method.\n\n## OUT OF SAMPLE TESTING\n\nIn this post, we will also follow the traditional machine learning workflow including, splitting the data into a training and testing samples, fitting a model, cross-validation and finally fitting the model on the out-of-sample dataset(testing dataset). This approach can inform us about the model's parsimony. In other words, can the model perform well on an unknown sample.\n\n```{r Import_Data}\nManager_Salaries <- read_csv(file = \"./2021_Cleaned_Job_Data.csv\")\n```\n\n```{r Training_Split}\nManager_Salaries <- recipe(new_annual_salary~.,Manager_Salaries) %>% \n  step_nzv(all_predictors()) %>% \n  prep() %>% \n  bake(Manager_Salaries)\n\nSalary_Split <- initial_split(na.omit(Manager_Salaries))\nSalary_Train <- training(Salary_Split)\nSalary_Test <- testing(Salary_Split)\n```\n\n# LASSO REGRESSION\n\nAs mentioned above, the data cleaning process was detailed on an earlier post. It is worth highlighting that the dataset contained several dummy variables indicating various respondent attributes such as job_title, industry, city, age group etc. Our dependent variable is the annual salary. Given the large number of independent variables. In the code below, we fit a LASSO regression model.\n\n```{r LASSO_Definition,echo=TRUE}\nX <- model.matrix(new_annual_salary ~.,Salary_Train)[,-1]\nY <- log(Salary_Train$new_annual_salary)\n\nlasso <- glmnet(x=X,\n       y= Y,\n       alpha = 1)\n```\n\n## IMPORTANT VARIABLE EXTRACTION\n\nThe resulting object is a list of class \"glmnet\". Essentially, the results contain all the iterations through alpha and to find the minimum lambda. As such, there are several results in the object. We are primarily interested in extracting the remaining variables at minimum lambda along with their coeffiecients. The [vip package](https://cran.r-project.org/web/packages/vip/vip.pdf) can automate the plotting of the results. However, we want to extract the variables names in order to fit them on our training test. There is probably a package to assist with this step somewhere in the wild, however, we haven't found it yet. Luckily in R, we can write custom functions. The code below details the variable_extractor function.\n\n```{r Extract_Variables,echo=TRUE}\nvariable_extractor <- function(a_list){\n  min_lambda <- data.frame(best_lambda =a_list[[\"lambda\"]]==min(a_list[[\"lambda\"]]))\n  min_lambda <- cbind.data.frame(data.frame(index = rownames(min_lambda)),\n                                 min_lambda)\n  min_lambda <- min_lambda %>% \n    filter(best_lambda == TRUE)\n  min_lambda <- as.double(unique(min_lambda$index))\n  lasso_variables <- as.matrix(coef(a_list))|>data.frame()\n  lasso_variables <- cbind.data.frame(variables = rownames(lasso_variables),\n                                      lasso_variables)\n  lasso_variables <- tibble(lasso_variables)\n  lasso_variables <- lasso_variables[,c(1,min_lambda+1)]\n  names(lasso_variables) <- c(\"variable\",\"importance\")\n  lasso_variables <- lasso_variables %>% \n    filter(variable != \"(Intercept)\",\n           importance != 0.00000000) %>% \n    arrange(desc(importance))\n\n  return(lasso_variables)\n}\n\nlasso_variable <- variable_extractor(lasso)\n```\n\nThe function takes a list of attribute \"glmnet\". Thereafter, we find the smallest lambda. In addition, we extract all the coefficients from the object and store them as a wide dataframe. The dataframe is subset to only contain the variable name along with the coefficients of the smallest lambda value. We discard the intercept and variable with coefficients that are equal to 0.0000000. The final output is identical to the variables extracted by the vip package. Finally, we subset both the training dataset and the testing dataset to only contain the selected independent variables. Since the outcome variable is expressed in USD terms, we log the outcome variable.\n\n```{r more_cleaning}\nSalary_Train <- cbind.data.frame(Salary_Train$new_annual_salary,\nSalary_Train %>% \n  select(lasso_variable$variable))\nSalary_Test <- cbind.data.frame(Salary_Test$new_annual_salary,\n                                Salary_Test %>% \n                                  select(lasso_variable$variable))\n\nnames(Salary_Train)[[1]] <- \"new_annual_salary\"\nnames(Salary_Test)[[1]] <- \"new_annual_salary\"\nSalary_Train$new_annual_salary <- log(Salary_Train$new_annual_salary)\nSalary_Test$new_annual_salary <- log(Salary_Test$new_annual_salary)\n```\n\n## TIDYMODELS: A CLEAN INTERFACE FOR MACHING LEARNING\n\nThe R programming language doesn't not lack methods for running machine learning algorithms, it is after all, a statistical programming language. The [tidymodels metapackage](https://www.tidymodels.org/) aims to provide a standard interface for modelling and machine learning using tidyverse principles. Below, we use the package to complete a number of steps including, crossfold_validation, model specification, fitting on the resamples and finally fitting the model on the training dataset.\n\n```{r Salary_Folds,echo=TRUE}\nSalary_Folds <- vfold_cv(Salary_Train)\n\nlm_spec <- linear_reg(engine = \"lm\")\n\nlm_recipe <- recipe(new_annual_salary ~.,Salary_Train) %>% \n  step_nzv(all_predictors())\n\nlm_wf <- workflow(lm_recipe,lm_spec)\n\ndoParallel::registerDoParallel(cores = 10)\nctrl_preds <- control_resamples(save_pred = TRUE)\ncv_results <- fit_resamples(lm_wf,Salary_Folds,control = ctrl_preds)\n\nlm_wf <- fit(lm_wf,Salary_Train)\n\ncollect_metrics(cv_results,summarize = FALSE) %>% \n  filter(.metric == \"rsq\") %>% \n  summarise(avg_estimate = mean(.estimate),\n            .groups = \"drop\")\n```\n\nAcross all 10 validation folds, the linear regression model's adjusted R-square averaged 0.272. It is possible to tune the parameters and use battery of other machine learning models to improve performance. In the previous post, we utilised random forest to try an improve the model. Despite, requiring additional computational power, the increases in peformance were marginal to neglible.\n\nAt this point, we have completed our first objective. We are able to reproduce the LASSO regresison results in the previous post. The next objective is to determine the parsimony of the model. Here, we fit model on the test data.\n\n```{r fit_on_sets}\nTest_Summary <- lm(new_annual_salary~.,data=Salary_Test)\nTraining_Summary <- lm(new_annual_salary ~.,data=Salary_Train)\n```\n\n```{r Comparison_Table}\nTraining_Model <- gtsummary::tbl_regression(Training_Summary,add_estimate_to_reference_rows=TRUE,\n                          intercept = TRUE) %>% \n  as_gt()\nTesting_Model <- tbl_regression(Test_Summary,add_estimate_to_reference_rows=TRUE,\n                                intercept=TRUE) %>% \n  as_gt()\n```\n\n```{r to_gt_train}\nTraining_Model <- Training_Model %>%\n  gt_theme_nytimes() %>% \n  tab_header(title= \"Linear Regression Model: Training Dataset\",\n             subtitle = \"Depedent Variable: log(Annual Salary(USD))\") %>% \n  gt::tab_source_note(\"Adjusted R-square: 0.2732\") %>% \n  gt::opt_align_table_header(align = \"center\")\n```\n\n```{r to_gt_test}\nTesting_Model <- Testing_Model %>% \n  gt_theme_nytimes() %>% \n  tab_header(title= \"Linear Regression Model: Test Dataset\",\n             subtitle = \"Depedent Variable: log(Annual Salary(USD))\") %>% \n  gt::tab_source_note(\"Adjusted R-square: 0.2624\") %>% \n  gt::opt_align_table_header(align = \"center\")\n```\n\n# RESULTS\n\n### Training Dataset\n\n```{r Training_Mod}\nTraining_Model\n```\n\n### Test Dataset\n\n```{r Test_mod}\nTesting_Model\n```\n\n# CONCLUSION\n\nThe model performs well on an out-of-sample dataset with year of experience (21 - 30 years), job title (Director) and highest level of education (PhD) being among the most important predictors of annual salary. Provided the questions utilised in 2022 are similar or identical to the 2021 survey, the model above can be evaluated on the 2022 survey results.\n\n# REFERENCES\n\nSilge,J. 2021.Fit and predict with tidymodels for #TidyTuesday bird baths in Australia. Available From: https://juliasilge.com/blog/bird-baths/ (Accessed 24 December 2021).\n\nWickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686\n\nSam Firke (2021). janitor: Simple Tools for Examining and Cleaning Dirty Data. R package version 2.1.0. https://CRAN.R-project.org/package=janitor\n\nKuhn et al., (2020). Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org\n\nJerome Friedman, Trevor Hastie, Robert Tibshirani (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22. https://www.jstatsoft.org/v33/i01/.\n\nRichard Iannone, Joe Cheng and Barret Schloerke (2021). gt: Easily Create Presentation-Ready Display Tables. R package version 0.3.1. https://CRAN.R-project.org/package=gt\n\nThomas Mock (2021). gtExtras: A Collection of Helper Functions for the gt Package. R package version 0.2.2.11. https://github.com/jthomasmock/gtExtras\n\nSjoberg DD, Whiting K, Curry M, Lavery JA, Larmarange J. Reproducible summary tables with the gtsummary package. The R Journal 2021;13:570--80. https://doi.org/10.32614/RJ-2021-053.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","editor":"visual","code-copy":true,"code-summary":"see code","theme":{"light":"materia","dark":"darkly"},"title":"Ask A Manager Survey","image":"./Ask_A_Manager_Logo.png","description":"This post is about the 2021 Ask A Manager Survey analysis. It provides an overview of the data analysis process attempting to fit a linear regression model to explain variances in reported salaries among the respondents","date":"December 25, 2021","author":"Sivuyile Nzimeni","categories":["data analysis","data modelling"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}