{"title":"Spam Emails","markdown":{"yaml":{"title":"Spam Emails","lang":"en-GB","date":"August 18,2023","author":"Sivuyile Nzimeni","image":"./Spam_Detection.png","image-alt":"An image illustrating an email folder classified as either spam or not.","description":"This post is about building a classification model for spam email detection. We explore a solution for handling an imbalanced dataset.","categories":["data modelling","data cleaning","visualisation"],"fig-dpi":300,"fig-align":"center","fig-cap-location":"top","code-copy":true,"code-line-numbers":true,"page-layout":"article","cap-location":"margin","number-sections":true,"toc":true,"lot":true,"lof":true,"toc-title":"CONTENTS","execute":{"echo":true,"warning":false,"message":false}},"headingText":"INTRODUCTION","containsRefs":false,"markdown":"\n\n\nThe [TidyTuesday](https://github.com/rfordatascience/tidytuesday) project is a weekly social data project. Each week an interesting dataset is shared with the community for members to clean, model and or visualise data. People are encouraged to share their code and posts through social media. In essence, it serves as good avenue to explore one's data skills and to learn with others. This week (2023/08/14), the dataset is comprised of spam and non-spam emails originating from [UCI Machine Learning Repository](http://archive.ics.uci.edu/dataset/94/spambase).\n\n## IMPORT DATA\n\n```{r Libraries}\n#| message: false\n#| warning: false\n#| include: true\n\nlapply(c(\"tidyverse\",\"janitor\",\"ggthemes\",\n         \"glmnet\",\"tidymodels\",\"vip\",\"ranger\",\n         \"themis\",\"gt\",\"gtExtras\"),\n       require,character.only = TRUE) |> \n  suppressWarnings() |> \n  suppressMessages() |> \n  invisible() # <1>\n\nEmails <- read.csv(file = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-08-15/spam.csv\") |> \n  clean_names() # <2>\n\ntheme_set(theme_minimal()) # <3>\nset.seed(2023)\n```\n\n1.  We load our preferred R packages. This style of loading packages is not standard. It helps to silence some of the messages and conflict warnings when loading meta packages such as `tidyverse` and `tidymodels`.\n\n2.  We import data from the tidytuesday github repository. Again, the use of `read.csv` in place of `read_csv` or other functions boils down to preference.\n\n3.  The `theme_set` function lets us select a theme for our visualisations.\n\nWe also import a number of auxilary packages to assist in the analysis. The imported data is relatively small only comprising of 4601 rows and 7 columns. Our outcome variable is `yesno`,i.e. Spam/Not Spam email. The remaining variables contains derived variables, frequency counts. These are essentials tallies of references to money, currency and exclamation marks (oddly named `bang`). The dataset presents a classification problem. We start by scaling the numeric variables.\n\n```{r Data_Preprocessing}\n#| label: Scale Numeric Variables\n#| include: true\n#| echo: true\n\nEmails[,-grep(\"yesno\",names(Emails))] <-  sapply(Emails[,sapply(Emails,is.numeric)],\n       function(x){\n         scale(x,center = TRUE,scale = TRUE) \n       }) # <1>\n```\n\n1.  Use `<-` as your separator between the right-hand-side and the left-hand-side of the code. On the left-hand-side, we select all variables except the target variable except the outcome variable. On the right-hand-side, we use a nested `sapply` function to subset only for predictor variables. Thereafter, we subset columns that are of type numeric. Finally, we call an anonymous function via `function(x)` and pass each variable through then scale function.\n\nIt appears convoluted. However, it completes a set of sub-setting and applies functions simultaneously. It is better practice to first define a function and sapply.\n\n## DATA SPLITTING\n\nPart of the data splitting involves some decisions about whether to stratify the outcome variable. Especially with an imbalanced dataset, it is important to have sufficient representation of the minority cases without leaking the outcomes in the training process. Next, we can do some data splitting into a training and testing set, stratified over the outcome variable. The `rsample` package serves a set of convenient function for the task. The majority of emails (60.56%) are not spam.\n\n```{r Data_Prep}\n#| label: Data Splitting\n\nData_Split <- initial_split(Emails,strata=yesno)\nData_Train <- training(Data_Split)\nData_Test <- testing(Data_Split)\n\n100/nrow(Data_Train)*table(Data_Train$yesno)\n```\n\n## MODEL FITTING\n\nTo\n\n```{r GLMNET}\nX_Train <- model.matrix(yesno ~.,data =Data_Train)[,-1]\nY_Train <- as.numeric(Data_Train$yesno==\"y\")\n\n\nRanger_Model <- ranger(x=X_Train,\n       y = Y_Train,\n       verbose = TRUE,\n       importance = \"impurity\",\n       splitrule = \"gini\",\n       seed = 2023,\n       classification = TRUE)\n\nRanger_Model$confusion.matrix \n```\n\n```{r Elastic_Model_Fitting}\nElastic_Net <- cv.glmnet(x=X_Train,\n                      y=Y_Train,\n                      alpha =0,\n                      family = \"binomial\",\n                      nfolds = 10,\n                      type.measure = \"class\")\n```\n\n```{r Train_Performance}\nTraining_Performance <- assess.glmnet(Elastic_Net,\n              newx=X_Train,\n              newy=Y_Train)\n\nFinal_Model <- glmnet(x=X_Train,\n                      y = Y_Train,\n                      lambda = min(Elastic_Net$lambda),\n                      alpha = 0,\n                      family = \"binomial\",\n                      standardize = FALSE)\n\n\nTraining_Performance <- data.frame(lapply(Training_Performance,unlist))\n\nrownames(Training_Performance) <- NULL\n\nTraining_Performance |> \n  gt() |> \n  tab_header(title = \"2023 Initial ElasticNet Model\",\n             subtitle = \"Training Model Performance\") |> \n  gt_theme_espn()\n```\n\n# REBALANCING OUTCOME\n\n```{r New_Recipes}\nData_Clean <- recipe(yesno ~.,data = Data_Train) |> \n  themis::step_bsmote(yesno,\n                      over_ratio=0.8) |> \n  prep() |> \n  bake(new_data = NULL) \n\nx_train <- model.matrix(yesno ~.,data = Data_Clean)[,-1]\ny_train <- Data_Clean$yesno\n\nElastic_Upsampled <- cv.glmnet(x=x_train,\n          y=y_train,\n          alpha = 0,\n          family=\"binomial\",\n          standardize = FALSE,\n          nfolds=10)\n\n\nTrained_Performance_2 <- assess.glmnet(Elastic_Upsampled,\n              newx = x_train,\n              newy = y_train,\n              family = \"binomial\",\n              alpha = 0,\n              standardize = FALSE)\n```\n\n```{r New_Model}\nFinalised_Model <- glmnet(x=x_train,\n       y=y_train,\n       family = \"binomial\",\n       alpha = 0,\n       standardize = FALSE,\n       lambda = Elastic_Upsampled$lambda.min)\n```\n\n```{r Compare_Performance}\nX_Test <- model.matrix(yesno ~.,data =Data_Test)[,-1]\nY_Test <- as.numeric(Data_Test$yesno==\"y\")\n\n\nFinal_Prediction <- predict(Final_Model,as.matrix(Data_Test[,-grep(\"\\\\byesno\\\\b\",\n                           names(Data_Test))]),\n        type = \"class\") |> \n  as.data.frame() |> \n  mutate(.pred_class = ifelse(s0 == \"0\",\"y\",\"n\")) |> \n  select(-s0) |> \n  mutate(across(everything(),\n                .fns=\\(x){factor(x,\n                                 level = c(\"n\",\"y\"))}))\n\n\nFinal_Prediction_2 <- predict(Finalised_Model,\n        as.matrix(Data_Test[,-grep(\"\\\\byesno\\\\b\",\n                                   names(Data_Test))]),\n        type = \"class\") |> \n  as.data.frame() |> \n  mutate(.pred_class = ifelse(s0 == \"0\",\"y\",\"n\")) |> \n  select(-s0) |> \n  mutate(across(everything(),\n                .fns=\\(x){factor(x,\n                                 level = c(\"n\",\"y\"))})) \n\n\nFinal_Prediction$truth <- Data_Test$yesno\nFinal_Prediction_2$truth <- Data_Test$yesno\n\nFinal_Prediction <- Final_Prediction |> \n  mutate(across(everything(),\n                .fns=\\(x){factor(x,\n                                 level = c(\"n\",\"y\"))}))\n\n\nFinal_Prediction_2 <- Final_Prediction_2 |> \n  mutate(across(everything(),\n                .fns = \\(x){\n                  factor(x,\n                         level = c(\"n\",\"y\"))\n                }))\n\nFinal_Prediction$truth |> table();Final_Prediction_2$truth |> table()\n\n```\n\n::: {.panel-tabset}\n\n## Model 1\n\n```{.r}\nFinal_Prediction |> \n  conf_mat(truth=truth, \n           estimate = .pred_class) |> \n  autoplot() |> \n  print()\n```\n## Model 2\n```{.r}\nFinal_Prediction_2 |> \n  conf_mat(truth = truth,\n           estimate = .pred_class) |> \n  autoplot()\n  \n```\n\n:::","srcMarkdownNoYaml":"\n\n# INTRODUCTION\n\nThe [TidyTuesday](https://github.com/rfordatascience/tidytuesday) project is a weekly social data project. Each week an interesting dataset is shared with the community for members to clean, model and or visualise data. People are encouraged to share their code and posts through social media. In essence, it serves as good avenue to explore one's data skills and to learn with others. This week (2023/08/14), the dataset is comprised of spam and non-spam emails originating from [UCI Machine Learning Repository](http://archive.ics.uci.edu/dataset/94/spambase).\n\n## IMPORT DATA\n\n```{r Libraries}\n#| message: false\n#| warning: false\n#| include: true\n\nlapply(c(\"tidyverse\",\"janitor\",\"ggthemes\",\n         \"glmnet\",\"tidymodels\",\"vip\",\"ranger\",\n         \"themis\",\"gt\",\"gtExtras\"),\n       require,character.only = TRUE) |> \n  suppressWarnings() |> \n  suppressMessages() |> \n  invisible() # <1>\n\nEmails <- read.csv(file = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-08-15/spam.csv\") |> \n  clean_names() # <2>\n\ntheme_set(theme_minimal()) # <3>\nset.seed(2023)\n```\n\n1.  We load our preferred R packages. This style of loading packages is not standard. It helps to silence some of the messages and conflict warnings when loading meta packages such as `tidyverse` and `tidymodels`.\n\n2.  We import data from the tidytuesday github repository. Again, the use of `read.csv` in place of `read_csv` or other functions boils down to preference.\n\n3.  The `theme_set` function lets us select a theme for our visualisations.\n\nWe also import a number of auxilary packages to assist in the analysis. The imported data is relatively small only comprising of 4601 rows and 7 columns. Our outcome variable is `yesno`,i.e. Spam/Not Spam email. The remaining variables contains derived variables, frequency counts. These are essentials tallies of references to money, currency and exclamation marks (oddly named `bang`). The dataset presents a classification problem. We start by scaling the numeric variables.\n\n```{r Data_Preprocessing}\n#| label: Scale Numeric Variables\n#| include: true\n#| echo: true\n\nEmails[,-grep(\"yesno\",names(Emails))] <-  sapply(Emails[,sapply(Emails,is.numeric)],\n       function(x){\n         scale(x,center = TRUE,scale = TRUE) \n       }) # <1>\n```\n\n1.  Use `<-` as your separator between the right-hand-side and the left-hand-side of the code. On the left-hand-side, we select all variables except the target variable except the outcome variable. On the right-hand-side, we use a nested `sapply` function to subset only for predictor variables. Thereafter, we subset columns that are of type numeric. Finally, we call an anonymous function via `function(x)` and pass each variable through then scale function.\n\nIt appears convoluted. However, it completes a set of sub-setting and applies functions simultaneously. It is better practice to first define a function and sapply.\n\n## DATA SPLITTING\n\nPart of the data splitting involves some decisions about whether to stratify the outcome variable. Especially with an imbalanced dataset, it is important to have sufficient representation of the minority cases without leaking the outcomes in the training process. Next, we can do some data splitting into a training and testing set, stratified over the outcome variable. The `rsample` package serves a set of convenient function for the task. The majority of emails (60.56%) are not spam.\n\n```{r Data_Prep}\n#| label: Data Splitting\n\nData_Split <- initial_split(Emails,strata=yesno)\nData_Train <- training(Data_Split)\nData_Test <- testing(Data_Split)\n\n100/nrow(Data_Train)*table(Data_Train$yesno)\n```\n\n## MODEL FITTING\n\nTo\n\n```{r GLMNET}\nX_Train <- model.matrix(yesno ~.,data =Data_Train)[,-1]\nY_Train <- as.numeric(Data_Train$yesno==\"y\")\n\n\nRanger_Model <- ranger(x=X_Train,\n       y = Y_Train,\n       verbose = TRUE,\n       importance = \"impurity\",\n       splitrule = \"gini\",\n       seed = 2023,\n       classification = TRUE)\n\nRanger_Model$confusion.matrix \n```\n\n```{r Elastic_Model_Fitting}\nElastic_Net <- cv.glmnet(x=X_Train,\n                      y=Y_Train,\n                      alpha =0,\n                      family = \"binomial\",\n                      nfolds = 10,\n                      type.measure = \"class\")\n```\n\n```{r Train_Performance}\nTraining_Performance <- assess.glmnet(Elastic_Net,\n              newx=X_Train,\n              newy=Y_Train)\n\nFinal_Model <- glmnet(x=X_Train,\n                      y = Y_Train,\n                      lambda = min(Elastic_Net$lambda),\n                      alpha = 0,\n                      family = \"binomial\",\n                      standardize = FALSE)\n\n\nTraining_Performance <- data.frame(lapply(Training_Performance,unlist))\n\nrownames(Training_Performance) <- NULL\n\nTraining_Performance |> \n  gt() |> \n  tab_header(title = \"2023 Initial ElasticNet Model\",\n             subtitle = \"Training Model Performance\") |> \n  gt_theme_espn()\n```\n\n# REBALANCING OUTCOME\n\n```{r New_Recipes}\nData_Clean <- recipe(yesno ~.,data = Data_Train) |> \n  themis::step_bsmote(yesno,\n                      over_ratio=0.8) |> \n  prep() |> \n  bake(new_data = NULL) \n\nx_train <- model.matrix(yesno ~.,data = Data_Clean)[,-1]\ny_train <- Data_Clean$yesno\n\nElastic_Upsampled <- cv.glmnet(x=x_train,\n          y=y_train,\n          alpha = 0,\n          family=\"binomial\",\n          standardize = FALSE,\n          nfolds=10)\n\n\nTrained_Performance_2 <- assess.glmnet(Elastic_Upsampled,\n              newx = x_train,\n              newy = y_train,\n              family = \"binomial\",\n              alpha = 0,\n              standardize = FALSE)\n```\n\n```{r New_Model}\nFinalised_Model <- glmnet(x=x_train,\n       y=y_train,\n       family = \"binomial\",\n       alpha = 0,\n       standardize = FALSE,\n       lambda = Elastic_Upsampled$lambda.min)\n```\n\n```{r Compare_Performance}\nX_Test <- model.matrix(yesno ~.,data =Data_Test)[,-1]\nY_Test <- as.numeric(Data_Test$yesno==\"y\")\n\n\nFinal_Prediction <- predict(Final_Model,as.matrix(Data_Test[,-grep(\"\\\\byesno\\\\b\",\n                           names(Data_Test))]),\n        type = \"class\") |> \n  as.data.frame() |> \n  mutate(.pred_class = ifelse(s0 == \"0\",\"y\",\"n\")) |> \n  select(-s0) |> \n  mutate(across(everything(),\n                .fns=\\(x){factor(x,\n                                 level = c(\"n\",\"y\"))}))\n\n\nFinal_Prediction_2 <- predict(Finalised_Model,\n        as.matrix(Data_Test[,-grep(\"\\\\byesno\\\\b\",\n                                   names(Data_Test))]),\n        type = \"class\") |> \n  as.data.frame() |> \n  mutate(.pred_class = ifelse(s0 == \"0\",\"y\",\"n\")) |> \n  select(-s0) |> \n  mutate(across(everything(),\n                .fns=\\(x){factor(x,\n                                 level = c(\"n\",\"y\"))})) \n\n\nFinal_Prediction$truth <- Data_Test$yesno\nFinal_Prediction_2$truth <- Data_Test$yesno\n\nFinal_Prediction <- Final_Prediction |> \n  mutate(across(everything(),\n                .fns=\\(x){factor(x,\n                                 level = c(\"n\",\"y\"))}))\n\n\nFinal_Prediction_2 <- Final_Prediction_2 |> \n  mutate(across(everything(),\n                .fns = \\(x){\n                  factor(x,\n                         level = c(\"n\",\"y\"))\n                }))\n\nFinal_Prediction$truth |> table();Final_Prediction_2$truth |> table()\n\n```\n\n::: {.panel-tabset}\n\n## Model 1\n\n```{.r}\nFinal_Prediction |> \n  conf_mat(truth=truth, \n           estimate = .pred_class) |> \n  autoplot() |> \n  print()\n```\n## Model 2\n```{.r}\nFinal_Prediction_2 |> \n  conf_mat(truth = truth,\n           estimate = .pred_class) |> \n  autoplot()\n  \n```\n\n:::"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"number-sections":true,"output-file":"Index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en-GB","fig-responsive":true,"quarto-version":"1.4.549","editor":"visual","code-copy":true,"code-summary":"see code","theme":{"light":"materia","dark":"darkly"},"title":"Spam Emails","date":"August 18,2023","author":"Sivuyile Nzimeni","image":"./Spam_Detection.png","image-alt":"An image illustrating an email folder classified as either spam or not.","description":"This post is about building a classification model for spam email detection. We explore a solution for handling an imbalanced dataset.","categories":["data modelling","data cleaning","visualisation"],"fig-cap-location":"top","page-layout":"article","cap-location":"margin","lot":true,"lof":true,"toc-title":"CONTENTS"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}