[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Sivuyile Nzimeni is a Data Analyst in the Faculty of Economic and Management Sciences, University of the Free State."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nAugust 2022 - Present | Data Analyst| Office of the Dean: Faculty of Economic and Management Sciences| University of the Free State\nNov 2020 - July 2022| Data Analyst; Teaching and Learning Coordinator | Centre for Teaching and Learning & Faculty of Economic and Management Sciences| University of the Free State\nJan 2018 - Nov 2020| Teaching and Learning Coordinator: Economic and Management Sciences | Centre for Teaching and Learning | University of the Free State\nJan 2017 - Dec 2017| Research Assistant | Department of Business Management | University of the Free State\nJul 2017 - Nov 2017| Part-Time Lecturer |Department of Business Management| University of the Free State\nNov 2015 - Nov 2016| Intern | Office of the Dean: Economic and Management Sciences |University of the Free State"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\n2016| University of the Free State |B.Com Honours with specialisation in Entrepreneurial Management\n2015| University of the Free State |B.A with majors in Business Management and Philosophy"
  },
  {
    "objectID": "about.html#interests",
    "href": "about.html#interests",
    "title": "About",
    "section": "Interests",
    "text": "Interests\n\nR Programming\nData Analysis in Higher Education\nManagement Studies\nEconomics\nSocial Media Analysis\nNatural Language Processing"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "About",
    "section": "Publications",
    "text": "Publications\nNzimeni, S. Mofokeng, M. 2022. ‘Automating tutorial attendance register capturing, preliminary results from a pilot project’. Siyaphumelela 2022: A Saide Project. Online, 25 - 27 June 2022.\nMuller, A. Nzimeni, S. Janse van Vuuren, Corlia. 2021. ‘Curriculum enhancement: reflections on the use of data, holistic student support and disciplinary skills development on a decade-long transformative journey’. 2021 University of the Free State Annual Teaching and Learning Conference.\nNzimeni, S. 2019. ‘Enrolment versus attendance: A preliminary investigation into the cost of tutorials’. Siyaphumelela 2019: A Saide Project. Johannesburg, 25 - 27 June 2019.\nNzimeni, S. Smit, AVA. 2018. ‘Is the quality of education impacting the global competitiveness of the South African business environment?’ 30th Annual Conference of the South African Institute of Management Scientists: Conference Proceedings, Stellenbosch University, Stellebosch, 16 - 19 September 2018."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "2023 TidyTuesday Week 1\n\n\n\nspatial data\n\n\ndata cleaning\n\n\nvisualisation\n\n\n\nVisualising Spatial Data in R using data from the IEC and Municipal Dermacation Board in South Africa.\n\n\n\nSivuyile Nzimeni\n\n\nJan 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022 Ask A Manager Salary Survey\n\n\n\nData Modelling\n\n\nData Cleaning\n\n\n\nAnalysing the 2022 Ask A Manager Salary Survey. In this post, we detail the data preprocessing procedure, feature-selection and modelling process to estimate salaries among…\n\n\n\nSivuyile Nzimeni\n\n\nDec 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022 Eskom Unavailabe Capacity\n\n\n\nnews\n\n\nAPI\n\n\nvisualisation\n\n\n\nVisualising Unavailable Energy in South Africa from 2018 to 2022.\n\n\n\nSivuyile Nzimeni\n\n\nSep 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022 Food Prices: Part 1\n\n\n\nnews\n\n\nAPI\n\n\ntable\n\n\n\nThis post explores 2020 - 2022 Food Prices in South Africa\n\n\n\nSivuyile Nzimeni\n\n\nJun 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSouth African Car Prices\n\n\n\nweb-scraping\n\n\ndata analysis\n\n\ndata modelling\n\n\n\nThis post details the data scraping process for obtaining data from a South African online vehicle marketplate. In addition, the post shares the results of a linear…\n\n\n\nSivuyile Nzimeni\n\n\nFeb 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nState of Capture Commission Report Part 1: Notes\n\n\n\nnews\n\n\npolitics\n\n\nlegal\n\n\n\nThis posts is the first installment of notes from reading the State of Capture Commission’s Report Part 1. This post post focuses on the first few pages of the report.\n\n\n\nSivuyile Nzimeni\n\n\nJan 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDepartment of Basic Education: Schools Database\n\n\n\nweb-scraping\n\n\ndata cleaning\n\n\n\nThis post details the data scraping process for obtaining the schools database of from the Department of Basic Education in South Africa.\n\n\n\nSivuyile Nzimeni\n\n\nDec 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAsk A Manager Survey\n\n\n\ndata analysis\n\n\ndata modelling\n\n\n\nThis post is about the 2021 Ask A Manager Survey analysis. It provides an overview of the data analysis process attempting to fit a linear regression model to explain…\n\n\n\nSivuyile Nzimeni\n\n\nDec 25, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021 Ask A Manager Salary Survey/index.html",
    "href": "posts/2021 Ask A Manager Salary Survey/index.html",
    "title": "Ask A Manager Survey",
    "section": "",
    "text": "In early 2021, the Ask A Manager blogsite ran their annual salary survey. The survey responses are stored on googlesheets, making the data accessible to all interested. In a previous post, we discussed data pre-processing and feature selection method. This post focuses two aspects,namely, 1) Reproducibility and 2) Out-of-Sample Testing.\n\n\nIn the previous post, we detailed the feature selection method by regularised regreesion, specifically, LASSO regression. In this post, we will attempt to reproduce the feature selection method.\n\n\n\nIn this post, we will also follow the traditional machine learning workflow including, splitting the data into a training and testing samples, fitting a model, cross-validation and finally fitting the model on the out-of-sample dataset(testing dataset). This approach can inform us about the model’s parsimony. In other words, can the model perform well on an unknown sample."
  },
  {
    "objectID": "posts/2021 Ask A Manager Salary Survey/index.html#important-variable-extraction",
    "href": "posts/2021 Ask A Manager Salary Survey/index.html#important-variable-extraction",
    "title": "Ask A Manager Survey",
    "section": "IMPORTANT VARIABLE EXTRACTION",
    "text": "IMPORTANT VARIABLE EXTRACTION\nThe resulting object is a list of class “glmnet”. Essentially, the results contain all the iterations through alpha and to find the minimum lambda. As such, there are several results in the object. We are primarily interested in extracting the remaining variables at minimum lambda along with their coeffiecients. The vip package can automate the plotting of the results. However, we want to extract the variables names in order to fit them on our training test. There is probably a package to assist with this step somewhere in the wild, however, we haven’t found it yet. Luckily in R, we can write custom functions. The code below details the variable_extractor function.\n\n\nsee code\nvariable_extractor <- function(a_list){\n  min_lambda <- data.frame(best_lambda =a_list[[\"lambda\"]]==min(a_list[[\"lambda\"]]))\n  min_lambda <- cbind.data.frame(data.frame(index = rownames(min_lambda)),\n                                 min_lambda)\n  min_lambda <- min_lambda %>% \n    filter(best_lambda == TRUE)\n  min_lambda <- as.double(unique(min_lambda$index))\n  lasso_variables <- as.matrix(coef(a_list))|>data.frame()\n  lasso_variables <- cbind.data.frame(variables = rownames(lasso_variables),\n                                      lasso_variables)\n  lasso_variables <- tibble(lasso_variables)\n  lasso_variables <- lasso_variables[,c(1,min_lambda+1)]\n  names(lasso_variables) <- c(\"variable\",\"importance\")\n  lasso_variables <- lasso_variables %>% \n    filter(variable != \"(Intercept)\",\n           importance != 0.00000000) %>% \n    arrange(desc(importance))\n\n  return(lasso_variables)\n}\n\nlasso_variable <- variable_extractor(lasso)\n\n\nThe function takes a list of attribute “glmnet”. Thereafter, we find the smallest lambda. In addition, we extract all the coefficients from the object and store them as a wide dataframe. The dataframe is subset to only contain the variable name along with the coefficients of the smallest lambda value. We discard the intercept and variable with coefficients that are equal to 0.0000000. The final output is identical to the variables extracted by the vip package. Finally, we subset both the training dataset and the testing dataset to only contain the selected independent variables. Since the outcome variable is expressed in USD terms, we log the outcome variable."
  },
  {
    "objectID": "posts/2021 Ask A Manager Salary Survey/index.html#tidymodels-a-clean-interface-for-maching-learning",
    "href": "posts/2021 Ask A Manager Salary Survey/index.html#tidymodels-a-clean-interface-for-maching-learning",
    "title": "Ask A Manager Survey",
    "section": "TIDYMODELS: A CLEAN INTERFACE FOR MACHING LEARNING",
    "text": "TIDYMODELS: A CLEAN INTERFACE FOR MACHING LEARNING\nThe R programming language doesn’t not lack methods for running machine learning algorithms, it is after all, a statistical programming language. The tidymodels metapackage aims to provide a standard interface for modelling and machine learning using tidyverse principles. Below, we use the package to complete a number of steps including, crossfold_validation, model specification, fitting on the resamples and finally fitting the model on the training dataset.\n\n\nsee code\nSalary_Folds <- vfold_cv(Salary_Train)\n\nlm_spec <- linear_reg(engine = \"lm\")\n\nlm_recipe <- recipe(new_annual_salary ~.,Salary_Train) %>% \n  step_nzv(all_predictors())\n\nlm_wf <- workflow(lm_recipe,lm_spec)\n\ndoParallel::registerDoParallel(cores = 10)\nctrl_preds <- control_resamples(save_pred = TRUE)\ncv_results <- fit_resamples(lm_wf,Salary_Folds,control = ctrl_preds)\n\nlm_wf <- fit(lm_wf,Salary_Train)\n\ncollect_metrics(cv_results,summarize = FALSE) %>% \n  filter(.metric == \"rsq\") %>% \n  summarise(avg_estimate = mean(.estimate),\n            .groups = \"drop\")\n\n\n# A tibble: 1 × 1\n  avg_estimate\n         <dbl>\n1        0.271\n\n\nAcross all 10 validation folds, the linear regression model’s adjusted R-square averaged 0.272. It is possible to tune the parameters and use battery of other machine learning models to improve performance. In the previous post, we utilised random forest to try an improve the model. Despite, requiring additional computational power, the increases in peformance were marginal to neglible.\nAt this point, we have completed our first objective. We are able to reproduce the LASSO regresison results in the previous post. The next objective is to determine the parsimony of the model. Here, we fit model on the test data."
  },
  {
    "objectID": "posts/2021 Department of Basic Education Scraping/index.html",
    "href": "posts/2021 Department of Basic Education Scraping/index.html",
    "title": "Department of Basic Education: Schools Database",
    "section": "",
    "text": "A few years ago, I was a student pursuing Masters of Commerce degree. The topic had something to do with the relationship between Education, Labour and Business outcomes. My pursuit of the qualification is defunct. However, there are several artefacts worth writting up. In this post, we will discuss the South African Schools Database. The Department of Basic Education regularly publishes versions of the School Database including a number of notable variables such as the school’s location, contact information, number of learners and teachers etc. The data is published in a non-standardised matter. This makes it an interesting data wrangling task.\n\n\nThe first hurdle is the volume of files publised on the DBE website. It is possible to download all 200 files by hand and save them to a directory of your choosing. However, such a process would be tedious and error prone(speaking from experience, ofcourse). The R-Programming language is a perfect companion for this task. To download the files, we can use two important packages, rvest and xml2.\nFirstly, we specify the url on the read_html function. Thereafter, we use the html_elements function to point to the html tag of interest. In this case, we are interest in the “a” tag, specifically, the href (or link) attribute.\nWe store the result in a data.frame object and use a regular expression to filter for values that start with a punctuation followed by “Link” and values that contain the term “forcedownload”. Finally, we append the path to file (our base url). The resulting data.frame contains all 200 downloadable files.\n\n\n\nWith the links in hand, we can tackle the next hurdle, downloading the files. Usually, we could use the commandline to download the files. For example, the single command below.\n\n\nsee code\nwget -i some_text_file.txt\n\n\nHowever, the DBE datasets are saved as either xlsx or xls format with a prompt on click to download the file. There are probably ways around this issue. Luckily, xml2 has a convenient function to handle this issue. In the code below, we use a for-loop to download each file in our dataset and save them in a specified sub folder. To avoid a break in the for-loop when an error occurs, we add the try function. As an add-on, we print a statement after each download. Depending on your internet connection speed and the website’s response time, this script can take five minutes to download all the files.\n\n\nsee code\nfor(i in seq_along(1:nrow(Data_Sets))){\n  try(download_xml(url = Data_Sets$dataset_links[[i]],\n               file = paste0(\"./Schools_Db/\",\"file_\",i,\".xlsx\")))\n  print(paste0(\"File \",i, \" downloaded \",\"proceeding to file \",i+1,\".\"))\n}\n\n\n\n\n\nAnother set of tasks to address is reading and cleaning the excel files. Using other software such as Excel or SPSS, these tasks would be cumbersome. Yet with R, Python or other programming languages, it possible handle more than one file at a time. Below, we use the readxl and purrr R packages to iteratively read and clean the files.\n\n\nsee code\n# All_Excel_Reader --------------------------------------------------------\nall_excel <- function(path){\n  collect_sheets <- excel_sheets(path)\n  number_of_sheets <- 1:length(collect_sheets)\n  per_sheet <- list()\n  for(i in seq_along(number_of_sheets)){\n    per_sheet[[i]] <- read_xlsx(path = path,\n                                sheet = collect_sheets[i])\n  }\n  return(per_sheet)}\n# All_Masterlist ----------------------------------------------------------\nEDU_Dbs <- data.frame(master_list = list.files(path = \"./Schools_Db\",\n           full.names=TRUE,\n           pattern = \".xlsx\")) %>% \n  mutate(schools_db = map(master_list,all_excel))\n\nEDU_Dbs <- EDU_Dbs %>% \n  unnest(schools_db)\n\nEDU_Dbs$schools_db <- lapply(EDU_Dbs$schools_db,sapply,as.character)\n\nEDU_Dbs$schools_db <- lapply(EDU_Dbs$schools_db,as.data.frame)\n\nEDU_Dbs <- EDU_Dbs %>% \n  unnest(schools_db)\n\nEDU_Dbs <- EDU_Dbs %>% \n  clean_names()\n\n\nThe resulting data.frame contains hundreds of thousands of rows and nearly 60 columns. Interestingly, most of these variable are effectively differing naming conventions such as emisno = natemis = oldnatemis = newnatemis. The insistent naming conventions extend to other variables such as gps coordinates and centre details. The spectacularly unoptimised script is availablehere. The scraper script,cleaning script and downloaded xlsx files are all available on the this github report."
  },
  {
    "objectID": "posts/2021 Second-Hand Cars in South Africa/Index.html",
    "href": "posts/2021 Second-Hand Cars in South Africa/Index.html",
    "title": "South African Car Prices",
    "section": "",
    "text": "South Africa has a plethora of online vehicle marketplaces. Often, their pool of vehicles for sale are usually > 50 000 on a daily basis. The vehicle listings offer a vast amount of car related data. Naturally, web-scraping the data provides an opportunity to fit a Machine-Learning model and endless exploration for petrol-heads (such as myself). Nearly all the online vehicle marketplaces have restrictive Terms and Conditions deterring the use of their data for commercial purposes and bombarding of their servers through web scrapping among other restrictions. Unfortunately, this means web scraping script cannot be shared in this post as it may reveal where the data were obtained, methodology of scraping the data etc. In addition, the website of the online vehicle marketplace will not be revealed."
  },
  {
    "objectID": "posts/2021 Second-Hand Cars in South Africa/Index.html#importing-the-data",
    "href": "posts/2021 Second-Hand Cars in South Africa/Index.html#importing-the-data",
    "title": "South African Car Prices",
    "section": "IMPORTING THE DATA",
    "text": "IMPORTING THE DATA\nBelow, we import the data and use dplyr::glimpse function to see the number of columns and values. The dataset contains a few important variables including the car_name and vehicle manufacturer. It is worth noting that the car_name variable appears to a free text field where the person listing the vehicle can modify the car name to include marketing terms such as: “excellent condition”,“reduce price” etc.\n\n\nsee code\nCar_Data <- read_csv(file =\"./2021-09-03_MANY_VEHICLES_Clean_Db.csv\") %>% \n  clean_names()\n\nglimpse(Car_Data)\n\n\nRows: 62,196\nColumns: 10\n$ car_name             <chr> \"9-3 Sport 2.0 Linear Lpt\", \"S40 2.0T\", \"V40 2.0\"…\n$ vehicle_manufacturer <chr> \"Saab\", \"Volvo\", \"Volvo\", \"Hyundai\", \"Volvo\", \"Ho…\n$ year                 <dbl> 2007, 1999, 2001, 2000, 2000, 1998, 1996, 1996, 2…\n$ mileage              <dbl> 200000, 285000, 271000, 125000, 190000, 267000, 1…\n$ price                <dbl> 13700, 18900, 18900, 20000, 20900, 21900, 21900, …\n$ fuel_type            <chr> \"Petrol\", \"Petrol\", \"Petrol\", \"Petrol\", \"Petrol\",…\n$ transmission         <chr> \"Manual\", \"Manual\", \"Manual\", \"Manual\", \"Automati…\n$ dealership           <chr> \"Mahala Motors\", \"WeBuyCars Midstream\", \"WeBuyCar…\n$ city_town            <chr> \"Klerksdorp\", \"Centurion\", \"Cape Town\", \"Johannes…\n$ province             <chr> \"North West Province\", \"Gauteng\", \"Western Cape\",…\n\n\nSimilarly, the free text field, also means that vehicle naming conventions deviate from the vehicle manufacturer specifications. Other text variables also contain these anomalous changes in text. The code below demonstrates an example of idiosyncrasies.\n\n\nsee code\nCar_Data %>% \n  filter(str_detect(car_name,\"condition\"))\n\n\n# A tibble: 4 × 10\n  car_name  vehic…¹  year mileage  price fuel_…² trans…³ deale…⁴ city_…⁵ provi…⁶\n  <chr>     <chr>   <dbl>   <dbl>  <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n1 Focus Ex… Ford     2008  305000  69950 Petrol  Manual  Cars a… Brakpan Gauteng\n2 Utility … Chevro…  2015  198000 114900 Petrol  Manual  United… Boksbu… Gauteng\n3 Clio Exc… Renault  2019   55000 160000 Petrol  Manual  Bright… Johann… Gauteng\n4 Ranger E… Ford     2017  107000 299900 Diesel  Automa… Bright… Johann… Gauteng\n# … with abbreviated variable names ¹​vehicle_manufacturer, ²​fuel_type,\n#   ³​transmission, ⁴​dealership, ⁵​city_town, ⁶​province"
  },
  {
    "objectID": "posts/2021 Second-Hand Cars in South Africa/Index.html#data-preprocessing",
    "href": "posts/2021 Second-Hand Cars in South Africa/Index.html#data-preprocessing",
    "title": "South African Car Prices",
    "section": "DATA PREPROCESSING",
    "text": "DATA PREPROCESSING\nWe would like to fit a model to better understand the determinants of vehicle prices. Here, pre-processing is important is especially important. Tidymodels and the textrecipes offer a range of functions to handle the whole modelling workflow. Below, are a number of pre-processing steps, firstly we use the unnest_tokens function from the tidytext package to process the the car_name variable. Thereafter, we handle the numerical values by applying a logarithm to outcome variable and standardising mileage. Finally, we use step_tokenize, step_tokenfilter and step_tf to tokenise, filter the those tokens and ultimately convert the tokens to a term frequency variables.\n\n\nsee code\nUseful_Car_Names <- Car_Data %>% \n  unnest_tokens(car_name,\n                output=\"car_name\") %>% \n  group_by(vehicle_manufacturer,car_name) %>% \n  summarise(n(),.groups = \"drop\") %>% \n  arrange(desc(`n()`)) %>% \n  anti_join(stop_words %>% \n              rename(car_name=word)) %>% \n  filter(!str_detect(car_name,\"\\\\d{1,}\"))\n\nCar_Data <- Car_Data %>% \n  unnest_tokens(car_name,output = \"car_name\") %>% \n  semi_join(Useful_Car_Names %>% \n              select(car_name,vehicle_manufacturer)) %>%\n  group_by(across(-car_name)) %>% \n  summarise(car_name = as.character(list(c(car_name))),\n            .groups = \"drop\") %>% \n  mutate(car_name = str_replace(car_name,\n                                'c[[:punct:]]{2,}',\"\"),\n         car_name = str_replace_all(car_name,\n                                    '\\\\\"',\"\"),\n         car_name = str_replace_all(car_name,\n                                    '[[:punct:]]{1,}$',\"\"),\n         age = 2021-year)\nCar_Data <- Car_Data %>% \n  mutate(across(c(car_name,dealership,city_town),\n                .fns = as_factor))\n\nNew_Car_Data <- recipe(price ~ .,data = Car_Data) %>% \n  step_log(all_outcomes()) %>%\n  step_normalize(mileage) %>% \n  step_tokenize(c(dealership,city_town,car_name)) %>%\n  step_tokenfilter(c(dealership,city_town,car_name)) %>%\n  step_tf(c(dealership,city_town,car_name)) %>% \n  step_dummy(c(vehicle_manufacturer,province,\n               fuel_type,transmission)) %>% \n  step_nzv(all_predictors()) %>% \n  prep() %>% \n  bake(Car_Data)"
  },
  {
    "objectID": "posts/2021 Second-Hand Cars in South Africa/Index.html#lasso-it-once-lasso-it-until-you-can-also-no-more",
    "href": "posts/2021 Second-Hand Cars in South Africa/Index.html#lasso-it-once-lasso-it-until-you-can-also-no-more",
    "title": "South African Car Prices",
    "section": "LASSO IT ONCE, LASSO IT UNTIL YOU CAN ALSO NO MORE",
    "text": "LASSO IT ONCE, LASSO IT UNTIL YOU CAN ALSO NO MORE\nThe resulting dataset contains 59417 rows across 27 variables. Given the dimension above, it prudent to do some additional feature select. LASSO regression helps with variable selection. In turn, we use the LASSO regression results to filter for the appropriate variables. The final variable set yields 24 predictor variables. The code below contains all details the LASSO implementation and subsequent filtering.\n\n\nsee code\nNew_Car_Data <- New_Car_Data %>%\n  select(-year)\n\nCar_split <- initial_split(New_Car_Data)\nCar_Training <- training(Car_split)\nCar_Test <- testing(Car_split)\n\nX <- model.matrix(price~.,Car_Training)[,-1]\nY <- Car_Training$price\nlasso_model <- glmnet(x = X,y=Y,\n       alpha=1)\n\nvariable_extractor <- function(a_list){\n  min_lambda <- data.frame(best_lambda =a_list[[\"lambda\"]]==min(a_list[[\"lambda\"]]))\n  min_lambda <- cbind.data.frame(data.frame(index = rownames(min_lambda)),\n                                 min_lambda)\n  min_lambda <- min_lambda %>% \n    filter(best_lambda == TRUE)\n  min_lambda <- as.double(unique(min_lambda$index))\n  lasso_variables <-\n    as.matrix(coef(a_list))|>data.frame()\n  \n  lasso_variables <- cbind.data.frame(variables = rownames(lasso_variables),\n                                      lasso_variables)\n  \n  lasso_variables <- tibble(lasso_variables)\n  lasso_variables <- lasso_variables[,c(1,min_lambda+1)]\n  names(lasso_variables) <- c(\"variable\",\"importance\")\n  lasso_variables <- lasso_variables %>% \n    filter(variable != \"(Intercept)\",\n           importance != 0.00000000) %>% \n    arrange(desc(importance))\n\n  return(lasso_variables)\n}\n\nVariables <- variable_extractor(lasso_model)\nCar_Training <- Car_Training[,c(\"price\",Variables$variable)]\nCar_Test <- Car_Test[,c(\"price\",Variables$variable)]"
  },
  {
    "objectID": "posts/2021 Second-Hand Cars in South Africa/Index.html#cross-validation",
    "href": "posts/2021 Second-Hand Cars in South Africa/Index.html#cross-validation",
    "title": "South African Car Prices",
    "section": "CROSS VALIDATION",
    "text": "CROSS VALIDATION\nBefore fitting to the test dataset, it worth investigating whether our model performs well against “shuffled” dataset of our training data. Fortunately, tidymodels and parsnip contain several functions to assist with the exercise.\nHere, we use the vfold_cv function to split our training data into random splits of equal size. Next, we use workflow to fit a linear model on the random splits. Subsequently, we plot the r-squares across all the folds.\nUltimately, cross validation helps us understand the performance of our model set of datasets by iterating through the training and test sample of each fold. The code below illustrates an implementation of cross validation.\n\n\nsee code\nCar_Training_cv <- vfold_cv(Car_Training)\n\nlinear_model <- linear_reg() %>% \n  set_engine(\"lm\")\n\ncv_outcomes <- workflow() %>% \n  add_model(linear_model) %>% \n  add_formula(price ~.) %>% \n  fit_resamples(Car_Training_cv) %>% \n  select(id,.metrics) %>% \n  unnest(.metrics) %>% \n  filter(.metric == \"rsq\") %>% \n  select(id,.metric,.estimator,.estimate)\n\ncv_plot <- cv_outcomes %>%  \n  ggplot(aes(id,.estimate,group=1,fill=id))+\n  geom_col(show.legend = FALSE,alpha=0.8)+\n  geom_text(aes(label=round(.estimate,2)))+\n  coord_flip()+\n  labs(title = \"Cross Validation Results\",\n       subtitle = \"Car Prices Linear Model: R-Square across 10 folds\",\n       x=NULL,\n       y = \"R-Square\")+\n  theme(plot.title = element_text(family = \"Arial Narrow\",\n                                  hjust = 0.5),\n        plot.subtitle = element_text(family = \"Arial Narrow\",\n                                     hjust = 0.5,face = \"italic\"))"
  },
  {
    "objectID": "posts/2022 Ask A Manager Salary Survey/index.html",
    "href": "posts/2022 Ask A Manager Salary Survey/index.html",
    "title": "2022 Ask A Manager Salary Survey",
    "section": "",
    "text": "Last year, we analysed the 2021 Ask A Manager Salary Survey. The dataset is interesting for many reasons including,most beneficially, the dataset is open and available for all interested parties. It is also offers an opportunity to move beyond old and boring datasets such as mtcars and iris. The results of the survey offer an opportunity to clean open-text fields, currency data and feature engineering for data modelling.\nFortunately, there is a 2022 Ask A Manager Salary Survey that is also freely available. Notably, there were only 15 465 respondents compared to 27 919 responds in the previous survey. It is difficult to estimate the reasons for the reduction in responses. Nonetheless,there are sufficient responses to model upon. The second batch of responses offers an opportunity to refine the data preprocessing and modelling process.\n\n\nUnlike in the previous post, the data preprocessing step is contained in a stand-alone script ./Scripts/2022_Data_Cleaner.R. This helps with two main aspects; i) practising modular scripts and ii) reducing the size of the analysis. The data cleaning script has several components in it. The table below summarises the aforementioned components.\n\n\nsee code\nlibrary(gt)\nlibrary(gtExtras)\n\ndata.frame(section = c(\"Importing Libraries\",\"Importing Data\",\"Open Text Cleaning\",\n                       \"List to Vec\",\"Completeness Check\"),\n           script_sample =c('lapply(as.list(c(\"tidyverse\",\"janitor\",\"arrow\",\n                 \"ggthemes\",\"tidytext\",\"tidymodels\",\n                 \"textrecipes\",\"glmnet\")),\n       require,character.only=TRUE) |>\n  suppressWarnings() |> \n  suppressMessages() |> \n  invisible()','read_csv(file = \"./Data/Ask A Manager Salary Survey 2022_Responses.csv\") |> \n  clean_names() |> \n  mutate(timestamp = mdy_hms(timestamp)) |> \n  filter(!is.na(job_title)) |> \n  mutate(additional_compensation = ifelse(is.na(additional_compensation),\n                                          0,additional_compensation),\n         full_compensation = salary+additional_compensation) |> \n  filter(!currency %in% c(\"Other\",\"HKD\")) |> \n  rownames_to_column(\"respondent\")','text_cleaner <- function(a_vec){\n  tokens <- str_split(a_vec,pattern=\"\\\\s{1,}\") |> \n    unlist()\n  tokens <- tokens[!grepl(\"\\\\s{1,}|[[:punct:]]|\\\\d{1,}\",tokens)] |> str_to_lower()\n  tokens <- unique(tokens) \n  tokens <- tokens[!tokens %in% c(stopwords::data_stopwords_nltk[[\"en\"]])]\n  return(tokens)}','list_to_vec <- function(a_vec){\n  new_vec <- a_vec |> \n    as.character()\nnew_vec <- gsub(\"(^c+[[:punct:]])|[[:punct:]]\",\n                replacement = \"\",\n                new_vec)\n  return(new_vec)}\n','completeness <- function(a_vec){\n  incomplete <- round(100/length(a_vec)*a_vec[is.na(a_vec)] |> length(),2)\nreturn(incomplete)}\n'),\n           reasoning = c(\"importing libraries required for preprocessing\",\n                         \"import data, convert date column to date type, remove unviable cases and create total compensation columns\",\n                         \"split open-text fields into tokens, remove punctuation, spaces and stopwords. The function returns a list of words per case\",\n                         \"Convert the list of words per case to a vector.\",\n                         \"Check the level of missing values in each column.\")) |> \n  gt() |> \n  gt_theme_nytimes() |> \n  tab_header(title = \"2022 Data Clean Script Overview\",\n                 subtitle = \"An overview of sections in the data cleaning script\") |> \n  opt_align_table_header(align = \"center\") |> \n  tab_style(style = list(\n    cell_fill(color= \"#490E6F\",alpha=0.8),\n    cell_text(color = \"white\",font=google_font(\"Fira Code\"),\n              align = \"center\",style = \"normal\",\n              weight= \"lighter\",whitespace = \"pre-line\")\n  ),\n  locations= cells_body(\n    columns = script_sample\n  ))\n\n\n\n\n\n\n  \n    \n      2022 Data Clean Script Overview\n    \n    \n      An overview of sections in the data cleaning script\n    \n  \n  \n    \n      section\n      script_sample\n      reasoning\n    \n  \n  \n    Importing Libraries\nlapply(as.list(c(\"tidyverse\",\"janitor\",\"arrow\",\n                 \"ggthemes\",\"tidytext\",\"tidymodels\",\n                 \"textrecipes\",\"glmnet\")),\n       require,character.only=TRUE) |>\n  suppressWarnings() |> \n  suppressMessages() |> \n  invisible()\nimporting libraries required for preprocessing\n    Importing Data\nread_csv(file = \"./Data/Ask A Manager Salary Survey 2022_Responses.csv\") |> \n  clean_names() |> \n  mutate(timestamp = mdy_hms(timestamp)) |> \n  filter(!is.na(job_title)) |> \n  mutate(additional_compensation = ifelse(is.na(additional_compensation),\n                                          0,additional_compensation),\n         full_compensation = salary+additional_compensation) |> \n  filter(!currency %in% c(\"Other\",\"HKD\")) |> \n  rownames_to_column(\"respondent\")\nimport data, convert date column to date type, remove unviable cases and create total compensation columns\n    Open Text Cleaning\ntext_cleaner <- function(a_vec){\n  tokens <- str_split(a_vec,pattern=\"\\s{1,}\") |> \n    unlist()\n  tokens <- tokens[!grepl(\"\\s{1,}|[[:punct:]]|\\d{1,}\",tokens)] |> str_to_lower()\n  tokens <- unique(tokens) \n  tokens <- tokens[!tokens %in% c(stopwords::data_stopwords_nltk[[\"en\"]])]\n  return(tokens)}\nsplit open-text fields into tokens, remove punctuation, spaces and stopwords. The function returns a list of words per case\n    List to Vec\nlist_to_vec <- function(a_vec){\n  new_vec <- a_vec |> \n    as.character()\nnew_vec <- gsub(\"(^c+[[:punct:]])|[[:punct:]]\",\n                replacement = \"\",\n                new_vec)\n  return(new_vec)}\n\nConvert the list of words per case to a vector.\n    Completeness Check\ncompleteness <- function(a_vec){\n  incomplete <- round(100/length(a_vec)*a_vec[is.na(a_vec)] |> length(),2)\nreturn(incomplete)}\n\nCheck the level of missing values in each column.\n  \n  \n  \n\n\nA table containing 2022 Data Cleaner sections\n\n\nThe table above contains a few convenience functions used throughout the script. Importing of libraries is completed through lapply followed by suppressWarnings, suppressMessages and invisible functions. The last three functions calls are probably bad practise as warnings may be helpful down the line. A crucial function is the importing data followed by some basic cleaning functions from lubridate and Wickham (2022) . A more exotic function is text_cleaner, the main idea is to remove stopwords, punctuation and white spaces. Ultimately, the function returns a list of terms per case. This practise is not tidy yet. Another convenience function list_to_vec,an imitation of concatenating text values in pivot tables in excel. Finally, the completeness function checks the percentage of missing values within a column. The function is intended to aid in checking the viability of a column in the dataset. Here, columns with missing value percentage > 5% are excluded from the dataset.\nThese functions are applied through purrr::map a tidyverse equivalent to base::lapply, along with dplyr::mutate and dplyr::across to implement the functions consistently.\nAnother important task of the cleaning the data is the conversion of currencies to USD for non-USD salary values. This way, we can get closer to comparing apples with apples. The International Monetary Fund’s Exchange Rate Report Wizard which allows users to exchange rates for multiple time ranges. Importantly, the Hong Kong Dollar is not listed in the report. As such, salaries reported in that currency are not included in the analysis.\nThe final preprocessing step relies on the tidymodels and Hvitfeldt (2022) packages to specify a recipe to tokenize the values derived from list_to_vec,extract their TFIDF (Term Frequency-Inverse Document Frequency) of the tokens, log the salary values and remove near-zero variance variables.\nFinally, the resulting data.frame is used in LASSO regression, which serves as our feature selection step.\n\n\nsee code\nsource(\"./Scripts/2022_Data_Cleaner.R\",\n       local = knitr::knit_global())\n\n\n\n\nAn image containing a Lasso Regression plot indicating coeffiecients\n\n\n\n\n\nUltimately, we extract the variables of interest from the lasso model through the function below. The function is an improvement on the previous post, as it does rely on converting each matrix into a data.frame. In this instance, the function subsets the last model and minimum lambda value from the matrices which are in turn converted to a data.frame. The function is similar to Stata17’s lassocoef or SPSS29’s Best option in Linear Lasso Regression: Option. The script outputs training and testing outputs for further analysis. In this way, the preprocessing section is isolated to a single script and negates the need to read-in data.\n\n\nsee code\nvariable_extractor <- function(a_list){\n  min_lambda <- a_list$lambda |> min()\n  last_model <- length(a_list$lambda)\n  lasso_variables <- as.matrix(coef(a_list))[,last_model] |> \n    data.frame()\n  lasso_variables$variable <- rownames(lasso_variables)\n  names(lasso_variables) <- c(\"coef\",\"variable\")\n  lasso_variables <- lasso_variables[,c(\"variable\",\"coef\")]\n  lasso_variables <- lasso_variables[!grepl(pattern = \"[[:punct:]]Intercept[[:punct:]]\",\n                                            lasso_variables$variable),]\nreturn(lasso_variables)}\n\nSalary_Modelling <- cbind(Salary_Modelling[,grep(\"full_comp_usd\",names(Salary_Modelling))],\nSalary_Modelling[,names(Salary_Modelling) %in% variable_extractor(Lasso_Model)$variable])"
  },
  {
    "objectID": "posts/2022 Ask A Manager Salary Survey/index.html#ols-model-swiss-knife-approach",
    "href": "posts/2022 Ask A Manager Salary Survey/index.html#ols-model-swiss-knife-approach",
    "title": "2022 Ask A Manager Salary Survey",
    "section": "2.1 OLS MODEL: SWISS KNIFE APPROACH",
    "text": "2.1 OLS MODEL: SWISS KNIFE APPROACH\n\n\nsee code\nols_model <- lm(full_comp_usd ~.,data = Training_Salary)\nlibrary(gtsummary)\n\ntbl_regression(ols_model,\n               add_estimate_to_reference_rows = TRUE,\n               exponentiate = FALSE) |> \n  add_glance_source_note(label = list(\n    r.squared ~ \"R2\",\n    adj.r.squared ~ \"Adj R2\",\n    p.value ~ \"p-value\",\n    statistic ~ \"F-Statistic\",\n    df ~ \"Degrees of Freedom\",\n    df.residual ~ \"Residual df\"),\n    include = c(r.squared,adj.r.squared,p.value,df,statistic,\n                df.residual)\n    ) |> \n  italicize_labels() \n\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    tfidf_employers_industry_administration\n0.17\n-0.07, 0.42\n0.2\n    tfidf_employers_industry_banking\n0.07\n-0.32, 0.45\n0.7\n    tfidf_employers_industry_care\n-0.20\n-0.46, 0.07\n0.15\n    tfidf_employers_industry_computing\n0.16\n-0.20, 0.51\n0.4\n    tfidf_employers_industry_education\n-0.12\n-0.14, -0.10\n<0.001\n    tfidf_employers_industry_engineering\n0.11\n0.03, 0.19\n0.009\n    tfidf_employers_industry_finance\n-0.01\n-0.39, 0.37\n>0.9\n    tfidf_employers_industry_government\n0.13\n-0.08, 0.33\n0.2\n    tfidf_employers_industry_health\n0.19\n-0.07, 0.46\n0.2\n    tfidf_employers_industry_manufacturing\n-0.02\n-0.09, 0.06\n0.7\n    tfidf_employers_industry_nonprofits\n-0.09\n-0.10, -0.07\n<0.001\n    tfidf_employers_industry_public\n-0.33\n-0.47, -0.19\n<0.001\n    tfidf_employers_industry_tech\n0.12\n-0.23, 0.48\n0.5\n    tfidf_functional_area_of_job_advertising\n0.31\n-0.49, 1.1\n0.4\n    tfidf_functional_area_of_job_banking\n-0.43\n-0.97, 0.11\n0.12\n    tfidf_functional_area_of_job_computing\n0.71\n-0.25, 1.7\n0.15\n    tfidf_functional_area_of_job_education\n-0.03\n-0.05, -0.01\n0.009\n    tfidf_functional_area_of_job_finance\n0.39\n-0.15, 0.93\n0.2\n    tfidf_functional_area_of_job_pr\n-0.30\n-1.1, 0.51\n0.5\n    tfidf_functional_area_of_job_tech\n-0.52\n-1.5, 0.44\n0.3\n    tfidf_job_title_manager\n0.08\n0.05, 0.11\n<0.001\n    tfidf_job_title_senior\n0.23\n0.19, 0.27\n<0.001\n    tfidf_country_canada\n0.17\n0.15, 0.19\n<0.001\n    tfidf_country_states\n-0.50\n-1.2, 0.19\n0.2\n    tfidf_country_united\n1.2\n0.56, 1.9\n<0.001\n    tfidf_race_white\n-0.09\n-0.14, -0.04\n<0.001\n  \n  \n    \n      R2 = 0.155; Adj R2 = 0.153; p-value = <0.001; Degrees of Freedom = 26; F-Statistic = 79.2; Residual df = 11,237\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\nOLS Regression Results illustrating an adjusted r-square of 0.15 on 11237 cases and global p value of < 0.001\n\n\nFitting a linear regression model is rather straight forward. The model above yields an Adjusted R Square of 0.153. In other words, the model explains 15% of the variance in salaries. The beta, confidence interval and p-values of the variables considered are listed on the table. One way to report the aforementioned results would be to utilise the report package which can aid in reporting models in standard manner including beta values, standard error, confidence intervals etc. for the respective variables considered in the model."
  },
  {
    "objectID": "posts/2022 Ask A Manager Salary Survey/index.html#mars-model-splines-and-lines",
    "href": "posts/2022 Ask A Manager Salary Survey/index.html#mars-model-splines-and-lines",
    "title": "2022 Ask A Manager Salary Survey",
    "section": "2.2 MARS MODEL: SPLINES AND LINES",
    "text": "2.2 MARS MODEL: SPLINES AND LINES\nBelow, we build a MARS model and iterate through 10 cross-validation folds. The resulting models, does not out perform the OLS model. The R-Square average R-Square for the MARS model is 0.15.\n\n\nsee code\nlibrary(earth)\n\nMARS_Model <- earth(full_comp_usd ~.,Training_Salary,\n      degree = 2,\n      glm = list(family = \"gaussian\"),\n      pmethod = \"exhaustive\",\n      nfold = 10,\n      ncross= 10,\n      varmod.method = \"earth\")\n\nFold_Rsquares <- lapply(MARS_Model$cv.list,\\(x){x[\"rsq\"]})\n\nFold_Rsquares <- lapply(Fold_Rsquares,data.frame)\n\ntheme_set(ggthemes::theme_solarized_2())\n\nmap2(Fold_Rsquares,names(Fold_Rsquares),function(x,y){\n  x |> \n    mutate(fold_source =y)\n})%>%\n  do.call(bind_rows,.) |> \n  mutate(iteration = str_extract(fold_source,\"\\\\d{1,2}$\") |> as.integer(),\n         fold_source = str_remove(fold_source,\"[[:punct:]]\\\\d{1,}\") |> as.factor(),\n         fold_source = fct_reorder(fold_source,iteration)) |> \n  ggplot(aes(iteration,rsq,group=fold_source))+\n  geom_line(aes(colour = fold_source),show.legend = FALSE)+\n  scale_colour_manual(values = c(\"fold1\" = \"#0F204B\",\n                                 \"fold2\"= \"#A71930\",\n                                 \"fold3\" = \"#000000\",\n                                 \"fold4\" = \"#9E83B7\",\n                                 \"fold5\"=\"#00B140\",\n                                 \"fold6\" = \"#BB133E\",\n                                 \"fold7\"= \"#490E6F\",\n                                 \"fold8\"= \"#0039A7\",\n                                 \"fold9\"= \"#00675A\",\n                                 \"fold10\"= \"#EA8400\"))+\n  scale_x_continuous(breaks = c(1:10))+\n  facet_wrap(~fold_source,\n             ncol = 2,\n             nrow=5, \n             scales = \"free\")\n\n\n\n\nMARS Model Results indicating a selected model with an R-Square of 0.15\n\n\n\n\n\nThere are several ways to improve the model. Such as including factor variables in place of dummy variables, utilising polynomial regression in place of the gaussian family among others. For a more detailed account on how to work with enhance MARS models, see Milborrow (n.d.) .\n\n\nsee code\nplotmo(MARS_Model)\n\n\n plotmo grid:    tfidf_employers_industry_administration\n                                                       0\n tfidf_employers_industry_banking tfidf_employers_industry_care\n                                0                             0\n tfidf_employers_industry_computing tfidf_employers_industry_education\n                                  0                                  0\n tfidf_employers_industry_engineering tfidf_employers_industry_finance\n                                    0                                0\n tfidf_employers_industry_government tfidf_employers_industry_health\n                                   0                               0\n tfidf_employers_industry_manufacturing tfidf_employers_industry_nonprofits\n                                      0                                   0\n tfidf_employers_industry_public tfidf_employers_industry_tech\n                               0                             0\n tfidf_functional_area_of_job_advertising tfidf_functional_area_of_job_banking\n                                        0                                    0\n tfidf_functional_area_of_job_computing tfidf_functional_area_of_job_education\n                                      0                                      0\n tfidf_functional_area_of_job_finance tfidf_functional_area_of_job_pr\n                                    0                               0\n tfidf_functional_area_of_job_tech tfidf_job_title_manager\n                                 0                       0\n tfidf_job_title_senior tfidf_country_canada tfidf_country_states\n                      0                    0            0.3854429\n tfidf_country_united tfidf_race_white\n            0.3849207        0.7425228\n\n\n\n\nMARS Model Results Visualisation containing variables and their relationships retrieved through the plotmo package."
  },
  {
    "objectID": "posts/2022 Ask A Manager Salary Survey/index.html#keras-gun-to-a-knife-fight",
    "href": "posts/2022 Ask A Manager Salary Survey/index.html#keras-gun-to-a-knife-fight",
    "title": "2022 Ask A Manager Salary Survey",
    "section": "2.3 KERAS: GUN TO A KNIFE FIGHT",
    "text": "2.3 KERAS: GUN TO A KNIFE FIGHT\nWe haven’t had a significant difference between the OLS model and MARS model. Below, we use build a Allaire and Chollet (2022) regression model. It is mostly likely a computationally inefficient approach but worth exploring nonetheless. Fortunately, with a few changes to Chollet, Kalinowski, and Allaire (2022) code , we can fit a regression model through Keras in R. Below, we change the data to matrix format suitable for the type of model we build. Thereafter, we fit it through 500 epochs and visualise the Mean Absolute Error and Loss (Mean Squared Error) of the model.\n\n\nsee code\n#|fig-cap: Specifying a Keras model for regression\nlibrary(keras)\ntrain_data <- model.matrix(full_comp_usd ~.,Training_Salary)[,-1]\ntrain_targets <- Training_Salary$full_comp_usd\nbuild_model <- function() {\nmodel <- keras_model_sequential() %>%\nlayer_dense(64, activation = \"relu\") %>%\nlayer_dense(64, activation = \"relu\") %>%\nlayer_dense(1)\nmodel %>% compile(optimizer = \"rmsprop\",\nloss = \"mse\",\nmetrics = \"mae\")\nmodel\n}\n\n\n\n\nsee code\n#|echo: true\n#|fig-cap: Fitting a regression Keras model \nk <- 4\nfold_id <- sample(rep(1:k, length.out = nrow(train_data)))\nnum_epochs <- 100\nall_scores <- numeric()\nfor (i in 1:k) {\ncat(\"Processing fold #\", i, \"\\n\")\nval_indices <- which(fold_id == i)\nval_data <- train_data[val_indices, ]\nval_targets <- train_targets[val_indices]\npartial_train_data <- train_data[-val_indices, ]\npartial_train_targets <- train_targets[-val_indices]\nmodel <- build_model()\nmodel %>% fit(\npartial_train_data,\npartial_train_targets,\nepochs = num_epochs,\nbatch_size = 16,\nverbose = 1\n)\nresults <- model %>% evaluate(val_data,\nval_targets, verbose = 0)\nall_scores[[i]] <- results[['mae']]\n}\n\n\nProcessing fold # 1 \nProcessing fold # 2 \nProcessing fold # 3 \nProcessing fold # 4 \n\n\n\n\nsee code\nnum_epochs <- 100\nall_mae_histories <- list()\nfor (i in 1:k) {\ncat(\"Processing fold #\", i, \"\\n\")\nval_indices <- which(fold_id == i)\nval_data <- train_data[val_indices, ]\nval_targets <- train_targets[val_indices]\npartial_train_data <- train_data[-val_indices, ]\npartial_train_targets <- train_targets[-val_indices]\nmodel <- build_model()\nhistory <- model %>% fit(\npartial_train_data, partial_train_targets,\nvalidation_data = list(val_data, val_targets),\nepochs = num_epochs, batch_size = 16, verbose = 1\n)\nmae_history <- history$metrics$val_mae\nall_mae_histories[[i]] <- mae_history\n}\n\n\nProcessing fold # 1 \nProcessing fold # 2 \nProcessing fold # 3 \nProcessing fold # 4 \n\n\nsee code\nplot(history)\n\n\n\n\n\nWe know the three model’s performance metrics, the next question we need to answer is whether the data maintains the similar results on an out-of-sample dataset. Here we rely on Kuhn and Wickham (2022), yardstick package to compare performance between the models. It is probably a better approach to use a tidymodel’s workflow and recipes as it provides all the functions needed for preprocessing and modelling."
  },
  {
    "objectID": "posts/2022 Eskom Capacity/index.html",
    "href": "posts/2022 Eskom Capacity/index.html",
    "title": "2022 Eskom Unavailabe Capacity",
    "section": "",
    "text": "In late 2007, Eskom- South Africa’s electricity public utility, announced a rolling Blackout programme to mitigate the risk of a national electricity grid collapse. Fifteen years later, the programme, euphemistically named ‘loadshedding’, persists. The energy provider has failed to provide a sufficient supply of energy to meet demand in their domestic markets. In fact, according to Bloomberg (n.d.) the energy crisis has worsened, 2022 has already had 100 days of rolling Blackouts.\nOf course, there are a plethora of reasons that explain the crisis including, corruption, political interference, political inaction, poor corporate governance, financial mismanagement and many others (see Grootes (2019) ). With the aforementioned context in mind, it worth exploring some data to form a more detailed understanding of the status of energy supply in the country.\n\n\nFortunately, the state-owned institution has a data portal replete with a data dictionary, data visualisation and data exporting functionality. Additionally, obtaining the bulk data export is take a few seconds. Ultimately, a person can access data from Eskom for the previous five years.\nThe first task of analysing the dataset is to understand it’s structure. In the Eskom bulk data contains several dimensions including energy generation, energy generation sources, energy demand and energy supply and importantly Capacity Loss data. Capacity Loss is measured by numerous variables depending on the type of loss. Below, the Eskom Data Portal provides an glossary of variable definitions. This analysis will focus on Capacity Loss Factor which is a measure of unavailable energy capacity.\n\n\nsee code\nEskom_Data <- read_csv(file =\"./ESK2567.csv\") |> \n  clean_names() |> \n  dplyr::mutate(date_time_hour_beginning = ymd_hms(date_time_hour_beginning)) |>\n  filter(date_time_hour_beginning <= Sys.Date())\n\n\n\nTotal Planned Capability Loss Factor: the ratio between the unavailable energy of the units that are out on planned maintenance over a period compared to the total net installed capacity of all units over the same period.\nTotal Unplanned Capability Loss Factor: the ratio between the unavailable energy of the units that are out on unplanned outages over a period compared to the total installed capacity of all units over the same period.\nTotal Other Capability Loss Factor: the ratio between the unavailable energy of the units that cannot be dispatched, due to constraints out of the power station management control, over a period over a period compared to the total net install capacity of all units over the same period.\n\nBased on the variable definitions above, we can observe a number of important aspects. First, the Capacity Loss Factor, is differentiated between planned and unplanned, failures to power station management control compared to the total net install capacity of all units. In essence is each variable, is a ratio between available and unavailable capacity. It is also important to highlight the date_time_hour_beginning variable, an hourly update of capacity availability.\n\n\n\n\n\n\nVisualising a large time-series dataset is challenging. Fortunately, the world of finance has solved this problem. Candlestick charts are popular financial charts which provide a succinct view of daily share price movement. They provide a daily distribution of share price activity i.e, candle charts are modified small box plots. As usual, Wickham (2022) ’s set of packages have all the tools required to clean data and visualise the data. Here, we ideally want to use an interactive visualisation. Naturally, R Core Team (2022) has several packages to implement this type of visualisation. In this post, we rely on Coene (2022) interpretation of the echarts package in JavaScript through echarts4r R package. Prior to plotting the visualisation, we create a data.frame with all required columns, namely, Unavailable Capacity Loss Factor at Open, Low and High.\nIn the code chunk below, we create function which takes a function, finds all the unique date in the dataset along with the variables required to plot the candlestick chart. The function returns a data.frame. The candle_data function is applied to the list of unique dates through lapply and do.call to finalise the dataset.\n\n\nsee code\ncandle_data <- function(a_df){\n  date <- a_df$date |> unique()\n  Open <- a_df[a_df$date_time_hour_beginning == min(a_df$date_time_hour_beginning),\"unavailable_capacity_loss\"][1]\n  Close <- a_df[a_df$date_time_hour_beginning == max(a_df$date_time_hour_beginning),\"unavailable_capacity_loss\"][1]\n  Low <- a_df[a_df$unavailable_capacity_loss==min(a_df$unavailable_capacity_loss),\"unavailable_capacity_loss\"][1]\n  High <- a_df[a_df$unavailable_capacity_loss == max(a_df$unavailable_capacity_loss),\"unavailable_capacity_loss\"][1]\n  new_data <-  data.frame(date,\n             Open,\n             Close,\n             Low,\n             High)\n  return(new_data)}\n\nCandles <- lapply(Unavailable_Capacity,candle_data)\nCandles <- do.call(bind_rows,Candles)\n\nCandles <- Candles |> \n  mutate(across(where(is.double),\n                .fns = round))\nCandles$date <- ymd(Candles$date)\n\n\n\n\nThe candlestick chart covers the full period from 2018 - 2022, it can be hard to interpret the chart for the period. And there is a plethora of complicated ways to interpret candlestick charts, that is a discussion for another day. For now, there is one glaring fact about the Unavailable Capacity Loss Factor, in each year it tests new highs. In simple terms, South Africa’s energy crisis is worsening. To narrow the time span and gain a more granular view of the data, one can use the slider on the visualisation below. You can also download the visualisation by clicking on the download icon."
  },
  {
    "objectID": "posts/2022 Food Prices/index.html",
    "href": "posts/2022 Food Prices/index.html",
    "title": "2022 Food Prices: Part 1",
    "section": "",
    "text": "Food Price inflation has been the subject of much media coverage. Obtaining source data to assess prices changes across of a variety of food products can be a difficult exercise. Aside from Statistics South Africa, Pietermaritzburg Economic Justice & Dignity group (PMBEJD) and others provide regular estimates of price changes. On the other hand,the Trundler API provides a robust API to track prices across many retailers in South Africa and abroad. Below, we use the Trundler API to access data for six food categories namely:\n\nEggs\nMilk\nMaize\nRice\nSunflower Oil\nWhite Sugar\n\nUsing the Trundler API, we narrowed the product search to a handful of competing products in two large South African Retailers. As a result, we don’t share the data extraction process in this post to protect the data extracted and API keys associated with them.\nThe table below provides an overview of food prices for the period 2020 - 2022. The able is interactive, one can sort by food category, retailer id, date columns and price.\n\n\n\n2020 - 2022 Price Changes\nPrice changes across six categories of foods: Eggs, Maize, Milk, Rice, Sunflower Oil and White Sugar\n\nData Source: Trundler API| Visualisation: Sivuyile Nzimeni| Date Extracted: 2022/06/01\n\n\n\n\nThere are a few notable observations. Excella Sunflower Oil 2L increased from R50 in 2020-06-07 to 20222-05-30 R110, a 120 percent increase. The price of Milk has remained stable throughout the past two years. Tastic Soft & Absorbing Long Grain White Rice 2kg has increased from R30 to R39, a 30 percent increase.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022 State Capture Report 1/index.html",
    "href": "posts/2022 State Capture Report 1/index.html",
    "title": "State of Capture Commission Report Part 1: Notes",
    "section": "",
    "text": "On 14 October 2016, the previous Public Protector, Adv Thuli Mandonsela, published the State of Capture Report. The famed public sector watchdog had a reputation for holding public officials accountable. The State of Capture report was no different.\nIt documented the pervasiveness of State Capture in the Jacob Zuma administrations. The report provided prima facie evidence of State Capture, including many public officials burning desire to visit the home of the notorious Gupta Brothers.\nThe report also lent credence to the assertion of several whistleblowers that Guptas were surreptitiously influencing key public-sector decisions. Most importantly, the report led to the Commission of Inquiry into State Capture.\nIn the years since, a flurry of litigation in the peripheral of State Capture, some directly related to the topic, ensued; Jacob Zuma resigned, the Adv Thuli Mandonsela completed her term as the Public Protector and moved on to Stellenbosch University, and the Commission commenced its work.\nOn 04 January 2022, the Commission published Volume 1 of 3 of its report. The 04 January 2022 report is a continuation of the initial report from the previous Public Protector. The Commission’s report provides comprehensive detail on the allegations of State Capture, investigating claims and issuing several findings against several parties.\n\n\nThe 04 January 2022 report is the culmination of 430 hearings, 778 videos and 170 666 Affidavits and Statements, 3 171 summons issued to witness and 1 380 Requests For Information (State Capture.org.za 2022). Reading the report’s contents is important to understand the depth and breadth of the State Capture project. Beyond this, it is also important to read through the information to defend against disinformation and misinformation campaigns that may arise on social media especially considering the Bell Pottinger scandal.\n\n\n\nThis post attempts to summarise the report’s contents into layman’s language. It is also an attempt to find salient points throughout the report and ultimately to gain an in-depth understanding of the report.\nThe report opens with background information on the establishment of the Commission. It provides descriptive statistics on the work of the Commission and recounts the Commission’s mandate. The Commission heard evidence related to the following State-Owned Enterprises and Private Enterprises:\n\nSouth African Airways (SAA) and Subsidiaries\nBOSASA\nDenel\nEskom\nEstina Dairy Farm\nPRASA\nSABC\nSARS\nSSA\nTransnet.\n\nChapter 1 of the report focuses on SAA and its subsidiaries. Chapters 2, 3, and 4 focus on The New Age and its dealings with government departments and State-Owned Entities, South African Revenue Service and Public Procurement in South Africa, respectively. This post will focus on the first 140 pages of the report, with the remaining pages covered in subsequent posts.\n\n\n\nThe report is unambiguous about the entity’s state while Ms Dudu Myeni and Ms Kwinana served as board members. In short, the report asserts that SAA experienced a steady decline in quality and effectiveness. This decline is largely attributed to the tenures of Ms Myeni and Ms Kwinana. Throughout their terms, corruption and fraud were rampant at the institution. Those that opposed illegal conduct were victimised and eventually removed from their positions. The Commission also highlights failures by SAA auditors to detect fraud and corruption. The collapse also extends to SAA’s internal audit function. There was also a wholesale failure in governance within the SAA and its subsidiaries (see paragraphs 11 – 20 of the report).\nThe Commission is particularly scathing on the conduct of Ms Myeni, arguing that she created the hostile environment described above through “a mixture of negligence, incompetence and deliberate corrupt intent” to further the project of dismantling governance at the entity (see Paragraph 13 of the report).\nThe Commission also implicates several other stakeholders as enablers of Myeni’s conduct. These stakeholders include former President Jacob Zuma (see paragraph 14), former Ministers Gigaba and Brown (see paragraph 18). Finally, the Cabinet, the Executive of South Africa, is also implicated for yielding to the preferences of the President rather than the interests of State-Owned Enterprises. Ms Carolous (previous SAA Board Chair) detailed several instances where then Minister of Public Enterprises, Mr Malusi Gigaba, took steps to undermine the authority of the Board and affect the strategic outcomes of the entity. These actions include:\n\nExcluding the Board Chair from Communication\nPublicly denouncing the Board, calling them unpatriotic and incompetent\nDelaying submission of the Request for a Guarantee to Treasury\nMisleading the Board Chair about the timeline of the proposal to Treasury Pressuring SAA to close the commercially viable Mumbai route.\nAppointing Ms Dudu Myeni as Board Chair despite her poor attendance record at SAA Board Meetings. (see Paragraphs 39 – 68)\n\nThe report also reveals that Mr Siyabonga Mahlangu, then adviser to the Minister of Public Enterprises, had an unquenchable thirst to visit the Gupta household. Mr Mahlangu’s affections for the Saxonwold compound also included inviting others to the address.\nOne such invitee was then Acting Chief Executive Officer, Mr Kona. Mr Kona alleges that Mr Tony Gupta offered him a large amount of money, which he refused. At the same meeting, Mr Kona informed Mr Tony Gupta about allocating a tender to Lufthansa Consultancy. In addition, Mr Kona alleged that Mr Tony Gupta was livid about the tender allocation.\nAfter the visit, Mr Kona received a call from the Director-General of Public Enterprises, Mr Tshediso Matona. The Director-General expressed his displeasure on the tender allocation to Lufthansa Consultancy. The grievances of the Director-General were to the extent that the Department of Public Enterprises deemed it necessary to investigate the tender. The investigation found no irregularities associated with the tender. Despite the findings of the investigation, the Department of Public Enterprises refused the continuance of the tender."
  },
  {
    "objectID": "posts/2023 TidyTuesday Week 1/index.html",
    "href": "posts/2023 TidyTuesday Week 1/index.html",
    "title": "2023 TidyTuesday Week 1",
    "section": "",
    "text": "1 INTRODUCTION\nIt has a been more than a year since, I published a #TidyTuesday submission, a weekly, data visualisation activity featuring a varied range of datasets. For the first week of 2023, the activity encourages participants to Bring their own data. Fortunately, I have been seating on a dataset from Independent Electoral Commission of South Africa (2023) containing the 2021 Local Government Election Results. The dataset is detailed, including results down to voting station level. The IEC also provides a data dictionary along with detailed methodology on how the results can be interpreted. We aren’t here to reproduce that process (that is for another day). We simply want to enhance the visualisation of the results.\n\n\nsee code\nlapply(c(\"tidyverse\",\"janitor\",\"arrow\",\n         \"sf\",\"ggthemes\",\"showtext\",\n         \"leaflet\"),\n       require,character.only = TRUE) |> \n  suppressWarnings() |> \n  suppressMessages()\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] TRUE\n\n[[6]]\n[1] TRUE\n\n[[7]]\n[1] TRUE\n\n\nsee code\nfont_add_google(\"IBM Plex Sans\")\nshowtext_auto()\n\n\n\n\n2 IMPORTANT DATA ASPECTS\nIn South Africa, the Municipal Demarcation Board (2023) , is the body responsible for the demarcation of South Africa into multiple levels such as District Municipalities, Metropolitan Municipalities, Local Municipalities and Voting Districts. In Local Government Elections, these demarcations in turn, determine seat allocation and across all types of municipalities.\nThere important nuances in respect to the Local Government Elections voting ballots; such as direct (Ward-level vote) and indirect votes(proportional representation). For purposes of this analysis, we will rely on illustrating voting outcomes at ward level. In effect, we have two datasets to work with, 2020 Municipal Demarcation Board ShapeFile and 2021 Local Government Election Results (Comma-Separated File).\nWrangling spatial data is made easy by the sf package ( Pebesma (2022) ). The voting results can be wrangled and joined to the sf object through the dplyr package ( Wickham (2022) ). Since this is R, there are a plethora of data visualisation packages to utilise as well. Here, we chiefly rely on two, ggplot2 and leaflet. The first can be used to create static maps while the latter offers a great set of tools for interactive visualisations. Below, we illustrate the process of importing that spatial data into R along with the csv of election results.\n\n\nsee code\nSA_Wards <- st_read(\"./2020_Spatial_Data/SA_Wards2020.shp\")\n\n\nReading layer `SA_Wards2020' from data source \n  `C:\\Users\\Sivuyile\\Desktop\\RStudio\\2022\\Personal_Site\\Personal_Site_3\\posts\\2023 TidyTuesday Week 1\\2020_Spatial_Data\\SA_Wards2020.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 4468 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 16.45189 ymin: -34.83417 xmax: 32.94498 ymax: -22.12503\nGeodetic CRS:  WGS 84\n\n\nsee code\nElection_Results <- read_parquet(file = \"./Final_Dbs/2016 - 2021_Local_Gov_Election_Results_2022-04-27.parquet\") |>\n  filter(election_year == 2021)\n\nglimpse(Election_Results)\n\n\nRows: 1,084,734\nColumns: 13\n$ source_file         <chr> \"./2021/EC.csv\", \"./2021/EC.csv\", \"./2021/EC.csv\",…\n$ province            <chr> \"Eastern Cape\", \"Eastern Cape\", \"Eastern Cape\", \"E…\n$ municipality        <chr> \"BUF - Buffalo City\", \"BUF - Buffalo City\", \"BUF -…\n$ ward                <chr> \"Ward 29200001\", \"Ward 29200001\", \"Ward 29200001\",…\n$ voting_district     <dbl> 10590151, 10590151, 10590151, 10590151, 10590151, …\n$ voting_station_name <chr> \"PEFFERVILLE CLINIC\", \"PEFFERVILLE CLINIC\", \"PEFFE…\n$ registered_voters   <dbl> 2724, 2724, 2724, 2724, 2724, 2724, 2724, 2724, 27…\n$ ballot_type         <chr> \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"P…\n$ spoilt_votes        <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ party_name          <chr> \"ABANTU BATHO CONGRESS\", \"AFRICA RESTORATION ALLIA…\n$ total_valid_votes   <dbl> 3, 6, 3, 7, 0, 286, 1, 1, 0, 0, 1, 0, 3, 615, 24, …\n$ date_generated      <chr> \"11/23/2021 4:17:50 PM\", \"11/23/2021 4:17:50 PM\", …\n$ election_year       <dbl> 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 20…\n\n\nAs mentioned above, we are only interested in Ward Ballot results, as a result, we filter out all indirect ballot types. Thereafter, we group the data by Province, Municipality and Ward and derive a sum of all valid votes. In other words, we exclude spoilt votes and do not consider voter turn out extra. We store the result in a data-frame called `Total _Votes`. As the name implies, the data-frame contains a sum of total valid votes per ward. Below, we add an additional variable, party-name, which helps us tally all the votes cast for a particular party or independent cast per ward. Finally, we filter for the maximum votes accumulated by the party/independent per ward.\n\n\nsee code\nTotal_Votes <- Election_Results |>\n  filter(ballot_type == \"Ward\") |> \n  group_by(across(.cols=c(province,municipality,ward))) |> \n  summarise(total_votes = sum(total_valid_votes),\n            .groups = \"drop\")\n\nWard_Results <-  Total_Votes |> \n  left_join(Election_Results |> \n              filter(ballot_type == \"Ward\") |>   \n  group_by(across(.cols=c(ward,party_name))) |> \n  summarise(party_votes= sum(total_valid_votes)) |>\n    filter(party_votes == max(party_votes)) |> \n    ungroup()) |> \n  mutate(support = 100/total_votes*party_votes)\n\nglimpse(Ward_Results)\n\n\nRows: 4,468\nColumns: 7\n$ province     <chr> \"Eastern Cape\", \"Eastern Cape\", \"Eastern Cape\", \"Eastern …\n$ municipality <chr> \"BUF - Buffalo City\", \"BUF - Buffalo City\", \"BUF - Buffal…\n$ ward         <chr> \"Ward 29200001\", \"Ward 29200002\", \"Ward 29200003\", \"Ward …\n$ total_votes  <dbl> 3803, 3299, 3326, 4524, 3506, 3421, 3446, 3917, 3359, 305…\n$ party_name   <chr> \"AFRICAN NATIONAL CONGRESS\", \"AFRICAN NATIONAL CONGRESS\",…\n$ party_votes  <dbl> 1748, 2883, 1630, 2561, 1912, 1420, 2174, 2397, 1513, 150…\n$ support      <dbl> 45.96371, 87.39012, 49.00782, 56.60920, 54.53508, 41.5083…\n\n\nTo finalise the ward results, the two data-frames are joined. Ultimately, we have a data-frame with the winner of the ward. To visualise the data, we join the spatial data to the ward results data-frame.\n\n\nsee code\nWard_Results <- left_join(SA_Wards,Ward_Results |>\n  mutate(ward = str_remove(ward,\"Ward\\\\s{1,}\")) |> \n  rename(WardID = ward) |> \n  select(-province))\n\n\nBefore completing our first visualisation, there are some mopping up required. These are mainly for aesthetic in nature, such as importing hex codes for each political party which largely resembles the respective party’s corporate colour and grouping colours for smaller parties (\\(parties with wards < 2\\)).\n\n\nsee code\nDescriptives_Results <- read_csv(file = \"./Final_Dbs/2021_Party_Results_Descriptives_2023-01-06.csv\")%>% \n  sapply(.,as.character)%>%\n  data.frame()\n\nDescriptives_Results$occurence <- as.integer(Descriptives_Results$occurence)\n\nOther_Politics <- paste(Descriptives_Results[Descriptives_Results$occurence <=2,\"party_name\"],\n      collapse = \",\")\n\nWard_Results <- Ward_Results |>\n  mutate(party_name = case_when(party_name %in% c(\"AL JAMA-AH,KAROO GEMEENSKAP PARTY\",\"TEAM SUGAR SOUTH AFRICA\",\"UMSOBOMVU RESIDENTS ASSOCIATION\",\"ACTIONSA,AZANIA RESIDENT PARTY\",\"BREEDEVALLEI ONAFHANKLIK\",\"CAPRICORN INDEPENDENT COMMUNITY ACTIVISTS FORUM\",\"CEDERBERG FIRST RESIDENTS ASSOCIATION\",\"DIENSLEWERINGS PARTY\",\"FORUM 4 SERVICE DELIVERY\",\"INDEPENDENT SOUTH AFRICAN NATIONAL CIVIC ORGANISATION\",\"LAND PARTY,NAMAKWA CIVIC MOVEMENT\",\"PLETT DEMOCRATIC CONGRESS\",\"SIYATHEMBA COMMUNITY MOVEMENT\",\"TSOGANG CIVIC MOVEMENT,UNITED DEMOCRATIC MOVEMENT\"\n) ~ \"OTHER\",\n                                TRUE ~ party_name))\n\nWards_Plot <- Ward_Results |> \nggplot()+\n  geom_sf(aes(fill = party_name),colour=\"black\")+\n  scale_fill_manual(values = \n                      c(\"AFRICAN NATIONAL CONGRESS\"= \"#FFD700\",\n              \"DEMOCRATIC ALLIANCE\"= \"#00008B\",\n              \"INKATHA FREEDOM PARTY\"=\"#FFFF00\",\n              \"INDEPENDENT\"=\"#90EE90\",\n              \"ECONOMIC FREEDOM FIGHTERS\"=\"#8b0000\",\n              \"MAPSIXTEEN CIVIC MOVEMENT\"=\"#9F2B68\",\n              \"NATIONAL FREEDOM PARTY\"=\"#808080\",\n              \"VRYHEIDSFRONT PLUS\"=\"#FFA500\",\n              \"PATRIOTIC ALLIANCE\"=\"#39FF14\",\n              \"ABANTU BATHO CONGRESS\"=\"#ADD8E6\",\n              \"AFRICAN PEOPLE'S MOVEMENT\"=\"#FFFFE0\",\n              \"GOOD\"=\"#F5E236\",\n              \"INDEPENDENT CIVIC ORGANISATION OF SOUTH AFRICA\"=\"#FA1138\",\n              \"SETSOTO SERVICE DELIVERY FORUM\"=\"#A5FA11\",\n              \"AL JAMA-AH\"= \"#334A05\",\n              \"KAROO GEMEENSKAP PARTY\"=\"#5BF5A3\",\n              \"TEAM SUGAR SOUTH AFRICA\"=\"#F2FF03\",\n              \"UMSOBOMVU RESIDENTS ASSOCIATION\"=\"#141212\",\n              \"OTHER\" = \"#FFC0CB\"))+\n  labs(title = \"2021 Local Government Election Results\",\n       subtitle = \"Ballot Type (Ward): Winning Party per ward in the 2021 South African Local Government Elections.\\nThe results do not include other ballot types such as DC 40% or Proportional Representation.\",\n       caption = \"Data Source: https://www.elections.org.za/\\nPlot: Sivuyile Nzimeni(sivuyilenzimeni.netlify.app)\",\n       fill = \"Party Name\")+\n  theme_void()+\n  theme(text = element_text(\"IBM Plex Sans\"),\n        plot.title =element_text(hjust=0.5,face = \"bold\"),\n        plot.subtitle = element_text(hjust = 0.5,face = \"italic\"),\n        plot.caption = element_text(hjust=0.8,face = \"italic\"),\n        legend.position = \"bottom\")\n\nWard_Results <-Ward_Results |> \n  left_join(Descriptives_Results |> select(-occurence))\n\nWard_Results <- Ward_Results |> \n  mutate(overview = paste0(\"<strong>\",Province,\"</strong>\",\"<br/>\",\n                              \"<strong>\",Municipali,\"</strong>\",\"<br/>\",\n                              '<strong>Ward ID</strong>',\"<br/>\",WardID,\"<br/>\",\n                              \"<strong>Winner</strong>:\",party_name,\"<br/>\",\n                              \"<strong>Support</strong>: \",round(support,2),\"%\"),\n         colour = str_trim(colour))\n\nWard_Results <- Ward_Results |> \n  mutate(overview = lapply(overview,htmltools::HTML)) |> \n  unnest(overview)\n\n\n\n\n3 VISUALISATION\n\n\nsee code\nWards_Plot\n\n\n\n\n\nThe resulting static visualisation illustrates that the African National Congress (ANC) won the majority of wards in South Africa followed by the Democratic Alliance (DA) in some urban areas. The visualisation above has be read in context.\n\nLand does not vote: the composition of wards is influenced by several factors such as population density, level of development, natural landscape etc. As such, it is possible to have a large ward (by land size) with minimal low density and vice versa.\nWard-Outcomes are partial: in Local Government Elections, voters have at least two ballots, direct votes and indirect votes. For Example, ActionSA obtained one ward in the 2021 LGE, they obtained more than 50 seats in the provinces where they competed.\nRigour: for a more comprehensive analysis, spatial econometrics offers a number of tools to help understand voting patterns over time. Naturally, the datasets required would need to be expanded to include Statistics South Africa (Census), previous voting patterns (IEC) and former demarcations (MDB).\n\n\n\n\n\n\n\nRendering Issue\n\n\n\nR has several packages for interactive visualisation. In the code, we attempted to use leaflet to render the interactive visualisation. The size of the data appears to be a hindrance, instead generating an error Fatal javascript OOM in Reached heap limit. Other users have experienced this issue. Follow Issue 2462 for more information and progress on the issue.\nThe rendering works locally. The script above provides the cleaning process before creating an interactive map in `leaflet` or another interactive visualisation library.\n\n\n\n\n4 SUMMARY\nIn the post, we imported a spatial data file from MDB along with election results from the IEC. Ward-Level outcomes are visualised through a choropleth map.\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\nReferences\n\nIndependent Electoral Commission of South Africa. 2023. ‘Municipal Election Results - Electoral Commission of South Africa’. https://results.elections.org.za/home/downloads/me-results.\n\n\nMunicipal Demarcation Board. 2023. ‘Municipal Demarcation Board’. https://dataportal-mdb-sa.opendata.arcgis.com/.\n\n\nPebesma, Edzer. 2022. Sf: Simple Features for r. https://CRAN.R-project.org/package=sf.\n\n\nWickham, Hadley. 2022. Tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse."
  }
]