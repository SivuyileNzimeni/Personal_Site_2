[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Sivuyile Nzimeni is a Data Analyst in the Faculty of Economic and Management Sciences, University of the Free State."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nAugust 2022 - Present | Data Analyst| Office of the Dean: Faculty of Economic and Management Sciences| University of the Free State\nNov 2020 - July 2022| Data Analyst; Teaching and Learning Coordinator | Centre for Teaching and Learning & Faculty of Economic and Management Sciences| University of the Free State\nJan 2018 - Nov 2020| Teaching and Learning Coordinator: Economic and Management Sciences | Centre for Teaching and Learning | University of the Free State\nJan 2017 - Dec 2017| Research Assistant | Department of Business Management | University of the Free State\nJul 2017 - Nov 2017| Part-Time Lecturer |Department of Business Management| University of the Free State\nNov 2015 - Nov 2016| Intern | Office of the Dean: Economic and Management Sciences |University of the Free State"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\n2016| University of the Free State |B.Com Honours with specialisation in Entrepreneurial Management\n2015| University of the Free State |B.A with majors in Business Management and Philosophy"
  },
  {
    "objectID": "about.html#interests",
    "href": "about.html#interests",
    "title": "About",
    "section": "Interests",
    "text": "Interests\n\nR Programming\nData Analysis in Higher Education\nManagement Studies\nEconomics\nSocial Media Analysis\nNatural Language Processing"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "About",
    "section": "Publications",
    "text": "Publications\nNzimeni, S. Mofokeng, M. 2022. ‘Automating tutorial attendance register capturing, preliminary results from a pilot project’. Siyaphumelela 2022: A Saide Project. Online, 25 - 27 June 2022.\nMuller, A. Nzimeni, S. Janse van Vuuren, Corlia. 2021. ‘Curriculum enhancement: reflections on the use of data, holistic student support and disciplinary skills development on a decade-long transformative journey’. 2021 University of the Free State Annual Teaching and Learning Conference.\nNzimeni, S. 2019. ‘Enrolment versus attendance: A preliminary investigation into the cost of tutorials’. Siyaphumelela 2019: A Saide Project. Johannesburg, 25 - 27 June 2019.\nNzimeni, S. Smit, AVA. 2018. ‘Is the quality of education impacting the global competitiveness of the South African business environment?’ 30th Annual Conference of the South African Institute of Management Scientists: Conference Proceedings, Stellenbosch University, Stellebosch, 16 - 19 September 2018."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "2022 Ask A Manager Salary Survey\n\n\n\nData Modelling\n\n\nData Cleaning\n\n\n\nAnalysing the 2022 Ask A Manager Salary Survey. In this post, we detail the data preprocessing procedure, feature-selection and modelling process to estimate salaries among…\n\n\n\nSivuyile Nzimeni\n\n\nDec 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022 Eskom Unavailabe Capacity\n\n\n\nnews\n\n\nAPI\n\n\nvisualisation\n\n\n\nVisualising Unavailable Energy in South Africa from 2018 to 2022.\n\n\n\nSivuyile Nzimeni\n\n\nSep 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022 Food Prices: Part 1\n\n\n\nnews\n\n\nAPI\n\n\ntable\n\n\n\nThis post explores 2020 - 2022 Food Prices in South Africa\n\n\n\nSivuyile Nzimeni\n\n\nJun 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSouth African Car Prices\n\n\n\nweb-scraping\n\n\ndata analysis\n\n\ndata modelling\n\n\n\nThis post details the data scraping process for obtaining data from a South African online vehicle marketplate. In addition, the post shares the results of a linear…\n\n\n\nSivuyile Nzimeni\n\n\nFeb 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2021 South African Local Government Election Results\n\n\n\nspatial data\n\n\ndata Cleaning\n\n\nvisualisation\n\n\n\nVisualising Spatial Data in R using data from the IEC and Municipal Dermacation Board in South Africa.\n\n\n\nSivuyile Nzimeni\n\n\nJan 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nState of Capture Commission Report Part 1: Notes\n\n\n\nnews\n\n\npolitics\n\n\nlegal\n\n\n\nThis posts is the first installment of notes from reading the State of Capture Commission’s Report Part 1. This post post focuses on the first few pages of the report.\n\n\n\nSivuyile Nzimeni\n\n\nJan 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDepartment of Basic Education: Schools Database\n\n\n\nweb-scraping\n\n\ndata cleaning\n\n\n\nThis post details the data scraping process for obtaining the schools database of from the Department of Basic Education in South Africa.\n\n\n\nSivuyile Nzimeni\n\n\nDec 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAsk A Manager Survey\n\n\n\ndata analysis\n\n\ndata modelling\n\n\n\nThis post is about the 2021 Ask A Manager Survey analysis. It provides an overview of the data analysis process attempting to fit a linear regression model to explain…\n\n\n\nSivuyile Nzimeni\n\n\nDec 25, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021 Ask A Manager Salary Survey/index.html",
    "href": "posts/2021 Ask A Manager Salary Survey/index.html",
    "title": "Ask A Manager Survey",
    "section": "",
    "text": "In early 2021, the Ask A Manager blogsite ran their annual salary survey. The survey responses are stored on googlesheets, making the data accessible to all interested. In a previous post, we discussed data pre-processing and feature selection method. This post focuses two aspects,namely, 1) Reproducibility and 2) Out-of-Sample Testing.\n\n\nIn the previous post, we detailed the feature selection method by regularised regreesion, specifically, LASSO regression. In this post, we will attempt to reproduce the feature selection method.\n\n\n\nIn this post, we will also follow the traditional machine learning workflow including, splitting the data into a training and testing samples, fitting a model, cross-validation and finally fitting the model on the out-of-sample dataset(testing dataset). This approach can inform us about the model’s parsimony. In other words, can the model perform well on an unknown sample."
  },
  {
    "objectID": "posts/2021 Ask A Manager Salary Survey/index.html#important-variable-extraction",
    "href": "posts/2021 Ask A Manager Salary Survey/index.html#important-variable-extraction",
    "title": "Ask A Manager Survey",
    "section": "IMPORTANT VARIABLE EXTRACTION",
    "text": "IMPORTANT VARIABLE EXTRACTION\nThe resulting object is a list of class “glmnet”. Essentially, the results contain all the iterations through alpha and to find the minimum lambda. As such, there are several results in the object. We are primarily interested in extracting the remaining variables at minimum lambda along with their coeffiecients. The vip package can automate the plotting of the results. However, we want to extract the variables names in order to fit them on our training test. There is probably a package to assist with this step somewhere in the wild, however, we haven’t found it yet. Luckily in R, we can write custom functions. The code below details the variable_extractor function.\n\n\nsee code\nvariable_extractor <- function(a_list){\n  min_lambda <- data.frame(best_lambda =a_list[[\"lambda\"]]==min(a_list[[\"lambda\"]]))\n  min_lambda <- cbind.data.frame(data.frame(index = rownames(min_lambda)),\n                                 min_lambda)\n  min_lambda <- min_lambda %>% \n    filter(best_lambda == TRUE)\n  min_lambda <- as.double(unique(min_lambda$index))\n  lasso_variables <- as.matrix(coef(a_list))|>data.frame()\n  lasso_variables <- cbind.data.frame(variables = rownames(lasso_variables),\n                                      lasso_variables)\n  lasso_variables <- tibble(lasso_variables)\n  lasso_variables <- lasso_variables[,c(1,min_lambda+1)]\n  names(lasso_variables) <- c(\"variable\",\"importance\")\n  lasso_variables <- lasso_variables %>% \n    filter(variable != \"(Intercept)\",\n           importance != 0.00000000) %>% \n    arrange(desc(importance))\n\n  return(lasso_variables)\n}\n\nlasso_variable <- variable_extractor(lasso)\n\n\nThe function takes a list of attribute “glmnet”. Thereafter, we find the smallest lambda. In addition, we extract all the coefficients from the object and store them as a wide dataframe. The dataframe is subset to only contain the variable name along with the coefficients of the smallest lambda value. We discard the intercept and variable with coefficients that are equal to 0.0000000. The final output is identical to the variables extracted by the vip package. Finally, we subset both the training dataset and the testing dataset to only contain the selected independent variables. Since the outcome variable is expressed in USD terms, we log the outcome variable."
  },
  {
    "objectID": "posts/2021 Ask A Manager Salary Survey/index.html#tidymodels-a-clean-interface-for-maching-learning",
    "href": "posts/2021 Ask A Manager Salary Survey/index.html#tidymodels-a-clean-interface-for-maching-learning",
    "title": "Ask A Manager Survey",
    "section": "TIDYMODELS: A CLEAN INTERFACE FOR MACHING LEARNING",
    "text": "TIDYMODELS: A CLEAN INTERFACE FOR MACHING LEARNING\nThe R programming language doesn’t not lack methods for running machine learning algorithms, it is after all, a statistical programming language. The tidymodels metapackage aims to provide a standard interface for modelling and machine learning using tidyverse principles. Below, we use the package to complete a number of steps including, crossfold_validation, model specification, fitting on the resamples and finally fitting the model on the training dataset.\n\n\nsee code\nSalary_Folds <- vfold_cv(Salary_Train)\n\nlm_spec <- linear_reg(engine = \"lm\")\n\nlm_recipe <- recipe(new_annual_salary ~.,Salary_Train) %>% \n  step_nzv(all_predictors())\n\nlm_wf <- workflow(lm_recipe,lm_spec)\n\ndoParallel::registerDoParallel(cores = 10)\nctrl_preds <- control_resamples(save_pred = TRUE)\ncv_results <- fit_resamples(lm_wf,Salary_Folds,control = ctrl_preds)\n\nlm_wf <- fit(lm_wf,Salary_Train)\n\ncollect_metrics(cv_results,summarize = FALSE) %>% \n  filter(.metric == \"rsq\") %>% \n  summarise(avg_estimate = mean(.estimate),\n            .groups = \"drop\")\n\n\n# A tibble: 1 × 1\n  avg_estimate\n         <dbl>\n1        0.270\n\n\nAcross all 10 validation folds, the linear regression model’s adjusted R-square averaged 0.272. It is possible to tune the parameters and use battery of other machine learning models to improve performance. In the previous post, we utilised random forest to try an improve the model. Despite, requiring additional computational power, the increases in peformance were marginal to neglible.\nAt this point, we have completed our first objective. We are able to reproduce the LASSO regresison results in the previous post. The next objective is to determine the parsimony of the model. Here, we fit model on the test data."
  },
  {
    "objectID": "posts/2021 Department of Basic Education Scraping/index.html",
    "href": "posts/2021 Department of Basic Education Scraping/index.html",
    "title": "Department of Basic Education: Schools Database",
    "section": "",
    "text": "CONCLUSION\nUsing R to clean data is a wise choice. This post highlighted an example of implementation on a relatively small dataset. The final dataset can be used to match school performance reports regularly published by the Department of Basic Education.\n\n\nREFERENCES\nWickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686\nSam Firke (2021). janitor: Simple Tools for Examining and Cleaning Dirty Data. R package version 2.1.0. https://CRAN.R-project.org/package=janitor\nHadley Wickham and Jennifer Bryan (2019). readxl: Read Excel Files. R package version 1.3.1. https://CRAN.R-project.org/package=readxl\nJeroen Ooms (2021). writexl: Export Data Frames to Excel ‘xlsx’ Format. R package version 1.4.0. https://CRAN.R-project.org/package=writexl\nHadley Wickham (2021). rvest: Easily Harvest (Scrape) Web Pages. R package version 1.0.2. https://CRAN.R-project.org/package=rvest\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021 Second-Hand Cars in South Africa/Index.html",
    "href": "posts/2021 Second-Hand Cars in South Africa/Index.html",
    "title": "South African Car Prices",
    "section": "",
    "text": "South Africa has a plethora of online vehicle marketplaces. Often, their pool of vehicles for sale are usually > 50 000 on a daily basis. The vehicle listings offer a vast amount of car related data. Naturally, web-scraping the data provides an opportunity to fit a Machine-Learning model and endless exploration for petrol-heads (such as myself). Nearly all the online vehicle marketplaces have restrictive Terms and Conditions deterring the use of their data for commercial purposes and bombarding of their servers through web scrapping among other restrictions. Unfortunately, this means web scraping script cannot be shared in this post as it may reveal where the data were obtained, methodology of scraping the data etc. In addition, the website of the online vehicle marketplace will not be revealed."
  },
  {
    "objectID": "posts/2021 Second-Hand Cars in South Africa/Index.html#importing-the-data",
    "href": "posts/2021 Second-Hand Cars in South Africa/Index.html#importing-the-data",
    "title": "South African Car Prices",
    "section": "IMPORTING THE DATA",
    "text": "IMPORTING THE DATA\nBelow, we import the data and use dplyr::glimpse function to see the number of columns and values. The dataset contains a few important variables including the car_name and vehicle manufacturer. It is worth noting that the car_name variable appears to a free text field where the person listing the vehicle can modify the car name to include marketing terms such as: “excellent condition”,“reduce price” etc.\n\n\nsee code\nCar_Data <- read_csv(file =\"./2021-09-03_MANY_VEHICLES_Clean_Db.csv\") %>% \n  clean_names()\n\nglimpse(Car_Data)\n\n\nRows: 62,196\nColumns: 10\n$ car_name             <chr> \"9-3 Sport 2.0 Linear Lpt\", \"S40 2.0T\", \"V40 2.0\"…\n$ vehicle_manufacturer <chr> \"Saab\", \"Volvo\", \"Volvo\", \"Hyundai\", \"Volvo\", \"Ho…\n$ year                 <dbl> 2007, 1999, 2001, 2000, 2000, 1998, 1996, 1996, 2…\n$ mileage              <dbl> 200000, 285000, 271000, 125000, 190000, 267000, 1…\n$ price                <dbl> 13700, 18900, 18900, 20000, 20900, 21900, 21900, …\n$ fuel_type            <chr> \"Petrol\", \"Petrol\", \"Petrol\", \"Petrol\", \"Petrol\",…\n$ transmission         <chr> \"Manual\", \"Manual\", \"Manual\", \"Manual\", \"Automati…\n$ dealership           <chr> \"Mahala Motors\", \"WeBuyCars Midstream\", \"WeBuyCar…\n$ city_town            <chr> \"Klerksdorp\", \"Centurion\", \"Cape Town\", \"Johannes…\n$ province             <chr> \"North West Province\", \"Gauteng\", \"Western Cape\",…\n\n\nSimilarly, the free text field, also means that vehicle naming conventions deviate from the vehicle manufacturer specifications. Other text variables also contain these anomalous changes in text. The code below demonstrates an example of idiosyncrasies.\n\n\nsee code\nCar_Data %>% \n  filter(str_detect(car_name,\"condition\"))\n\n\n# A tibble: 4 × 10\n  car_name  vehic…¹  year mileage  price fuel_…² trans…³ deale…⁴ city_…⁵ provi…⁶\n  <chr>     <chr>   <dbl>   <dbl>  <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n1 Focus Ex… Ford     2008  305000  69950 Petrol  Manual  Cars a… Brakpan Gauteng\n2 Utility … Chevro…  2015  198000 114900 Petrol  Manual  United… Boksbu… Gauteng\n3 Clio Exc… Renault  2019   55000 160000 Petrol  Manual  Bright… Johann… Gauteng\n4 Ranger E… Ford     2017  107000 299900 Diesel  Automa… Bright… Johann… Gauteng\n# … with abbreviated variable names ¹​vehicle_manufacturer, ²​fuel_type,\n#   ³​transmission, ⁴​dealership, ⁵​city_town, ⁶​province"
  },
  {
    "objectID": "posts/2021 Second-Hand Cars in South Africa/Index.html#data-preprocessing",
    "href": "posts/2021 Second-Hand Cars in South Africa/Index.html#data-preprocessing",
    "title": "South African Car Prices",
    "section": "DATA PREPROCESSING",
    "text": "DATA PREPROCESSING\nWe would like to fit a model to better understand the determinants of vehicle prices. Here, pre-processing is important is especially important. Tidymodels and the textrecipes offer a range of functions to handle the whole modelling workflow. Below, are a number of pre-processing steps, firstly we use the unnest_tokens function from the tidytext package to process the the car_name variable. Thereafter, we handle the numerical values by applying a logarithm to outcome variable and standardising mileage. Finally, we use step_tokenize, step_tokenfilter and step_tf to tokenise, filter the those tokens and ultimately convert the tokens to a term frequency variables.\n\n\nsee code\nUseful_Car_Names <- Car_Data %>% \n  unnest_tokens(car_name,\n                output=\"car_name\") %>% \n  group_by(vehicle_manufacturer,car_name) %>% \n  summarise(n(),.groups = \"drop\") %>% \n  arrange(desc(`n()`)) %>% \n  anti_join(stop_words %>% \n              rename(car_name=word)) %>% \n  filter(!str_detect(car_name,\"\\\\d{1,}\"))\n\nCar_Data <- Car_Data %>% \n  unnest_tokens(car_name,output = \"car_name\") %>% \n  semi_join(Useful_Car_Names %>% \n              select(car_name,vehicle_manufacturer)) %>%\n  group_by(across(-car_name)) %>% \n  summarise(car_name = as.character(list(c(car_name))),\n            .groups = \"drop\") %>% \n  mutate(car_name = str_replace(car_name,\n                                'c[[:punct:]]{2,}',\"\"),\n         car_name = str_replace_all(car_name,\n                                    '\\\\\"',\"\"),\n         car_name = str_replace_all(car_name,\n                                    '[[:punct:]]{1,}$',\"\"),\n         age = 2021-year)\nCar_Data <- Car_Data %>% \n  mutate(across(c(car_name,dealership,city_town),\n                .fns = as_factor))\n\nNew_Car_Data <- recipe(price ~ .,data = Car_Data) %>% \n  step_log(all_outcomes()) %>%\n  step_normalize(mileage) %>% \n  step_tokenize(c(dealership,city_town,car_name)) %>%\n  step_tokenfilter(c(dealership,city_town,car_name)) %>%\n  step_tf(c(dealership,city_town,car_name)) %>% \n  step_dummy(c(vehicle_manufacturer,province,\n               fuel_type,transmission)) %>% \n  step_nzv(all_predictors()) %>% \n  prep() %>% \n  bake(Car_Data)"
  },
  {
    "objectID": "posts/2021 Second-Hand Cars in South Africa/Index.html#lasso-it-once-lasso-it-until-you-can-also-no-more",
    "href": "posts/2021 Second-Hand Cars in South Africa/Index.html#lasso-it-once-lasso-it-until-you-can-also-no-more",
    "title": "South African Car Prices",
    "section": "LASSO IT ONCE, LASSO IT UNTIL YOU CAN ALSO NO MORE",
    "text": "LASSO IT ONCE, LASSO IT UNTIL YOU CAN ALSO NO MORE\nThe resulting dataset contains 59417 rows across 27 variables. Given the dimension above, it prudent to do some additional feature select. LASSO regression helps with variable selection. In turn, we use the LASSO regression results to filter for the appropriate variables. The final variable set yields 24 predictor variables. The code below contains all details the LASSO implementation and subsequent filtering.\n\n\nsee code\nNew_Car_Data <- New_Car_Data %>%\n  select(-year)\n\nCar_split <- initial_split(New_Car_Data)\nCar_Training <- training(Car_split)\nCar_Test <- testing(Car_split)\n\nX <- model.matrix(price~.,Car_Training)[,-1]\nY <- Car_Training$price\nlasso_model <- glmnet(x = X,y=Y,\n       alpha=1)\n\nvariable_extractor <- function(a_list){\n  min_lambda <- data.frame(best_lambda =a_list[[\"lambda\"]]==min(a_list[[\"lambda\"]]))\n  min_lambda <- cbind.data.frame(data.frame(index = rownames(min_lambda)),\n                                 min_lambda)\n  min_lambda <- min_lambda %>% \n    filter(best_lambda == TRUE)\n  min_lambda <- as.double(unique(min_lambda$index))\n  lasso_variables <-\n    as.matrix(coef(a_list))|>data.frame()\n  \n  lasso_variables <- cbind.data.frame(variables = rownames(lasso_variables),\n                                      lasso_variables)\n  \n  lasso_variables <- tibble(lasso_variables)\n  lasso_variables <- lasso_variables[,c(1,min_lambda+1)]\n  names(lasso_variables) <- c(\"variable\",\"importance\")\n  lasso_variables <- lasso_variables %>% \n    filter(variable != \"(Intercept)\",\n           importance != 0.00000000) %>% \n    arrange(desc(importance))\n\n  return(lasso_variables)\n}\n\nVariables <- variable_extractor(lasso_model)\nCar_Training <- Car_Training[,c(\"price\",Variables$variable)]\nCar_Test <- Car_Test[,c(\"price\",Variables$variable)]"
  },
  {
    "objectID": "posts/2021 Second-Hand Cars in South Africa/Index.html#cross-validation",
    "href": "posts/2021 Second-Hand Cars in South Africa/Index.html#cross-validation",
    "title": "South African Car Prices",
    "section": "CROSS VALIDATION",
    "text": "CROSS VALIDATION\nBefore fitting to the test dataset, it worth investigating whether our model performs well against “shuffled” dataset of our training data. Fortunately, tidymodels and parsnip contain several functions to assist with the exercise.\nHere, we use the vfold_cv function to split our training data into random splits of equal size. Next, we use workflow to fit a linear model on the random splits. Subsequently, we plot the r-squares across all the folds.\nUltimately, cross validation helps us understand the performance of our model set of datasets by iterating through the training and test sample of each fold. The code below illustrates an implementation of cross validation.\n\n\nsee code\nCar_Training_cv <- vfold_cv(Car_Training)\n\nlinear_model <- linear_reg() %>% \n  set_engine(\"lm\")\n\ncv_outcomes <- workflow() %>% \n  add_model(linear_model) %>% \n  add_formula(price ~.) %>% \n  fit_resamples(Car_Training_cv) %>% \n  select(id,.metrics) %>% \n  unnest(.metrics) %>% \n  filter(.metric == \"rsq\") %>% \n  select(id,.metric,.estimator,.estimate)\n\ncv_plot <- cv_outcomes %>%  \n  ggplot(aes(id,.estimate,group=1,fill=id))+\n  geom_col(show.legend = FALSE,alpha=0.8)+\n  geom_text(aes(label=round(.estimate,2)))+\n  coord_flip()+\n  labs(title = \"Cross Validation Results\",\n       subtitle = \"Car Prices Linear Model: R-Square across 10 folds\",\n       x=NULL,\n       y = \"R-Square\")+\n  theme(plot.title = element_text(family = \"Arial Narrow\",\n                                  hjust = 0.5),\n        plot.subtitle = element_text(family = \"Arial Narrow\",\n                                     hjust = 0.5,face = \"italic\"))"
  },
  {
    "objectID": "posts/2022 Eskom Capacity/index.html",
    "href": "posts/2022 Eskom Capacity/index.html",
    "title": "2022 Eskom Unavailabe Capacity",
    "section": "",
    "text": "CONCLUSION\nIn this post, we visualised data Capacity Loss Factor data from Eskom. The goal was visualisation, a more detailed analysis would employ time series analysis to determine the whether the data is stationary or non-stationary, selecting the appropriate filters etc. Woodward Wayne, Gary, Henry, and Elliot, Allan (n.d.) provides a detailed treatment of applied time series analysis in R. Fortunately, the energy crisis at Eskom is sufficiently glaring as to require no sophisticated analysis.\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\nReferences\n\nBloomberg. n.d. “South Africa Hits 100 Days of Load-Shedding in 2022.” https://mybroadband.co.za/news/energy/460710-south-africa-hits-100-days-of-load-shedding-in-2022.html.\n\n\nCoene, John. 2022. Echarts4r: Create Interactive Graphs with Echarts JavaScript Version 5. https://CRAN.R-project.org/package=echarts4r.\n\n\nGrootes, Stephen. 2019. “ANALYSIS: Twelve Years of Load Shedding  Written, Starring & Directed by the ANC.” https://www.dailymaverick.co.za/article/2019-12-09-twelve-years-of-load-shedding-written-starring-directed-by-the-anc/.\n\n\nR Core Team. 2022. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2022. Tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWoodward Wayne, Gary, Henry, and Elliot, Allan. n.d. “Applied Time Series Analysis with R.” https://www.routledge.com/Applied-Time-Series-Analysis-with-R/Woodward-Gray-Elliott/p/book/9781032097220."
  },
  {
    "objectID": "posts/2022 Food Prices/index.html",
    "href": "posts/2022 Food Prices/index.html",
    "title": "2022 Food Prices: Part 1",
    "section": "",
    "text": "Food Price inflation has been the subject of much media coverage. Obtaining source data to assess prices changes across of a variety of food products can be a difficult exercise. Aside from Statistics South Africa, Pietermaritzburg Economic Justice & Dignity group (PMBEJD) and others provide regular estimates of price changes. On the other hand,the Trundler API provides a robust API to track prices across many retailers in South Africa and abroad. Below, we use the Trundler API to access data for six food categories namely:\n\nEggs\nMilk\nMaize\nRice\nSunflower Oil\nWhite Sugar\n\nUsing the Trundler API, we narrowed the product search to a handful of competing products in two large South African Retailers. As a result, we don’t share the data extraction process in this post to protect the data extracted and API keys associated with them.\nThe table below provides an overview of food prices for the period 2020 - 2022. The able is interactive, one can sort by food category, retailer id, date columns and price.\n\n\n\n2020 - 2022 Price Changes\nPrice changes across six categories of foods: Eggs, Maize, Milk, Rice, Sunflower Oil and White Sugar\n\nData Source: Trundler API| Visualisation: Sivuyile Nzimeni| Date Extracted: 2022/06/01\n\n\n\n\nThere are a few notable observations. Excella Sunflower Oil 2L increased from R50 in 2020-06-07 to 20222-05-30 R110, a 120 percent increase. The price of Milk has remained stable throughout the past two years. Tastic Soft & Absorbing Long Grain White Rice 2kg has increased from R30 to R39, a 30 percent increase.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022 State Capture Report 1/index.html",
    "href": "posts/2022 State Capture Report 1/index.html",
    "title": "State of Capture Commission Report Part 1: Notes",
    "section": "",
    "text": "CONCLUSION\nThis is the tip of the iceberg. There several specific allegation against Ms Dudu Myeni and Ms Kwinana. These will be discussed in subsequent summaries (trying to keep the word count below 1000). For now, it clear that State Capture was (or is) present at South African Airways. The following post will detail specific incidents of how State Capture took hold of the entity with specific reference to the conduct of Ms Dudu Myeni."
  },
  {
    "objectID": "posts/2022_NYC_Elevators/Index.html",
    "href": "posts/2022_NYC_Elevators/Index.html",
    "title": "2022 TidyTuesday NYC Elevators",
    "section": "",
    "text": "see code\nmissingness <- function(a_vec){\n proportion <- round(100/length(a_vec)*a_vec[is.na(a_vec)] |> length(),2)\n return(proportion)}\n\n\n\n\nsee code\nElevators <- elevators\n\nMissing_Proportion <- sapply(Elevators,missingness) |> \n  as.data.frame()\n\nMissing_Proportion <- cbind(rownames(Missing_Proportion),Missing_Proportion)\nnames(Missing_Proportion) <- c(\"variable\",\"missing_proportion\")\n\n\n\n\nsee code\ntibble(Missing_Proportion) |> print(n=25)\n\n\n# A tibble: 25 × 2\n   variable               missing_proportion\n   <chr>                               <dbl>\n 1 device_number                        0   \n 2 bin                                  0   \n 3 tax_block                            0.01\n 4 tax_lot                              0.01\n 5 house_number                         0   \n 6 street_name                          0   \n 7 zip_code                            10.6 \n 8 borough                              0   \n 9 device_type                          0   \n10 lastper_insp_date                    2.94\n11 approval_date                       38.2 \n12 manufacturer                        74.9 \n13 travel_distance                     28.0 \n14 speed_fpm                            0   \n15 capacity_lbs                         0.63\n16 car_buffer_type                     33.0 \n17 governor_type                       31.5 \n18 machine_type                         6.21\n19 safety_type                         23.8 \n20 mode_operation                       4.57\n21 floor_from                           0.5 \n22 floor_to                             3.3 \n23 latitude                             0   \n24 longitude                            0   \n25 elevators_per_building               0   \n\n\n\n\nsee code\nElevators <- Elevators[,names(Elevators) %in% c(Missing_Proportion[Missing_Proportion$missing_proportion < 5,\"variable\"])]\n\n\n\n\nsee code\nElevators |> \nggplot(aes(longitude,latitude))+\n  geom_point(aes(color = borough),\n             show.legend = FALSE)+\n  theme_void()+\n  labs(title = \"2022 NYC Elevators\",\n       subtitle = \"NYC elevators across the five boroughs\",\n       x=NULL,y=NULL,\n       caption = paste0(\"Plot: Sivuyile Nzimeni\\nData Source: Hvitfeldt E (2022). _elevators: Data Package Containing Information About Elevators in NYC_.\\nhttps://github.com/EmilHvitfeldt/elevators\\n https://emilhvitfeldt.github.io/elevators/.\"))+\n  theme(text = element_text(family = \"IBM Plex Sans\"),\n        plot.title = element_text(face = \"bold\",hjust=0.5),\n        plot.subtitle = element_text(face = \"italic\",hjust=0.5),\n        plot.caption = element_text(face = \"italic\",size =3))\n\n\n\n\n\nPlot of NYC Elevators\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022 Ask A Manager Salary Survey/index.html",
    "href": "posts/2022 Ask A Manager Salary Survey/index.html",
    "title": "2022 Ask A Manager Salary Survey",
    "section": "",
    "text": "Last year, we analysed the 2021 Ask A Manager Salary Survey. The dataset is interesting for many reasons including,most beneficially, the dataset is open and available for all interested parties. It is also offers an opportunity to move beyond old and boring datasets such as mtcars and iris. The results of the survey offer an opportunity to clean open-text fields, currency data and feature engineering for data modelling.\nFortunately, there is a 2022 Ask A Manager Salary Survey that is also freely available. Notably, there were only 15 465 respondents compared to 27 919 responds in the previous survey. It is difficult to estimate the reasons for the reduction in responses. Nonetheless,there are sufficient responses to model upon. The second batch of responses offers an opportunity to refine the data preprocessing and modelling process.\n\n\nUnlike in the previous post, the data preprocessing step is contained in a stand-alone script ./Scripts/2022_Data_Cleaner.R. This helps with two main aspects; i) practising modular scripts and ii) reducing the size of the analysis. The data cleaning script has several components in it. The table below summarises the aforementioned components.\n\n\nsee code\nlibrary(gt)\nlibrary(gtExtras)\n\ndata.frame(section = c(\"Importing Libraries\",\"Importing Data\",\"Open Text Cleaning\",\n                       \"List to Vec\",\"Completeness Check\"),\n           script_sample =c('lapply(as.list(c(\"tidyverse\",\"janitor\",\"arrow\",\n                 \"ggthemes\",\"tidytext\",\"tidymodels\",\n                 \"textrecipes\",\"glmnet\")),\n       require,character.only=TRUE) |>\n  suppressWarnings() |> \n  suppressMessages() |> \n  invisible()','read_csv(file = \"./Data/Ask A Manager Salary Survey 2022_Responses.csv\") |> \n  clean_names() |> \n  mutate(timestamp = mdy_hms(timestamp)) |> \n  filter(!is.na(job_title)) |> \n  mutate(additional_compensation = ifelse(is.na(additional_compensation),\n                                          0,additional_compensation),\n         full_compensation = salary+additional_compensation) |> \n  filter(!currency %in% c(\"Other\",\"HKD\")) |> \n  rownames_to_column(\"respondent\")','text_cleaner <- function(a_vec){\n  tokens <- str_split(a_vec,pattern=\"\\\\s{1,}\") |> \n    unlist()\n  tokens <- tokens[!grepl(\"\\\\s{1,}|[[:punct:]]|\\\\d{1,}\",tokens)] |> str_to_lower()\n  tokens <- unique(tokens) \n  tokens <- tokens[!tokens %in% c(stopwords::data_stopwords_nltk[[\"en\"]])]\n  return(tokens)}','list_to_vec <- function(a_vec){\n  new_vec <- a_vec |> \n    as.character()\nnew_vec <- gsub(\"(^c+[[:punct:]])|[[:punct:]]\",\n                replacement = \"\",\n                new_vec)\n  return(new_vec)}\n','completeness <- function(a_vec){\n  incomplete <- round(100/length(a_vec)*a_vec[is.na(a_vec)] |> length(),2)\nreturn(incomplete)}\n'),\n           reasoning = c(\"importing libraries required for preprocessing\",\n                         \"import data, convert date column to date type, remove unviable cases and create total compensation columns\",\n                         \"split open-text fields into tokens, remove punctuation, spaces and stopwords. The function returns a list of words per case\",\n                         \"Convert the list of words per case to a vector.\",\n                         \"Check the level of missing values in each column.\")) |> \n  gt() |> \n  gt_theme_nytimes() |> \n  tab_header(title = \"2022 Data Clean Script Overview\",\n                 subtitle = \"An overview of sections in the data cleaning script\") |> \n  opt_align_table_header(align = \"center\") |> \n  tab_style(style = list(\n    cell_fill(color= \"#490E6F\",alpha=0.8),\n    cell_text(color = \"white\",font=google_font(\"Fira Code\"),\n              align = \"center\",style = \"normal\",\n              weight= \"lighter\",whitespace = \"pre-line\")\n  ),\n  locations= cells_body(\n    columns = script_sample\n  ))\n\n\n\n\n\n\n  \n    \n      2022 Data Clean Script Overview\n    \n    \n      An overview of sections in the data cleaning script\n    \n  \n  \n    \n      section\n      script_sample\n      reasoning\n    \n  \n  \n    Importing Libraries\nlapply(as.list(c(\"tidyverse\",\"janitor\",\"arrow\",\n                 \"ggthemes\",\"tidytext\",\"tidymodels\",\n                 \"textrecipes\",\"glmnet\")),\n       require,character.only=TRUE) |>\n  suppressWarnings() |> \n  suppressMessages() |> \n  invisible()\nimporting libraries required for preprocessing\n    Importing Data\nread_csv(file = \"./Data/Ask A Manager Salary Survey 2022_Responses.csv\") |> \n  clean_names() |> \n  mutate(timestamp = mdy_hms(timestamp)) |> \n  filter(!is.na(job_title)) |> \n  mutate(additional_compensation = ifelse(is.na(additional_compensation),\n                                          0,additional_compensation),\n         full_compensation = salary+additional_compensation) |> \n  filter(!currency %in% c(\"Other\",\"HKD\")) |> \n  rownames_to_column(\"respondent\")\nimport data, convert date column to date type, remove unviable cases and create total compensation columns\n    Open Text Cleaning\ntext_cleaner <- function(a_vec){\n  tokens <- str_split(a_vec,pattern=\"\\s{1,}\") |> \n    unlist()\n  tokens <- tokens[!grepl(\"\\s{1,}|[[:punct:]]|\\d{1,}\",tokens)] |> str_to_lower()\n  tokens <- unique(tokens) \n  tokens <- tokens[!tokens %in% c(stopwords::data_stopwords_nltk[[\"en\"]])]\n  return(tokens)}\nsplit open-text fields into tokens, remove punctuation, spaces and stopwords. The function returns a list of words per case\n    List to Vec\nlist_to_vec <- function(a_vec){\n  new_vec <- a_vec |> \n    as.character()\nnew_vec <- gsub(\"(^c+[[:punct:]])|[[:punct:]]\",\n                replacement = \"\",\n                new_vec)\n  return(new_vec)}\n\nConvert the list of words per case to a vector.\n    Completeness Check\ncompleteness <- function(a_vec){\n  incomplete <- round(100/length(a_vec)*a_vec[is.na(a_vec)] |> length(),2)\nreturn(incomplete)}\n\nCheck the level of missing values in each column.\n  \n  \n  \n\n\nA table containing 2022 Data Cleaner sections\n\n\nThe table above contains a few convenience functions used throughout the script. Importing of libraries is completed through lapply followed by suppressWarnings, suppressMessages and invisible functions. The last three functions calls are probably bad practise as warnings may be helpful down the line. A crucial function is the importing data followed by some basic cleaning functions from lubridate and Wickham (2022) . A more exotic function is text_cleaner, the main idea is to remove stopwords, punctuation and white spaces. Ultimately, the function returns a list of terms per case. This practise is not tidy yet. Another convenience function list_to_vec,an imitation of concatenating text values in pivot tables in excel. Finally, the completeness function checks the percentage of missing values within a column. The function is intended to aid in checking the viability of a column in the dataset. Here, columns with missing value percentage > 5% are excluded from the dataset.\nThese functions are applied through purrr::map a tidyverse equivalent to base::lapply, along with dplyr::mutate and dplyr::across to implement the functions consistently.\nAnother important task of the cleaning the data is the conversion of currencies to USD for non-USD salary values. This way, we can get closer to comparing apples with apples. The International Monetary Fund’s Exchange Rate Report Wizard which allows users to exchange rates for multiple time ranges. Importantly, the Hong Kong Dollar is not listed in the report. As such, salaries reported in that currency are not included in the analysis.\nThe final preprocessing step relies on the tidymodels and Hvitfeldt (2022) packages to specify a recipe to tokenize the values derived from list_to_vec,extract their TFIDF (Term Frequency-Inverse Document Frequency) of the tokens, log the salary values and remove near-zero variance variables.\nFinally, the resulting data.frame is used in LASSO regression, which serves as our feature selection step.\n\n\nsee code\nsource(\"./Scripts/2022_Data_Cleaner.R\",\n       local = knitr::knit_global())\n\n\n\n\nAn image containing a Lasso Regression plot indicating coeffiecients\n\n\n\n\n\nUltimately, we extract the variables of interest from the lasso model through the function below. The function is an improvement on the previous post, as it does rely on converting each matrix into a data.frame. In this instance, the function subsets the last model and minimum lambda value from the matrices which are in turn converted to a data.frame. The function is similar to Stata17’s lassocoef or SPSS29’s Best option in Linear Lasso Regression: Option. The script outputs training and testing outputs for further analysis. In this way, the preprocessing section is isolated to a single script and negates the need to read-in data.\n\n\nsee code\nvariable_extractor <- function(a_list){\n  min_lambda <- a_list$lambda |> min()\n  last_model <- length(a_list$lambda)\n  lasso_variables <- as.matrix(coef(a_list))[,last_model] |> \n    data.frame()\n  lasso_variables$variable <- rownames(lasso_variables)\n  names(lasso_variables) <- c(\"coef\",\"variable\")\n  lasso_variables <- lasso_variables[,c(\"variable\",\"coef\")]\n  lasso_variables <- lasso_variables[!grepl(pattern = \"[[:punct:]]Intercept[[:punct:]]\",\n                                            lasso_variables$variable),]\nreturn(lasso_variables)}\n\nSalary_Modelling <- cbind(Salary_Modelling[,grep(\"full_comp_usd\",names(Salary_Modelling))],\nSalary_Modelling[,names(Salary_Modelling) %in% variable_extractor(Lasso_Model)$variable])"
  },
  {
    "objectID": "posts/2022 Ask A Manager Salary Survey/index.html#ols-model",
    "href": "posts/2022 Ask A Manager Salary Survey/index.html#ols-model",
    "title": "2022 Ask A Manager Salary Survey",
    "section": "2.1 OLS MODEL",
    "text": "2.1 OLS MODEL\n\n\nsee code\nols_model <- lm(full_comp_usd ~.,data = Training_Salary)\nlibrary(gtsummary)\n\ntbl_regression(ols_model,\n               add_estimate_to_reference_rows = TRUE,\n               exponentiate = FALSE) |> \n  add_glance_source_note(label = list(\n    r.squared ~ \"R2\",\n    adj.r.squared ~ \"Adj R2\",\n    p.value ~ \"p-value\",\n    statistic ~ \"F-Statistic\",\n    df ~ \"Degrees of Freedom\",\n    df.residual ~ \"Residual df\"),\n    include = c(r.squared,adj.r.squared,p.value,df,statistic,\n                df.residual)\n    ) |> \n  italicize_labels() \n\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    tfidf_employers_industry_administration\n0.17\n-0.07, 0.42\n0.2\n    tfidf_employers_industry_banking\n0.07\n-0.32, 0.45\n0.7\n    tfidf_employers_industry_care\n-0.20\n-0.46, 0.07\n0.15\n    tfidf_employers_industry_computing\n0.16\n-0.20, 0.51\n0.4\n    tfidf_employers_industry_education\n-0.12\n-0.14, -0.10\n<0.001\n    tfidf_employers_industry_engineering\n0.11\n0.03, 0.19\n0.009\n    tfidf_employers_industry_finance\n-0.01\n-0.39, 0.37\n>0.9\n    tfidf_employers_industry_government\n0.13\n-0.08, 0.33\n0.2\n    tfidf_employers_industry_health\n0.19\n-0.07, 0.46\n0.2\n    tfidf_employers_industry_manufacturing\n-0.02\n-0.09, 0.06\n0.7\n    tfidf_employers_industry_nonprofits\n-0.09\n-0.10, -0.07\n<0.001\n    tfidf_employers_industry_public\n-0.33\n-0.47, -0.19\n<0.001\n    tfidf_employers_industry_tech\n0.12\n-0.23, 0.48\n0.5\n    tfidf_functional_area_of_job_advertising\n0.31\n-0.49, 1.1\n0.4\n    tfidf_functional_area_of_job_banking\n-0.43\n-0.97, 0.11\n0.12\n    tfidf_functional_area_of_job_computing\n0.71\n-0.25, 1.7\n0.15\n    tfidf_functional_area_of_job_education\n-0.03\n-0.05, -0.01\n0.009\n    tfidf_functional_area_of_job_finance\n0.39\n-0.15, 0.93\n0.2\n    tfidf_functional_area_of_job_pr\n-0.30\n-1.1, 0.51\n0.5\n    tfidf_functional_area_of_job_tech\n-0.52\n-1.5, 0.44\n0.3\n    tfidf_job_title_manager\n0.08\n0.05, 0.11\n<0.001\n    tfidf_job_title_senior\n0.23\n0.19, 0.27\n<0.001\n    tfidf_country_canada\n0.17\n0.15, 0.19\n<0.001\n    tfidf_country_states\n-0.50\n-1.2, 0.19\n0.2\n    tfidf_country_united\n1.2\n0.56, 1.9\n<0.001\n    tfidf_race_white\n-0.09\n-0.14, -0.04\n<0.001\n  \n  \n    \n      R2 = 0.155; Adj R2 = 0.153; p-value = <0.001; Degrees of Freedom = 26; F-Statistic = 79.2; Residual df = 11,237\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\nOLS Regression Results illustrating an adjusted r-square of 0.15 on 11237 cases and global p value of < 0.001\n\n\nFitting a linear regression model is rather straight forward. The model above yields an Adjusted R Square of 0.153. In other words, the model explains 15% of the variance in salaries. The beta, confidence interval and p-values of the variables considered are listed on the table. One way to report the aforementioned results would be to utilise the report package which can aid in reporting models in standard manner including beta values, standard error, confidence intervals etc. for the respective variables considered in the model."
  },
  {
    "objectID": "posts/2022 Ask A Manager Salary Survey/index.html#mars-model",
    "href": "posts/2022 Ask A Manager Salary Survey/index.html#mars-model",
    "title": "2022 Ask A Manager Salary Survey",
    "section": "2.2 MARS MODEL",
    "text": "2.2 MARS MODEL\nBelow, we build a MARS model\n\n\nsee code\nlibrary(earth)\n\nMARS_Model <- earth(full_comp_usd ~.,Training_Salary,\n      degree = 2,\n      glm = list(family = \"gaussian\"),\n      pmethod = \"exhaustive\",\n      nfold = 10,\n      ncross= 10,\n      varmod.method = \"earth\")\n\nFold_Rsquares <- lapply(MARS_Model$cv.list,\\(x){x[\"rsq\"]})\n\nFold_Rsquares <- lapply(Fold_Rsquares,data.frame)\n\ntheme_set(ggthemes::theme_solarized_2())\n\nmap2(Fold_Rsquares,names(Fold_Rsquares),function(x,y){\n  x |> \n    mutate(fold_source =y)\n})%>%\n  do.call(bind_rows,.) |> \n  mutate(iteration = str_extract(fold_source,\"\\\\d{1,2}$\") |> as.integer(),\n         fold_source = str_remove(fold_source,\"[[:punct:]]\\\\d{1,}\") |> as.factor(),\n         fold_source = fct_reorder(fold_source,iteration)) |> \n  ggplot(aes(iteration,rsq,group=fold_source))+\n  geom_line(aes(colour = fold_source),show.legend = FALSE)+\n  scale_colour_manual(values = c(\"fold1\" = \"#0F204B\",\n                                 \"fold2\"= \"#A71930\",\n                                 \"fold3\" = \"#000000\",\n                                 \"fold4\" = \"#9E83B7\",\n                                 \"fold5\"=\"#00B140\",\n                                 \"fold6\" = \"#BB133E\",\n                                 \"fold7\"= \"#490E6F\",\n                                 \"fold8\"= \"#0039A7\",\n                                 \"fold9\"= \"#00675A\",\n                                 \"fold10\"= \"#EA8400\"))+\n  scale_x_continuous(breaks = c(1:10))+\n  facet_wrap(~fold_source,\n             ncol = 2,\n             nrow=5, \n             scales = \"free\")\n\n\n\n\nMARS Model Results indicating a selected model with an R-Square of 0.15"
  },
  {
    "objectID": "posts/2022 Ask A Manager Salary Survey/index.html#ols-model-swiss-knife-approach",
    "href": "posts/2022 Ask A Manager Salary Survey/index.html#ols-model-swiss-knife-approach",
    "title": "2022 Ask A Manager Salary Survey",
    "section": "2.1 OLS MODEL: SWISS KNIFE APPROACH",
    "text": "2.1 OLS MODEL: SWISS KNIFE APPROACH\n\n\nsee code\nols_model <- lm(full_comp_usd ~.,data = Training_Salary)\nlibrary(gtsummary)\n\ntbl_regression(ols_model,\n               add_estimate_to_reference_rows = TRUE,\n               exponentiate = FALSE) |> \n  add_glance_source_note(label = list(\n    r.squared ~ \"R2\",\n    adj.r.squared ~ \"Adj R2\",\n    p.value ~ \"p-value\",\n    statistic ~ \"F-Statistic\",\n    df ~ \"Degrees of Freedom\",\n    df.residual ~ \"Residual df\"),\n    include = c(r.squared,adj.r.squared,p.value,df,statistic,\n                df.residual)\n    ) |> \n  italicize_labels() \n\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    tfidf_employers_industry_administration\n0.17\n-0.07, 0.42\n0.2\n    tfidf_employers_industry_banking\n0.07\n-0.32, 0.45\n0.7\n    tfidf_employers_industry_care\n-0.20\n-0.46, 0.07\n0.15\n    tfidf_employers_industry_computing\n0.16\n-0.20, 0.51\n0.4\n    tfidf_employers_industry_education\n-0.12\n-0.14, -0.10\n<0.001\n    tfidf_employers_industry_engineering\n0.11\n0.03, 0.19\n0.009\n    tfidf_employers_industry_finance\n-0.01\n-0.39, 0.37\n>0.9\n    tfidf_employers_industry_government\n0.13\n-0.08, 0.33\n0.2\n    tfidf_employers_industry_health\n0.19\n-0.07, 0.46\n0.2\n    tfidf_employers_industry_manufacturing\n-0.02\n-0.09, 0.06\n0.7\n    tfidf_employers_industry_nonprofits\n-0.09\n-0.10, -0.07\n<0.001\n    tfidf_employers_industry_public\n-0.33\n-0.47, -0.19\n<0.001\n    tfidf_employers_industry_tech\n0.12\n-0.23, 0.48\n0.5\n    tfidf_functional_area_of_job_advertising\n0.31\n-0.49, 1.1\n0.4\n    tfidf_functional_area_of_job_banking\n-0.43\n-0.97, 0.11\n0.12\n    tfidf_functional_area_of_job_computing\n0.71\n-0.25, 1.7\n0.15\n    tfidf_functional_area_of_job_education\n-0.03\n-0.05, -0.01\n0.009\n    tfidf_functional_area_of_job_finance\n0.39\n-0.15, 0.93\n0.2\n    tfidf_functional_area_of_job_pr\n-0.30\n-1.1, 0.51\n0.5\n    tfidf_functional_area_of_job_tech\n-0.52\n-1.5, 0.44\n0.3\n    tfidf_job_title_manager\n0.08\n0.05, 0.11\n<0.001\n    tfidf_job_title_senior\n0.23\n0.19, 0.27\n<0.001\n    tfidf_country_canada\n0.17\n0.15, 0.19\n<0.001\n    tfidf_country_states\n-0.50\n-1.2, 0.19\n0.2\n    tfidf_country_united\n1.2\n0.56, 1.9\n<0.001\n    tfidf_race_white\n-0.09\n-0.14, -0.04\n<0.001\n  \n  \n    \n      R2 = 0.155; Adj R2 = 0.153; p-value = <0.001; Degrees of Freedom = 26; F-Statistic = 79.2; Residual df = 11,237\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\nOLS Regression Results illustrating an adjusted r-square of 0.15 on 11237 cases and global p value of < 0.001\n\n\nFitting a linear regression model is rather straight forward. The model above yields an Adjusted R Square of 0.153. In other words, the model explains 15% of the variance in salaries. The beta, confidence interval and p-values of the variables considered are listed on the table. One way to report the aforementioned results would be to utilise the report package which can aid in reporting models in standard manner including beta values, standard error, confidence intervals etc. for the respective variables considered in the model."
  },
  {
    "objectID": "posts/2022 Ask A Manager Salary Survey/index.html#mars-model-splines-and-lines",
    "href": "posts/2022 Ask A Manager Salary Survey/index.html#mars-model-splines-and-lines",
    "title": "2022 Ask A Manager Salary Survey",
    "section": "2.2 MARS MODEL: SPLINES AND LINES",
    "text": "2.2 MARS MODEL: SPLINES AND LINES\nBelow, we build a MARS model and iterate through 10 cross-validation folds. The resulting models, does not out perform the OLS model. The R-Square average R-Square for the MARS model is 0.15.\n\n\nsee code\nlibrary(earth)\n\nMARS_Model <- earth(full_comp_usd ~.,Training_Salary,\n      degree = 2,\n      glm = list(family = \"gaussian\"),\n      pmethod = \"exhaustive\",\n      nfold = 10,\n      ncross= 10,\n      varmod.method = \"earth\")\n\nFold_Rsquares <- lapply(MARS_Model$cv.list,\\(x){x[\"rsq\"]})\n\nFold_Rsquares <- lapply(Fold_Rsquares,data.frame)\n\ntheme_set(ggthemes::theme_solarized_2())\n\nmap2(Fold_Rsquares,names(Fold_Rsquares),function(x,y){\n  x |> \n    mutate(fold_source =y)\n})%>%\n  do.call(bind_rows,.) |> \n  mutate(iteration = str_extract(fold_source,\"\\\\d{1,2}$\") |> as.integer(),\n         fold_source = str_remove(fold_source,\"[[:punct:]]\\\\d{1,}\") |> as.factor(),\n         fold_source = fct_reorder(fold_source,iteration)) |> \n  ggplot(aes(iteration,rsq,group=fold_source))+\n  geom_line(aes(colour = fold_source),show.legend = FALSE)+\n  scale_colour_manual(values = c(\"fold1\" = \"#0F204B\",\n                                 \"fold2\"= \"#A71930\",\n                                 \"fold3\" = \"#000000\",\n                                 \"fold4\" = \"#9E83B7\",\n                                 \"fold5\"=\"#00B140\",\n                                 \"fold6\" = \"#BB133E\",\n                                 \"fold7\"= \"#490E6F\",\n                                 \"fold8\"= \"#0039A7\",\n                                 \"fold9\"= \"#00675A\",\n                                 \"fold10\"= \"#EA8400\"))+\n  scale_x_continuous(breaks = c(1:10))+\n  facet_wrap(~fold_source,\n             ncol = 2,\n             nrow=5, \n             scales = \"free\")\n\n\n\n\nMARS Model Results indicating a selected model with an R-Square of 0.15\n\n\n\n\n\nThere are several ways to improve the model. Such as including factor variables in place of dummy variables, utilising polynomial regression in place of the gaussian family among others. For a more detailed account on how to work with enhance MARS models, see Milborrow (n.d.) .\n\n\nsee code\nplotmo(MARS_Model)\n\n\n plotmo grid:    tfidf_employers_industry_administration\n                                                       0\n tfidf_employers_industry_banking tfidf_employers_industry_care\n                                0                             0\n tfidf_employers_industry_computing tfidf_employers_industry_education\n                                  0                                  0\n tfidf_employers_industry_engineering tfidf_employers_industry_finance\n                                    0                                0\n tfidf_employers_industry_government tfidf_employers_industry_health\n                                   0                               0\n tfidf_employers_industry_manufacturing tfidf_employers_industry_nonprofits\n                                      0                                   0\n tfidf_employers_industry_public tfidf_employers_industry_tech\n                               0                             0\n tfidf_functional_area_of_job_advertising tfidf_functional_area_of_job_banking\n                                        0                                    0\n tfidf_functional_area_of_job_computing tfidf_functional_area_of_job_education\n                                      0                                      0\n tfidf_functional_area_of_job_finance tfidf_functional_area_of_job_pr\n                                    0                               0\n tfidf_functional_area_of_job_tech tfidf_job_title_manager\n                                 0                       0\n tfidf_job_title_senior tfidf_country_canada tfidf_country_states\n                      0                    0            0.3854429\n tfidf_country_united tfidf_race_white\n            0.3849207        0.7425228\n\n\n\n\nMARS Model Results Visualisation containing variables and their relationships retrieved through the plotmo package."
  },
  {
    "objectID": "posts/2022 Ask A Manager Salary Survey/index.html#keras-gun-to-a-knife-fight",
    "href": "posts/2022 Ask A Manager Salary Survey/index.html#keras-gun-to-a-knife-fight",
    "title": "2022 Ask A Manager Salary Survey",
    "section": "2.3 KERAS: GUN TO A KNIFE FIGHT",
    "text": "2.3 KERAS: GUN TO A KNIFE FIGHT\nWe haven’t had a significant difference between the OLS model and MARS model. Below, we use build a Allaire and Chollet (2022) regression model. It is mostly likely a computationally inefficient approach but worth exploring nonetheless. Fortunately, with a few changes to Chollet, Kalinowski, and Allaire (2022) code , we can fit a regression model through Keras in R. Below, we change the data to matrix format suitable for the type of model we build. Thereafter, we fit it through 500 epochs and visualise the Mean Absolute Error and Loss (Mean Squared Error) of the model.\n\n\nsee code\n#|fig-cap: Specifying a Keras model for regression\nlibrary(keras)\ntrain_data <- model.matrix(full_comp_usd ~.,Training_Salary)[,-1]\ntrain_targets <- Training_Salary$full_comp_usd\nbuild_model <- function() {\nmodel <- keras_model_sequential() %>%\nlayer_dense(64, activation = \"relu\") %>%\nlayer_dense(64, activation = \"relu\") %>%\nlayer_dense(1)\nmodel %>% compile(optimizer = \"rmsprop\",\nloss = \"mse\",\nmetrics = \"mae\")\nmodel\n}\n\n\n\n\nsee code\n#|echo: true\n#|fig-cap: Fitting a regression Keras model \nk <- 4\nfold_id <- sample(rep(1:k, length.out = nrow(train_data)))\nnum_epochs <- 100\nall_scores <- numeric()\nfor (i in 1:k) {\ncat(\"Processing fold #\", i, \"\\n\")\nval_indices <- which(fold_id == i)\nval_data <- train_data[val_indices, ]\nval_targets <- train_targets[val_indices]\npartial_train_data <- train_data[-val_indices, ]\npartial_train_targets <- train_targets[-val_indices]\nmodel <- build_model()\nmodel %>% fit(\npartial_train_data,\npartial_train_targets,\nepochs = num_epochs,\nbatch_size = 16,\nverbose = 1\n)\nresults <- model %>% evaluate(val_data,\nval_targets, verbose = 0)\nall_scores[[i]] <- results[['mae']]\n}\n\n\nProcessing fold # 1 \nProcessing fold # 2 \nProcessing fold # 3 \nProcessing fold # 4 \n\n\n\n\nsee code\nnum_epochs <- 100\nall_mae_histories <- list()\nfor (i in 1:k) {\ncat(\"Processing fold #\", i, \"\\n\")\nval_indices <- which(fold_id == i)\nval_data <- train_data[val_indices, ]\nval_targets <- train_targets[val_indices]\npartial_train_data <- train_data[-val_indices, ]\npartial_train_targets <- train_targets[-val_indices]\nmodel <- build_model()\nhistory <- model %>% fit(\npartial_train_data, partial_train_targets,\nvalidation_data = list(val_data, val_targets),\nepochs = num_epochs, batch_size = 16, verbose = 1\n)\nmae_history <- history$metrics$val_mae\nall_mae_histories[[i]] <- mae_history\n}\n\n\nProcessing fold # 1 \nProcessing fold # 2 \nProcessing fold # 3 \nProcessing fold # 4 \n\n\nsee code\nplot(history)\n\n\n\n\n\nWe know the three model’s performance metrics, the next question we need to answer is whether the data maintains the similar results on an out-of-sample dataset. Here we rely on Kuhn and Wickham (2022), yardstick package to compare performance between the models. It is probably a better approach to use a tidymodel’s workflow and recipes as it provides all the functions needed for preprocessing and modelling."
  },
  {
    "objectID": "posts/2023 TidyTuesday Week 1/index.html",
    "href": "posts/2023 TidyTuesday Week 1/index.html",
    "title": "2021 South African Local Government Election Results",
    "section": "",
    "text": "1 INTRODUCTION\nIt has a been more than a year since, I published a #TidyTuesday submission, a weekly, data visualisation activity featuring a varied range of datasets. For the first week of 2023, the activity encourages participants to Bring their own data. Fortunately, I have been seating on a dataset from Independent Electoral Commission of South Africa (2023) containing the 2021 Local Government Election Results. The dataset is detailed, including results down to voting station level. The IEC also provides a data dictionary along with detailed methodology on how the results can be interpreted. We aren’t here to reproduce that process (that is for another day). We simply want to enhance the visualisation of the results.\n\n\nsee code\nlapply(c(\"tidyverse\",\"janitor\",\"arrow\",\n         \"sf\",\"ggthemes\",\"showtext\",\n         \"leaflet\"),\n       require,character.only = TRUE) |> \n  suppressWarnings() |> \n  suppressMessages()\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] TRUE\n\n[[6]]\n[1] TRUE\n\n[[7]]\n[1] TRUE\n\n\nsee code\nfont_add_google(\"IBM Plex Sans\")\nshowtext_auto()\n\n\n\n\n2 IMPORTANT DATA ASPECTS\nIn South Africa, the Municipal Demarcation Board (2023) , is the body responsible for the demarcation of South Africa into multiple levels such as District Municipalities, Metropolitan Municipalities, Local Municipalities and Voting Districts. In Local Government Elections, these demarcations in turn, determine seat allocation and across all types of municipalities.\nThere important nuances in respect to the Local Government Elections voting ballots; such as direct (Ward-level vote) and indirect votes(proportional representation). For purposes of this analysis, we will rely on illustrating voting outcomes at ward level. In effect, we have two datasets to work with, 2020 Municipal Demarcation Board ShapeFile and 2021 Local Government Election Results (Comma-Separated File).\nWrangling spatial data is made easy by the sf package ( Pebesma (2022) ). The voting results can be wrangled and joined to the sf object through the dplyr package ( Wickham (2022) ). Since this is R, there are a plethora of data visualisation packages to utilise as well. Here, we chiefly rely on two, ggplot2 and leaflet. The first can be used to create static maps while the latter offers a great set of tools for interactive visualisations. Below, we illustrate the process of importing that spatial data into R along with the csv of election results.\n\n\nsee code\nSA_Wards <- st_read(\"./2020_Spatial_Data/SA_Wards2020.shp\")\n\n\nReading layer `SA_Wards2020' from data source \n  `C:\\Users\\Sivuyile\\Desktop\\RStudio\\Personal_Site\\Personal_Site_3\\posts\\2023 TidyTuesday Week 1\\2020_Spatial_Data\\SA_Wards2020.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 4468 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 16.45189 ymin: -34.83417 xmax: 32.94498 ymax: -22.12503\nGeodetic CRS:  WGS 84\n\n\nsee code\nElection_Results <- read_parquet(file = \"./Final_Dbs/2016 - 2021_Local_Gov_Election_Results_2022-04-27.parquet\") |>\n  filter(election_year == 2021)\n\nglimpse(Election_Results)\n\n\nRows: 1,084,734\nColumns: 13\n$ source_file         <chr> \"./2021/EC.csv\", \"./2021/EC.csv\", \"./2021/EC.csv\",…\n$ province            <chr> \"Eastern Cape\", \"Eastern Cape\", \"Eastern Cape\", \"E…\n$ municipality        <chr> \"BUF - Buffalo City\", \"BUF - Buffalo City\", \"BUF -…\n$ ward                <chr> \"Ward 29200001\", \"Ward 29200001\", \"Ward 29200001\",…\n$ voting_district     <dbl> 10590151, 10590151, 10590151, 10590151, 10590151, …\n$ voting_station_name <chr> \"PEFFERVILLE CLINIC\", \"PEFFERVILLE CLINIC\", \"PEFFE…\n$ registered_voters   <dbl> 2724, 2724, 2724, 2724, 2724, 2724, 2724, 2724, 27…\n$ ballot_type         <chr> \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"P…\n$ spoilt_votes        <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ party_name          <chr> \"ABANTU BATHO CONGRESS\", \"AFRICA RESTORATION ALLIA…\n$ total_valid_votes   <dbl> 3, 6, 3, 7, 0, 286, 1, 1, 0, 0, 1, 0, 3, 615, 24, …\n$ date_generated      <chr> \"11/23/2021 4:17:50 PM\", \"11/23/2021 4:17:50 PM\", …\n$ election_year       <dbl> 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 20…\n\n\nAs mentioned above, we are only interested in Ward Ballot results, as a result, we filter out all indirect ballot types. Thereafter, we group the data by Province, Municipality and Ward and derive a sum of all valid votes. In other words, we exclude spoilt votes and do not consider voter turn out extra. We store the result in a data-frame called `Total _Votes`. As the name implies, the data-frame contains a sum of total valid votes per ward. Below, we add an additional variable, party-name, which helps us tally all the votes cast for a particular party or independent cast per ward. Finally, we filter for the maximum votes accumulated by the party/independent per ward.\n\n\nsee code\nTotal_Votes <- Election_Results |>\n  filter(ballot_type == \"Ward\") |> \n  group_by(across(.cols=c(province,municipality,ward))) |> \n  summarise(total_votes = sum(total_valid_votes),\n            .groups = \"drop\")\n\nWard_Results <-  Total_Votes |> \n  left_join(Election_Results |> \n              filter(ballot_type == \"Ward\") |>   \n  group_by(across(.cols=c(ward,party_name))) |> \n  summarise(party_votes= sum(total_valid_votes)) |>\n    filter(party_votes == max(party_votes)) |> \n    ungroup()) |> \n  mutate(support = 100/total_votes*party_votes)\n\nglimpse(Ward_Results)\n\n\nRows: 4,468\nColumns: 7\n$ province     <chr> \"Eastern Cape\", \"Eastern Cape\", \"Eastern Cape\", \"Eastern …\n$ municipality <chr> \"BUF - Buffalo City\", \"BUF - Buffalo City\", \"BUF - Buffal…\n$ ward         <chr> \"Ward 29200001\", \"Ward 29200002\", \"Ward 29200003\", \"Ward …\n$ total_votes  <dbl> 3803, 3299, 3326, 4524, 3506, 3421, 3446, 3917, 3359, 305…\n$ party_name   <chr> \"AFRICAN NATIONAL CONGRESS\", \"AFRICAN NATIONAL CONGRESS\",…\n$ party_votes  <dbl> 1748, 2883, 1630, 2561, 1912, 1420, 2174, 2397, 1513, 150…\n$ support      <dbl> 45.96371, 87.39012, 49.00782, 56.60920, 54.53508, 41.5083…\n\n\nTo finalise the ward results, the two data-frames are joined. Ultimately, we have a data-frame with the winner of the ward. To visualise the data, we join the spatial data to the ward results data-frame.\n\n\nsee code\nWard_Results <- left_join(SA_Wards,Ward_Results |>\n  mutate(ward = str_remove(ward,\"Ward\\\\s{1,}\")) |> \n  rename(WardID = ward) |> \n  select(-province))\n\n\nBefore completing our first visualisation, there are some mopping up required. These are mainly for aesthetic in nature, such as importing hex codes for each political party which largely resembles the respective party’s corporate colour and grouping colours for smaller parties (\\(parties with wards < 2\\)).\n\n\nsee code\nDescriptives_Results <- read_csv(file = \"./Final_Dbs/2021_Party_Results_Descriptives_2023-01-06.csv\")%>% \n  sapply(.,as.character)%>%\n  data.frame()\n\nDescriptives_Results$occurence <- as.integer(Descriptives_Results$occurence)\n\nOther_Politics <- paste(Descriptives_Results[Descriptives_Results$occurence <=2,\"party_name\"],\n      collapse = \",\")\n\nWard_Results <- Ward_Results |>\n  mutate(party_name = case_when(party_name %in% c(\"AL JAMA-AH,KAROO GEMEENSKAP PARTY\",\"TEAM SUGAR SOUTH AFRICA\",\"UMSOBOMVU RESIDENTS ASSOCIATION\",\"ACTIONSA,AZANIA RESIDENT PARTY\",\"BREEDEVALLEI ONAFHANKLIK\",\"CAPRICORN INDEPENDENT COMMUNITY ACTIVISTS FORUM\",\"CEDERBERG FIRST RESIDENTS ASSOCIATION\",\"DIENSLEWERINGS PARTY\",\"FORUM 4 SERVICE DELIVERY\",\"INDEPENDENT SOUTH AFRICAN NATIONAL CIVIC ORGANISATION\",\"LAND PARTY,NAMAKWA CIVIC MOVEMENT\",\"PLETT DEMOCRATIC CONGRESS\",\"SIYATHEMBA COMMUNITY MOVEMENT\",\"TSOGANG CIVIC MOVEMENT,UNITED DEMOCRATIC MOVEMENT\"\n) ~ \"OTHER\",\n                                TRUE ~ party_name))\n\nWards_Plot <- Ward_Results |> \nggplot()+\n  geom_sf(aes(fill = party_name),colour=\"black\")+\n  scale_fill_manual(values = \n                      c(\"AFRICAN NATIONAL CONGRESS\"= \"#FFD700\",\n              \"DEMOCRATIC ALLIANCE\"= \"#00008B\",\n              \"INKATHA FREEDOM PARTY\"=\"#FFFF00\",\n              \"INDEPENDENT\"=\"#90EE90\",\n              \"ECONOMIC FREEDOM FIGHTERS\"=\"#8b0000\",\n              \"MAPSIXTEEN CIVIC MOVEMENT\"=\"#9F2B68\",\n              \"NATIONAL FREEDOM PARTY\"=\"#808080\",\n              \"VRYHEIDSFRONT PLUS\"=\"#FFA500\",\n              \"PATRIOTIC ALLIANCE\"=\"#39FF14\",\n              \"ABANTU BATHO CONGRESS\"=\"#ADD8E6\",\n              \"AFRICAN PEOPLE'S MOVEMENT\"=\"#FFFFE0\",\n              \"GOOD\"=\"#F5E236\",\n              \"INDEPENDENT CIVIC ORGANISATION OF SOUTH AFRICA\"=\"#FA1138\",\n              \"SETSOTO SERVICE DELIVERY FORUM\"=\"#A5FA11\",\n              \"AL JAMA-AH\"= \"#334A05\",\n              \"KAROO GEMEENSKAP PARTY\"=\"#5BF5A3\",\n              \"TEAM SUGAR SOUTH AFRICA\"=\"#F2FF03\",\n              \"UMSOBOMVU RESIDENTS ASSOCIATION\"=\"#141212\",\n              \"OTHER\" = \"#FFC0CB\"))+\n  labs(title = \"2021 Local Government Election Results\",\n       subtitle = \"Ballot Type (Ward): Winning Party per ward in the 2021 South African Local Government Elections.\\nThe results do not include other ballot types such as DC 40% or Proportional Representation.\",\n       caption = \"Data Source: https://www.elections.org.za/\\nPlot: Sivuyile Nzimeni(sivuyilenzimeni.netlify.app)\",\n       fill = \"Party Name\")+\n  theme_void()+\n  theme(text = element_text(\"IBM Plex Sans\"),\n        plot.title =element_text(hjust=0.5,face = \"bold\"),\n        plot.subtitle = element_text(hjust = 0.5,face = \"italic\"),\n        plot.caption = element_text(hjust=0.8,face = \"italic\"),\n        legend.position = \"bottom\")\n\nWard_Results <-Ward_Results |> \n  left_join(Descriptives_Results |> select(-occurence))\n\nWard_Results <- Ward_Results |> \n  mutate(overview = paste0(\"<strong>\",Province,\"</strong>\",\"<br/>\",\n                              \"<strong>\",Municipali,\"</strong>\",\"<br/>\",\n                              '<strong>Ward ID</strong>',\"<br/>\",WardID,\"<br/>\",\n                              \"<strong>Winner</strong>:\",party_name,\"<br/>\",\n                              \"<strong>Support</strong>: \",round(support,2),\"%\"),\n         colour = str_trim(colour))\n\nWard_Results <- Ward_Results |> \n  mutate(overview = lapply(overview,htmltools::HTML)) |> \n  unnest(overview)\n\n\n\n\n3 VISUALISATION\n\n\nsee code\nWards_Plot\n\n\n\n\n\nThe resulting static visualisation illustrates that the African National Congress (ANC) won the majority of wards in South Africa followed by the Democratic Alliance (DA) in some urban areas. The visualisation above has be read in context.\n\nLand does not vote: the composition of wards is influenced by several factors such as population density, level of development, natural landscape etc. As such, it is possible to have a large ward (by land size) with minimal low density and vice versa.\nWard-Outcomes are partial: in Local Government Elections, voters have at least two ballots, direct votes and indirect votes. For Example, ActionSA obtained one ward in the 2021 LGE, they obtained more than 50 seats in the provinces where they competed.\nRigour: for a more comprehensive analysis, spatial econometrics offers a number of tools to help understand voting patterns over time. Naturally, the datasets required would need to be expanded to include Statistics South Africa (Census), previous voting patterns (IEC) and former demarcations (MDB).\n\n\n\n\n\n\n\nRendering Issue\n\n\n\nR has several packages for interactive visualisation. In the code, we attempted to use leaflet to render the interactive visualisation. The size of the data appears to be a hindrance, instead generating an error Fatal javascript OOM in Reached heap limit. Other users have experienced this issue. Follow Issue 2462 for more information and progress on the issue.\nThe rendering works locally. The script above provides the cleaning process before creating an interactive map in `leaflet` or another interactive visualisation library.\n\n\n\n\n4 SUMMARY\nIn the post, we imported a spatial data file from MDB along with election results from the IEC. Ward-Level outcomes are visualised through a choropleth map.\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\nReferences\n\nIndependent Electoral Commission of South Africa. 2023. ‘Municipal Election Results - Electoral Commission of South Africa’. https://results.elections.org.za/home/downloads/me-results.\n\n\nMunicipal Demarcation Board. 2023. ‘Municipal Demarcation Board’. https://dataportal-mdb-sa.opendata.arcgis.com/.\n\n\nPebesma, Edzer. 2022. Sf: Simple Features for r. https://CRAN.R-project.org/package=sf.\n\n\nWickham, Hadley. 2022. Tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse."
  }
]